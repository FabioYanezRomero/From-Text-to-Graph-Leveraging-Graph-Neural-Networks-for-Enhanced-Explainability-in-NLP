#!/usr/bin/env python3
"""
GNN Data Processor

This module processes word embeddings and prepares them for use in Graph Neural Networks.
It creates graph structures from text data using the embeddings generated by the embedding_generator.py.
"""

import os
import argparse
import logging
import json
import pickle as pkl
import numpy as np
from tqdm import tqdm
import torch
import torch.nn.functional as F
import torch_geometric
from torch_geometric.data import Data
import networkx as nx

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

def get_dataset_info(embeddings_dir, split):
    """Get information about the dataset without loading all embeddings"""
    logger.info(f"Getting dataset info from {embeddings_dir}")
    
    # Check the embeddings directory
    split_dir = os.path.join(embeddings_dir, split)
    metadata_path = os.path.join(split_dir, 'metadata.pkl')
    word_embeddings_path = os.path.join(split_dir, 'word_embeddings.pkl')
    sentence_embeddings_path = os.path.join(split_dir, 'sentence_embeddings.pkl')
    special_embeddings_path = os.path.join(split_dir, 'special_embeddings.pkl')
    
    # Check if the required files exist
    if not all(os.path.exists(p) for p in [word_embeddings_path, sentence_embeddings_path, metadata_path]):
        logger.error(f"Missing required files in {split_dir}")
        raise FileNotFoundError(f"Missing required files in {split_dir}")
    
    with open(metadata_path, 'rb') as f:
        metadata = pkl.load(f)
    
    total_samples = metadata['total_samples']
    
    logger.info(f"Dataset has {total_samples} samples")
    
    # Check if special embeddings exist
    has_special_embeddings = os.path.exists(special_embeddings_path)
    logger.info(f"Special embeddings found: {has_special_embeddings}")
    
    return {
        'total_samples': total_samples,
        'split_dir': split_dir,
        'has_special_embeddings': has_special_embeddings
    }

def load_embeddings(split_dir):
    """Load embeddings from a directory"""
    logger.info(f"Loading embeddings from {split_dir}")
    
    try:
        # Load word embeddings
        word_embeddings_path = os.path.join(split_dir, 'word_embeddings.pkl')
        logger.info(f"Loading word embeddings from {word_embeddings_path}")
        with open(word_embeddings_path, 'rb') as f:
            word_embeddings = pkl.load(f)
        
        # Load sentence embeddings
        sentence_embeddings_path = os.path.join(split_dir, 'sentence_embeddings.pkl')
        logger.info(f"Loading sentence embeddings from {sentence_embeddings_path}")
        with open(sentence_embeddings_path, 'rb') as f:
            sentence_embeddings = pkl.load(f)
        
        # Load metadata
        metadata_path = os.path.join(split_dir, 'metadata.pkl')
        logger.info(f"Loading metadata from {metadata_path}")
        with open(metadata_path, 'rb') as f:
            metadata = pkl.load(f)
            texts = metadata['texts']
            labels = metadata['labels']
        
        # Check for special embeddings
        special_embeddings_path = os.path.join(split_dir, 'special_embeddings.pkl')
        special_embeddings = None
        if os.path.exists(special_embeddings_path):
            logger.info(f"Loading special embeddings from {special_embeddings_path}")
            with open(special_embeddings_path, 'rb') as f:
                special_embeddings = pkl.load(f)
        
        logger.info(f"Loaded {len(texts)} samples")
        return word_embeddings, sentence_embeddings, special_embeddings, texts, labels
    
    except Exception as e:
        logger.error(f"Error loading embeddings from {split_dir}: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise

def create_word_graphs(word_embeddings, sentence_embeddings, texts, labels, special_embeddings=None, window_size=3, edge_type='window'):
    """
    Create graph structures from word embeddings
    
    Args:
        word_embeddings: List of word embeddings for each sample
        sentence_embeddings: List of sentence embeddings for each sample
        texts: List of texts (lists of words)
        labels: List of labels
        special_embeddings: Dictionary of special embeddings for constituency trees
        window_size: Size of window for creating edges
        edge_type: Type of edges to create ('window' or 'fully_connected')
        
    Returns:
        List of torch_geometric.data.Data objects
    """
    logger.info(f"Creating graphs with edge_type={edge_type}, window_size={window_size}")
    logger.info(f"Input sizes: {len(word_embeddings)} word embeddings, {len(sentence_embeddings)} sentence embeddings, {len(texts)} texts, {len(labels)} labels")
    
    graphs = []
    
    # Define constituency dictionary for special tokens
    constituency_dict = {
        '«SENTENCE»': 'Sentence',
        '«REDUCED RELATIVE CLAUSE»': 'Reduced relative clause',
        '«NOUN PHRASE»': 'Noun phrase',
        '«VERB PHRASE»': 'Verb phrase',
        '«PREPOSITIONAL PHRASE»': 'Prepositional phrase',
        '«ADVERB PHRASE»': 'Adverb phrase',
        '«UNLIKE COORDINATED PHRASE»': 'Unlike coordinated phrase',
        '«ADJECTIVE PHRASE»': 'Adjective phrase',
        '«SUBORDINATE CLAUSE»': 'Subordinate clause',
        '«WH-ADVERB PHRASE»': 'Wh-adverb phrase',
        '«WHADVP»': 'Wh-adverb phrase',
        '«NX»': 'Nominal phrase',
        '«QUANTIFIER PHRASE»': 'Quantifier phrase',
        '«NOUN PHRASE (NO HEAD)»': 'Noun phrase (no head)',
        '«PARTICLE»': 'Particle',
        '«PARENTETICAL»': 'Parentetical',
        '«WH-NOUN PHRASE»': 'Wh-noun phrase',
        '«WH-ADJECTIVE PHRASE»': 'Wh-adjective phrase',
        '«NOT A CONSTITUENT»': 'Not a constituent',
        '«FRAGMENT»': 'Fragment',
        '«INVERTED SENTENCE»': 'Inverted sentence',
        '«INTERJECTION»': 'Interjection',
        '«WH-PREPOSITIONAL PHRASE»': 'Wh-prepositional phrase',
        '«QUESTION»': 'Question',
        '«SUBORDINATE CLAUSE QUESTION»': 'Subordinate clause question',
        '«CONJUCTION PHRASE»': 'Conjunction phrase',
        '«UNKNOWN»': 'Unknown',
        '«LIST MARKER»': 'List marker',
        'constituency relation': 'Constituency relation'
    }
    
    for i, (sample_word_embeddings, sentence_embedding, text, label) in enumerate(zip(word_embeddings, sentence_embeddings, texts, labels)):
        if i % 5 == 0:
            logger.info(f"Processing sample {i}/{len(word_embeddings)}")
        
        # Skip empty samples
        if len(sample_word_embeddings) == 0 or len(text) == 0:
            logger.warning(f"Skipping empty sample {i}: words={len(sample_word_embeddings)}, text={len(text)}")
            continue
        
        # Log sample details
        logger.debug(f"Sample {i}: {len(sample_word_embeddings)} words, label={label}")
        
        try:
            # Process embeddings based on text content
            processed_embeddings = []
            
            for j, word in enumerate(text):
                # Check if this is a special constituency token
                if word in constituency_dict and special_embeddings is not None and word in special_embeddings:
                    # Use special embedding for constituency token
                    logger.debug(f"Using special embedding for '{word}'")
                    processed_embeddings.append(special_embeddings[word])
                else:
                    # Use regular word embedding
                    if j < len(sample_word_embeddings):
                        processed_embeddings.append(sample_word_embeddings[j])
                    else:
                        logger.warning(f"Missing embedding for word {j} ('{word}') in sample {i}")
                        # Use a zero vector as fallback
                        if isinstance(sample_word_embeddings[0], torch.Tensor):
                            processed_embeddings.append(torch.zeros_like(sample_word_embeddings[0]))
                        else:
                            processed_embeddings.append(np.zeros_like(sample_word_embeddings[0]))
            
            # Convert to tensor
            if len(processed_embeddings) > 0:
                try:
                    # First convert all embeddings to tensors if they're not already
                    tensor_embeddings = []
                    for emb in processed_embeddings:
                        if isinstance(emb, torch.Tensor):
                            tensor_embeddings.append(emb.float())
                        else:
                            # Convert numpy array to tensor
                            tensor_embeddings.append(torch.tensor(emb, dtype=torch.float))
                    
                    # Stack all tensors
                    x = torch.stack(tensor_embeddings)
                except Exception as e:
                    logger.error(f"Error converting embeddings to tensor for sample {i}: {str(e)}")
                    continue
            else:
                logger.warning(f"No processed embeddings for sample {i}")
                continue
                
            # Create edges based on specified type
            if edge_type == 'window':
                # Create window-based edges
                edge_index = []
                for j in range(len(text)):
                    # Connect to words within window
                    for k in range(max(0, j - window_size), min(len(text), j + window_size + 1)):
                        if j != k:  # Don't connect word to itself
                            edge_index.append([j, k])
                
                if not edge_index:  # Check if edge_index is empty
                    logger.warning(f"No edges created for sample {i} with {len(text)} words")
                    continue
                
                edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
                
            elif edge_type == 'fully_connected':
                # Create fully connected edges
                edge_index = []
                for j in range(len(text)):
                    for k in range(len(text)):
                        if j != k:  # Don't connect word to itself
                            edge_index.append([j, k])
                
                if not edge_index:  # Check if edge_index is empty
                    logger.warning(f"No edges created for sample {i} with {len(text)} words")
                    continue
                
                edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
            
            else:
                raise ValueError(f"Unsupported edge type: {edge_type}")
                
            # Create graph data object
            graph_data = Data(
                x=x,
                edge_index=edge_index,
                y=torch.tensor([label], dtype=torch.long),
                text=text,
                sentence_embedding=torch.tensor(sentence_embedding, dtype=torch.float)
            )
            
            graphs.append(graph_data)
            
        except Exception as e:
            logger.error(f"Error creating graph for sample {i}: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
    
    logger.info(f"Created {len(graphs)} graphs successfully")
    return graphs

def process_embeddings_batch(batch_word_embeddings, batch_sentence_embeddings, batch_texts, batch_labels, config, split_output_dir, batch_idx, special_embeddings=None):
    """Process a single batch of embeddings"""
    # Create graph structures for this batch
    batch_graphs = create_word_graphs(
        batch_word_embeddings,
        batch_sentence_embeddings,
        batch_texts,
        batch_labels,
        special_embeddings=special_embeddings,
        window_size=config['window_size'],
        edge_type=config['edge_type']
    )
    
    # Save batch graphs with consistent numerical naming to preserve original order
    # Format with leading zeros to ensure correct sorting
    batch_graphs_path = os.path.join(split_output_dir, f"graphs_{config['edge_type']}_batch_{batch_idx:04d}.pkl")
    with open(batch_graphs_path, 'wb') as f:
        pkl.dump(batch_graphs, f)
    
    logger.info(f"Saved batch {batch_idx+1} with {len(batch_graphs)} graphs to {batch_graphs_path}")
    logger.info(f"Original chunk index: {batch_idx} - This preserves the original data order")
    
    # Clear memory
    del batch_word_embeddings, batch_sentence_embeddings, batch_texts, batch_labels, batch_graphs
    
    # Force garbage collection
    import gc
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    return batch_graphs_path

def process_embeddings(config):
    """Process embeddings and create graph structures"""
    # Set paths
    embeddings_dir = os.path.join(config['embeddings_dir'], config['dataset_name'].replace('/', '_'))
    output_dir = os.path.join(config['output_dir'], config['dataset_name'].replace('/', '_'))
    os.makedirs(output_dir, exist_ok=True)
    
    # Process each split
    for split in ['train', 'validation', 'test']:
        # Check if this split exists
        split_dir = os.path.join(embeddings_dir, split)
        if not os.path.exists(split_dir):
            logger.warning(f"Split directory not found: {split_dir}")
            continue
        
        logger.info(f"Processing {split} split")
        
        # Create output directory for this split
        split_output_dir = os.path.join(output_dir, split)
        os.makedirs(split_output_dir, exist_ok=True)
        
        # Get dataset info
        dataset_info = get_dataset_info(embeddings_dir, split)
        logger.info(f"Dataset info: {dataset_info}")
        
        batch_paths = []
        
        # Get the split directory
        split_dir = dataset_info['split_dir']
        
        # Load embeddings
        try:
            logger.info(f"Loading embeddings from {split_dir}")
            word_embeddings, sentence_embeddings, special_embeddings, texts, labels = load_embeddings(split_dir)
            
            # Get total samples
            total_samples = len(texts)
            if total_samples == 0:
                logger.error(f"No samples found in {split} split")
                continue
            
            # Process in small batches to manage memory
            batch_size = config.get('batch_size', 10)  # Default batch size of 10
            num_batches = (total_samples + batch_size - 1) // batch_size  # Ceiling division
            
            logger.info(f"Processing {total_samples} samples in {num_batches} batches of size {batch_size}")
            
            # Process in batches
            for batch_idx in range(num_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, total_samples)
                
                logger.info(f"Processing batch {batch_idx+1}/{num_batches} (samples {start_idx} to {end_idx-1})")
                
                # Extract batch
                batch_word_embeddings = word_embeddings[start_idx:end_idx]
                batch_sentence_embeddings = sentence_embeddings[start_idx:end_idx]
                batch_texts = texts[start_idx:end_idx]
                batch_labels = labels[start_idx:end_idx]
                
                # Process this batch
                batch_path = process_embeddings_batch(
                    batch_word_embeddings,
                    batch_sentence_embeddings,
                    batch_texts,
                    batch_labels,
                    config,
                    split_output_dir,
                    batch_idx,
                    special_embeddings=special_embeddings
                )
                
                batch_paths.append(batch_path)
                
                # Clear memory after each batch
                del batch_word_embeddings, batch_sentence_embeddings, batch_texts, batch_labels
                
                # Force garbage collection
                import gc
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
        except Exception as e:
            logger.error(f"Error processing embeddings from {split_dir}: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
                    
        # Save batch paths for this split
        split_info_path = os.path.join(split_output_dir, 'batch_paths.pkl')
        with open(split_info_path, 'wb') as f:
            pkl.dump(batch_paths, f)
        
        logger.info(f"Saved {len(batch_paths)} batch paths to {split_info_path}")
    
    logger.info("Finished processing all splits")
    logger.info(f"Graph processing completed for {config['dataset_name']}")
    return output_dir

def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description='Process embeddings and create graph structures for GNN')
    
    # Required arguments
    parser.add_argument('--dataset_name', type=str, required=True, help='Dataset name (e.g., stanfordnlp/sst2, setfit/ag_news)')
    parser.add_argument('--embeddings_dir', type=str, required=True, help='Directory containing the embeddings')
    
    # Optional arguments
    parser.add_argument('--window_size', type=int, default=3, help='Window size for creating edges')
    parser.add_argument('--edge_type', type=str, default='window', choices=['window', 'fully_connected'], help='Type of edges to create')
    parser.add_argument('--output_dir', type=str, default='/app/src/Clean_Code/output/gnn_graphs', help='Output directory')
    parser.add_argument('--batch_size', type=int, default=25, help='Batch size for processing embeddings (smaller values use less memory)')
    
    return parser.parse_args()

def main():
    """Main entry point"""
    # Parse arguments
    args = parse_args()
    
    # Create config from arguments
    config = {
        'dataset_name': args.dataset_name,
        'embeddings_dir': args.embeddings_dir,
        'window_size': args.window_size,
        'edge_type': args.edge_type,
        'output_dir': args.output_dir,
        'batch_size': args.batch_size
    }
    
    # Process embeddings
    process_embeddings(config)

if __name__ == "__main__":
    main()
