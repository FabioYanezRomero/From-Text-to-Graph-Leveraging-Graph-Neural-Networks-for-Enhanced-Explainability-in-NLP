"""
Graph Data Processor

This module processes word embeddings and prepares them for use in Graph Neural Networks.
It creates graph structures from text data using the embeddings generated by the embedding_generator.py.
"""

import os
import sys
import argparse
import json
import pickle as pkl
import logging
import numpy as np
from tqdm import tqdm
import torch
import torch.nn.functional as F
import torch_geometric

# Set up device for PyTorch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)
logger.info(f"Using device: {device}")
from torch_geometric.data import Data
import networkx as nx

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

def load_embeddings(input_dir, dataset, embedding_model, split):
    """Load word and sentence embeddings from disk, handling chunked data structure.
    
    Args:
        input_dir: Base directory containing the input embeddings
        dataset: Name of the dataset
        embedding_model: Name of the embedding model used
        split: Data split ('train', 'val', 'test')
        
    Returns:
        tuple: (texts, labels, word_embeddings, sentence_embeddings)
    """
    import os
    import pickle
    import numpy as np
    from glob import glob
    
    # Construct base path
    base_path = os.path.join(input_dir, dataset, split)
    
    # Find all chunk directories
    chunk_dirs = sorted([d for d in glob(os.path.join(base_path, 'chunk_*')) if os.path.isdir(d)])
    
    if not chunk_dirs:
        logger.error(f"No chunk directories found in {base_path}")
        return None
    
    all_word_embeddings = []
    all_sentence_embeddings = []
    all_texts = []
    all_labels = []
    
    logger.info(f"Found {len(chunk_dirs)} chunks in {base_path}")
    
    try:
        # Process each chunk
        for chunk_dir in chunk_dirs:
            try:
                # Load data from chunk
                word_embeddings_path = os.path.join(chunk_dir, 'word_embeddings.pkl')
                sentence_embeddings_path = os.path.join(chunk_dir, 'sentence_embeddings.pkl')
                metadata_path = os.path.join(chunk_dir, 'metadata.pkl')
                
                # Check if required files exist
                if not all(os.path.exists(p) for p in [word_embeddings_path, sentence_embeddings_path, metadata_path]):
                    logger.warning(f"Skipping incomplete chunk: {chunk_dir}")
                    continue
                
                # Load metadata which contains texts and labels
                with open(metadata_path, 'rb') as f:
                    metadata = pickle.load(f)
                
                # Load word and sentence embeddings
                with open(word_embeddings_path, 'rb') as f:
                    word_embeddings = pickle.load(f)
                with open(sentence_embeddings_path, 'rb') as f:
                    sentence_embeddings = pickle.load(f)
                
                # Extract texts and labels from metadata
                texts = [item['text'] for item in metadata]
                labels = [item['label'] for item in metadata]
                
                # Convert to lists if they're numpy arrays
                if isinstance(word_embeddings, np.ndarray):
                    word_embeddings = list(word_embeddings)
                if isinstance(sentence_embeddings, np.ndarray):
                    sentence_embeddings = list(sentence_embeddings)
                if isinstance(texts, np.ndarray):
                    texts = list(texts)
                if isinstance(labels, np.ndarray):
                    labels = list(labels)
                
                # Add to the combined lists
                all_word_embeddings.extend(word_embeddings)
                all_sentence_embeddings.extend(sentence_embeddings)
                all_texts.extend(texts)
                all_labels.extend(labels)
                
                logger.info(f"Loaded {len(word_embeddings)} samples from {os.path.basename(chunk_dir)}")
                
            except Exception as e:
                logger.error(f"Error loading chunk {chunk_dir}: {str(e)}")
                import traceback
                logger.error(traceback.format_exc())
        
        if not all_word_embeddings:
            logger.error("No data was loaded from any chunk")
            return None
            
        logger.info(f"Successfully loaded {len(all_word_embeddings)} total samples from {len(chunk_dirs)} chunks")
        return all_texts, all_labels, all_word_embeddings, all_sentence_embeddings
        
    except Exception as e:
        logger.error(f"Error in load_embeddings: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return None

def save_graphs(graphs, output_dir, batch_size=32, num_workers=4):
    """Save a list of PyTorch Geometric Data objects to disk.
    
    Args:
        graphs: List of PyTorch Geometric Data objects
        output_dir: Directory to save the graphs
        batch_size: Number of graphs to save in each file
        num_workers: Number of worker processes to use
    """
    import os
    import torch
    from torch_geometric.data import Data, Batch
    from torch_geometric.loader import DataLoader
    
    os.makedirs(output_dir, exist_ok=True)
    
    # Create a DataLoader to handle batching
    loader = DataLoader(graphs, batch_size=batch_size, shuffle=False, num_workers=num_workers)
    
    # Save each batch
    for batch_idx, batch in enumerate(loader):
        batch_path = os.path.join(output_dir, f'batch_{batch_idx:04d}.pt')
        try:
            torch.save(batch, batch_path)
            logger.info(f"Saved batch {batch_idx} with {len(batch)} graphs to {batch_path}")
        except Exception as e:
            logger.error(f"Error saving batch {batch_idx}: {str(e)}")
    
    logger.info(f"Saved {len(graphs)} graphs in {len(loader)} batches to {output_dir}")

def get_dataset_info(embeddings_dir):
    """Get information about the dataset without loading all embeddings
    
    Args:
        embeddings_dir: Directory containing embeddings
        
    Returns:
        dataset_info: Dictionary containing dataset information
        is_chunked: Whether embeddings are stored in chunks
    """
    # First, check if we need to look in a nested directory structure
    metadata_path = os.path.join(embeddings_dir, 'metadata.json')
    
    # If metadata.json is not found directly, look for subdirectories that might contain it
    if not os.path.exists(metadata_path):
        subdirs = [d for d in os.listdir(embeddings_dir) if os.path.isdir(os.path.join(embeddings_dir, d))]
        
        for subdir in subdirs:
            nested_path = os.path.join(embeddings_dir, subdir, 'metadata.json')
            if os.path.exists(nested_path):
                logger.info(f"Found metadata in nested directory: {nested_path}")
                metadata_path = nested_path
                embeddings_dir = os.path.join(embeddings_dir, subdir)
                break
    
    # Check if embeddings are stored in chunks
    embedding_chunks_dir = os.path.join(embeddings_dir, 'embedding_chunks')
    is_chunked = os.path.exists(embedding_chunks_dir)
    
    if not os.path.exists(metadata_path):
        raise FileNotFoundError(f"Metadata file not found in {embeddings_dir} or its subdirectories")
    
    with open(metadata_path, 'r') as f:
        metadata = json.load(f)
    
    if is_chunked:
        # Get list of chunk files
        chunk_files = sorted([f for f in os.listdir(embedding_chunks_dir) if f.endswith('.pkl')])
        
        dataset_info = {
            'total_samples': metadata.get('num_samples', 0),
            'chunk_dirs': [embedding_chunks_dir],
            'num_chunks': len(chunk_files)
        }
    else:
        dataset_info = {
            'total_samples': metadata.get('num_samples', 0)
        }
    
    return dataset_info, is_chunked

def load_chunk_from_disk(chunk_dir):
    """Load a chunk of embeddings from disk
    
    Args:
        chunk_dir: Directory containing chunk data
        
    Returns:
        word_embeddings, sentence_embeddings, texts, labels
    """
    # Get all chunk files
    chunk_files = [f for f in os.listdir(chunk_dir) if 
                 (f.endswith('.pkl') and f.startswith('chunk_')) or 
                 (f.endswith('.npz') and f.startswith('embeddings_chunk_'))]
    
    def extract_chunk_number(filename):
        if filename.startswith('chunk_'):
            return int(filename.replace('chunk_', '').replace('.pkl', ''))
        elif filename.startswith('embeddings_chunk_'):
            return int(filename.replace('embeddings_chunk_', '').replace('.npz', ''))
    
    # Sort chunk files numerically by their index
    chunk_files = sorted(chunk_files, key=extract_chunk_number)
    
    # Process only one chunk file at a time to save memory
    if not chunk_files:
        logger.warning(f"No chunk files found in {chunk_dir}")
        return [], [], [], []
    
    # Load the first chunk file to get data
    chunk_file = chunk_files[0]
    chunk_path = os.path.join(chunk_dir, chunk_file)
    
    word_embeddings = []
    sentence_embeddings = []
    texts = []
    labels = []
    
    if chunk_file.endswith('.pkl'):
        # Load pickle file
        with open(chunk_path, 'rb') as f:
            chunk_data = pkl.load(f)
        
        # Extract data
        if isinstance(chunk_data, dict):
            word_embeddings = chunk_data.get('word_embeddings', [])
            sentence_embeddings = chunk_data.get('sentence_embeddings', [])
            texts = chunk_data.get('texts', [])
            labels = chunk_data.get('labels', [])
        elif isinstance(chunk_data, list) and len(chunk_data) == 4:
            word_embeddings, sentence_embeddings, texts, labels = chunk_data
    
    elif chunk_file.endswith('.npz'):
        # Load numpy file
        chunk_data = np.load(chunk_path, allow_pickle=True)
        
        # Extract data
        if 'word_embeddings' in chunk_data:
            word_embeddings = chunk_data['word_embeddings']
        if 'sentence_embeddings' in chunk_data:
            sentence_embeddings = chunk_data['sentence_embeddings']
        if 'texts' in chunk_data:
            texts = chunk_data['texts']
        if 'labels' in chunk_data:
            labels = chunk_data['labels']
    
    logger.info(f"Loaded {len(word_embeddings)} samples from {chunk_file}")
    # If labels are missing, create dummy labels
    if ((isinstance(labels, list) and not labels) or 
        (isinstance(labels, np.ndarray) and len(labels) == 0)) and \
       word_embeddings is not None and len(word_embeddings) > 0:
        logger.warning(f"No labels found in chunks, creating dummy labels")
        labels = [0] * len(word_embeddings)
    
    return word_embeddings, sentence_embeddings, texts, labels

def load_special_embeddings(embeddings_dir):
    """Load special embeddings for constituency tokens
    
    Args:
        embeddings_dir: Directory containing embeddings
        
    Returns:
        special_embeddings: Dictionary of special embeddings
    """
    import os
    from glob import glob
    
    # Check if special embeddings exist in main directory
    special_embeddings_path = os.path.join(embeddings_dir, 'special_embeddings.pkl')
    
    if os.path.exists(special_embeddings_path):
        logger.info(f"Found special embeddings at: {special_embeddings_path}")
        with open(special_embeddings_path, 'rb') as f:
            special_embeddings = pkl.load(f)
        return special_embeddings
    
    # Check if special embeddings exist in special directory
    special_dir = os.path.join(os.path.dirname(embeddings_dir), 'special')
    special_embeddings_path = os.path.join(special_dir, 'special_embeddings.pkl')
    
    if os.path.exists(special_embeddings_path):
        logger.info(f"Found special embeddings at: {special_embeddings_path}")
        with open(special_embeddings_path, 'rb') as f:
            special_embeddings = pkl.load(f)
        return special_embeddings
    
    # Search recursively for special_embeddings.pkl in all subdirectories
    # Use glob to find all special_embeddings.pkl files under the embeddings_dir
    special_embeddings_files = glob(os.path.join(embeddings_dir, '**', 'special_embeddings.pkl'), recursive=True)
    
    if special_embeddings_files:
        # Use the first found file
        special_embeddings_path = special_embeddings_files[0]
        logger.info(f"Found special embeddings through recursive search: {special_embeddings_path}")
        with open(special_embeddings_path, 'rb') as f:
            special_embeddings = pkl.load(f)
        return special_embeddings
    
    # If still not found, try to look in split directories based on dataset
    # Extract dataset name from embeddings_dir
    dataset_name = os.path.basename(os.path.dirname(embeddings_dir))
    
    # Define appropriate splits based on dataset
    if 'ag_news' in dataset_name or 'ag-news' in dataset_name:
        splits = ['train', 'test']
        logger.info(f"Using train and test splits for AG News dataset")
    elif 'sst-2' in dataset_name or 'sst2' in dataset_name:
        splits = ['train', 'validation']
        logger.info(f"Using train and validation splits for SST-2 dataset")
    else:
        # Default to all possible splits
        splits = ['train', 'validation', 'test', 'val']
        logger.info(f"Using all possible splits for unknown dataset: {dataset_name}")
    
    for split in splits:
        split_dir = os.path.join(embeddings_dir, split)
        if os.path.isdir(split_dir):
            special_embeddings_files = glob(os.path.join(split_dir, '**', 'special_embeddings.pkl'), recursive=True)
            if special_embeddings_files:
                special_embeddings_path = special_embeddings_files[0]
                logger.info(f"Found special embeddings in {split} split: {special_embeddings_path}")
                with open(special_embeddings_path, 'rb') as f:
                    special_embeddings = pkl.load(f)
                return special_embeddings
    
    logger.warning(f"No special embeddings found in {embeddings_dir} or its subdirectories")
    return None

def load_constituency_tree(tree_path):
    """Load a pre-generated constituency tree from file
    
    Args:
        tree_path: Path to the pickle file containing the tree
        
    Returns:
        The loaded tree object
    """
    import pickle
    with open(tree_path, 'rb') as f:
        return pickle.load(f)

def extract_tree_structure(tree):
    """Extract node features and edge indices from a constituency tree
    
    Args:
        tree: The constituency tree object
        
    Returns:
        tuple: (node_features, edge_indices)
    """
    import networkx as nx
    
    # Convert tree to networkx graph
    G = nx.Graph()
    
    # We'll use a queue for BFS traversal
    from collections import deque
    queue = deque([(tree, 0)])  # (node, parent_id)
    node_id = 0
    
    # Dictionaries to store node information
    node_features = {}
    edge_indices = []
    
    while queue:
        node, parent_id = queue.popleft()
        current_id = node_id
        node_id += 1
        
        # Store node feature (using label or other identifier)
        if hasattr(node, 'label'):
            node_features[current_id] = node.label[0] if isinstance(node.label, list) else node.label
        
        # Add edge to parent if not root
        if parent_id != -1:
            edge_indices.append((parent_id, current_id))
        
        # Add children to queue
        if hasattr(node, 'children'):
            for child in node.children:
                queue.append((child, current_id))

    return node_features, edge_indices

def create_word_graphs(word_embeddings, sentence_embeddings, texts, labels, dataset_name, split='train', edge_type='constituency', show_progress=False):
    """Create word graphs from word embeddings and constituency trees

    Args:
        word_embeddings: List of word embeddings
        sentence_embeddings: List of sentence embeddings
        texts: List of texts
        labels: List of labels
        dataset_name: Name of the dataset
        split: Split name
        edge_type: Type of edges to use

    Returns:
        List of graphs
    """
    # Extract dataset provider and name
    if '/' in dataset_name:
        provider, name = dataset_name.split('/', 1)
    else:
        # If no provider is specified, try to determine from the dataset name
        if dataset_name == 'stanfordnlp':
            provider, name = 'stanfordnlp', 'sst2'
        else:
            provider, name = 'stanfordnlp', dataset_name

    # Handle case sensitivity in provider names by checking actual directory names
    text_graphs_dir = '/app/src/Clean_Code/output/text_graphs'
    if os.path.exists(text_graphs_dir):
        for dir_name in os.listdir(text_graphs_dir):
            if dir_name.lower() == provider.lower():
                provider = dir_name  # Use the actual directory name with correct capitalization
                break

    logger.debug(f"Dataset name received in create_word_graphs: {dataset_name}")
    logger.debug(f"Provider: {provider}, Name: {name}")

    base_dir = os.path.join(text_graphs_dir, provider, name, split, 'constituency')
    logger.debug(f"Looking for constituency trees in: {base_dir}")

    if not os.path.exists(base_dir):
        logger.warning(f"Constituency tree directory not found: {base_dir}. No graphs will be created.")
        return []

    logger.debug(f"Creating graphs from pre-generated constituency trees in {base_dir}")
    logger.debug(f"Input sizes: {len(word_embeddings)} word embeddings, {len(sentence_embeddings)} sentence embeddings, {len(texts)} texts, {len(labels)} labels")
    
    graphs = []
    
    # Get list of all tree files
    tree_files = sorted([f for f in os.listdir(base_dir) if f.endswith('.pkl')], 
                        key=lambda x: int(x.split('.')[0]) if x.split('.')[0].isdigit() else float('inf'))
    
    # Process each sample with optional progress bar
    range_iter = tqdm(range(len(word_embeddings)), desc=f"Creating {split} graphs", unit="graph", position=0, leave=True, colour='green') if show_progress else range(len(word_embeddings))
    for i in range_iter:
        try:
            # Try different tree file naming patterns
            tree_file = os.path.join(base_dir, f"tree_{i}.pkl")
            simple_tree_file = os.path.join(base_dir, f"{i}.pkl")
            
            # Check if any tree file pattern exists
            if os.path.exists(tree_file):
                tree_path = tree_file
            elif os.path.exists(simple_tree_file):
                tree_path = simple_tree_file
            else:
                logger.warning(f"No tree file found for sample {i}")
                continue
            
            # Load the tree
            tree = load_constituency_tree(tree_path)
            
            # Get the corresponding embeddings and label
            word_embedding = word_embeddings[i] if i < len(word_embeddings) else None
            sentence_embedding = sentence_embeddings[i] if i < len(sentence_embeddings) else None
            label = labels[i] if i < len(labels) else None
            text = texts[i] if i < len(texts) else None
            
            # Handle label tensor
            if label is not None and isinstance(label, torch.Tensor):
                if label.numel() > 1:
                    label = label[0].item()
                else:
                    label = label.item()
            
            # Extract tree structure
            node_features, edge_indices = extract_tree_structure(tree)
            
            if not edge_indices:
                logger.warning(f"No edges extracted from tree for sample {i}")
                continue
            
            # Create the graph
            graph_data = create_graph_from_tree(
                node_features, 
                edge_indices, 
                word_embedding, 
                sentence_embedding, 
                text, 
                label
            )
            
            graphs.append(graph_data)
            
        except Exception as e:
            logger.error(f"Error processing sample {i}: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
    
    logger.debug(f"Created {len(graphs)} graphs successfully")
    return graphs

def create_graph_from_tree(node_features, edge_indices, word_embeddings, sentence_embedding, text, label):
    import torch
    from torch_geometric.data import Data
    
    # Convert edge indices to tensor and move to appropriate device
    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous().to(device)
    
    # Process node features (embeddings)
    num_nodes = len(node_features)
    
    # Initialize node features tensor
    if word_embeddings and len(word_embeddings) > 0:
        # Use the first embedding to determine feature dimension
        feature_dim = word_embeddings[0].shape[0] if hasattr(word_embeddings[0], 'shape') else len(word_embeddings[0])
    else:
        # Fallback to sentence embedding dimension
        feature_dim = sentence_embedding.shape[0] if sentence_embedding is not None else 768
    
    x = torch.zeros((num_nodes, feature_dim), dtype=torch.float, device=device)
    
    # Assign word embeddings to leaf nodes
    leaf_count = 0
    for node_id, node_feature in node_features.items():
        # Check if this is a leaf node (no outgoing edges where this is the parent)
        is_leaf = not any(p == node_id for p, _ in edge_indices)
        if is_leaf and word_embeddings and leaf_count < len(word_embeddings):
            x[node_id] = word_embeddings[leaf_count]
            leaf_count += 1
    
    # For non-leaf nodes, use the mean of their children's embeddings
    # Process nodes in reverse order (leaves to root)
    for node_id in reversed(range(num_nodes)):
        children = [c for p, c in edge_indices if p == node_id]
        if children:
            child_embeddings = x[children]
            x[node_id] = child_embeddings.mean(dim=0)
    
    # Create graph data object
    graph_data = Data(
        x=x,
        edge_index=edge_index,
        y=torch.tensor([label], dtype=torch.long, device=device) if label is not None else None,
        text=text,
        sentence_embedding=torch.tensor(sentence_embedding, dtype=torch.float, device=device) if sentence_embedding is not None else None
    )
    
    return graph_data

def process_embeddings_batch(batch_word_embeddings, batch_sentence_embeddings, batch_texts, batch_labels, config, split_output_dir, batch_idx, special_embeddings=None):
    """Process a single batch of embeddings"""
    # Log label source
    label_source = config.get('label_source', 'original')
    if label_source == 'llm':
        logger.debug(f"Using LLM predictions as labels for batch {batch_idx}")
    
    # Extract dataset name from the path
    # We need to use the original dataset_name passed from process_embeddings
    # Instead of trying to extract it from the split_output_dir which might be incorrect
    
    # Get the dataset name from the config
    dataset_name = config.get('dataset_name', 'stanfordnlp/sst2')
    
    # Log the dataset name we're using
    logger.debug(f"Using dataset name: {dataset_name} for graph creation")

    
    # Extract split from the path
    split = os.path.basename(split_output_dir).replace('_llm_labels', '')
    
    # Create graphs
    batch_graphs = create_word_graphs(
        batch_word_embeddings,
        batch_sentence_embeddings,
        batch_texts,
        batch_labels,
        dataset_name,
        split,
        config['edge_type'],
        show_progress=False  # Disable progress bar for batch processing
    )
    
    # Save batch graphs with consistent numerical naming to preserve original order
    # Format with leading zeros to ensure correct sorting
    batch_graphs_path = os.path.join(split_output_dir, f"graphs_{config['edge_type']}_batch_{batch_idx:04d}.pkl")
    
    with open(batch_graphs_path, 'wb') as f:
        pkl.dump(batch_graphs, f)
    
    logger.debug(f"Saved {len(batch_graphs)} graphs to {batch_graphs_path}")
    
    return len(batch_graphs)

def load_llm_predictions(predictions_file, split='test'):
    """Load LLM predictions from a JSON file using the best epoch
    
    Args:
        predictions_file: Path to the JSON file containing LLM predictions
        split: Data split to load predictions for ('train', 'validation', 'test')
        
    Returns:
        Dictionary mapping data index to predicted label
    """
    # Log the function call for debugging
    logger.debug(f"Loading LLM predictions from {predictions_file} for split {split}")
    import json
    import os
    import sys
    
    # Map split names to match those in the predictions file
    split_map = {
        'train': 'train',
        'validation': 'validation',
        'test': 'test'
    }
    
    split_name = split_map.get(split, split)
    
    try:
        # Get the model directory from the predictions file path
        model_dir = os.path.dirname(predictions_file)
        
        # Find the best epoch by analyzing the classification reports
        best_epoch = find_best_epoch(model_dir)
        
        if best_epoch is None:
            logger.warning(f"Could not determine best epoch, using latest epoch from predictions")
            # Fallback to the latest epoch if best epoch cannot be determined
            with open(predictions_file, 'r') as f:
                predictions_data = json.load(f)
            
            max_epoch = 0
            for item in predictions_data:
                if item.get('dataset') == split_name and item.get('epoch', 0) > max_epoch:
                    max_epoch = item.get('epoch', 0)
            best_epoch = max_epoch
        
        # Check if there's an epoch numbering mismatch between classification reports and predictions
        # Load all predictions to check epoch numbering
        with open(predictions_file, 'r') as f:
            predictions_data = json.load(f)
        
        # Get the unique epochs in the predictions file
        pred_epochs = set(item.get('epoch') for item in predictions_data if item.get('epoch') is not None)
        logger.info(f"Epochs in predictions file: {sorted(pred_epochs)}")
        
        # Check if the best_epoch exists in the predictions file
        if best_epoch not in pred_epochs:
            logger.warning(f"Best epoch {best_epoch} not found in predictions file epochs {sorted(pred_epochs)}")
            
            # Check if there's a 0/1-based indexing mismatch
            if best_epoch - 1 in pred_epochs:
                logger.info(f"Found epoch {best_epoch-1} in predictions file, likely a 0/1-based indexing mismatch")
                best_epoch = best_epoch - 1
            elif best_epoch + 1 in pred_epochs:
                logger.info(f"Found epoch {best_epoch+1} in predictions file, likely a 0/1-based indexing mismatch")
                best_epoch = best_epoch + 1
            else:
                # If best epoch is not found at all, use the latest available epoch
                if pred_epochs:
                    latest_epoch = max(pred_epochs)
                    logger.info(f"Using latest available epoch {latest_epoch} from predictions file")
                    best_epoch = latest_epoch
        
        # Load predictions
        with open(predictions_file, 'r') as f:
            predictions_data = json.load(f)
        
        # Create a dictionary mapping data index to predicted label for the best epoch
        predictions = {}
        matched_epoch_count = 0
        
        for item in predictions_data:
            if item.get('dataset') == split_name and item.get('epoch') == best_epoch:
                predictions[item.get('data_index')] = item.get('predicted_label')
                matched_epoch_count += 1
        
        # If we didn't find any predictions for the best epoch, try adjacent epochs
        if matched_epoch_count == 0:
            logger.warning(f"No predictions found for epoch {best_epoch}, trying adjacent epochs")
            
            # Try epochs +/- 1 from the best epoch
            for offset in [-1, 1]:
                test_epoch = best_epoch + offset
                test_count = 0
                
                for item in predictions_data:
                    if item.get('dataset') == split_name and item.get('epoch') == test_epoch:
                        test_count += 1
                
                if test_count > 0:
                    logger.info(f"Found {test_count} predictions for epoch {test_epoch}, using this instead")
                    
                    # Use this epoch instead
                    for item in predictions_data:
                        if item.get('dataset') == split_name and item.get('epoch') == test_epoch:
                            predictions[item.get('data_index')] = item.get('predicted_label')
                    
                    best_epoch = test_epoch
                    break
        
        # Log some sample predictions for debugging
        sample_items = list(predictions.items())[:5]
        logger.debug(f"Sample predictions (data_index: predicted_label): {sample_items}")
        
        # Log statistics about the predictions
        logger.debug(f"Loaded {len(predictions)} LLM predictions for {split_name} split from best epoch {best_epoch}")
        
        # Log the range of data indices
        if predictions:
            min_idx = min(predictions.keys())
            max_idx = max(predictions.keys())
            logger.debug(f"Data index range: {min_idx} to {max_idx}")
        
        return predictions
    
    except Exception as e:
        logger.error(f"Error loading LLM predictions: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return {}

def find_best_epoch(model_dir, metric='f1-score', split='validation'):
    """Find the best epoch based on validation F1 score
    
    Args:
        model_dir: Directory containing model checkpoints and classification reports
        metric: Metric to use for selecting the best epoch (default: 'f1-score')
        split: Data split to use for evaluation (default: 'validation')
        
    Returns:
        Best epoch number or None if it cannot be determined
    """
    import glob
    import json
    import os
    
    # Find all classification reports for the specified split
    report_files = glob.glob(os.path.join(model_dir, f"classification_report_{split}_epoch*.json"))
    
    # If no validation reports are found, try test reports
    if not report_files and split == 'validation':
        logger.warning(f"No classification reports found for validation split, trying test split instead")
        report_files = glob.glob(os.path.join(model_dir, f"classification_report_test_epoch*.json"))
        split = 'test'
    
    if not report_files:
        logger.warning(f"No classification reports found in {model_dir} for split {split}")
        return None
    
    best_score = -1
    best_epoch = None
    
    # Iterate through all epochs
    for report_file in report_files:
        try:
            with open(report_file, 'r') as f:
                report = json.load(f)
            
            # Extract epoch number from filename
            epoch = int(os.path.basename(report_file).split('_')[-1].split('.')[0].replace('epoch', ''))
            
            # For multi-class classification, use macro avg F1-score
            # For binary classification, use weighted avg F1-score
            if 'macro avg' in report and metric in report['macro avg']:
                score = report['macro avg'][metric]
            elif 'weighted avg' in report and metric in report['weighted avg']:
                score = report['weighted avg'][metric]
            else:
                logger.warning(f"Metric {metric} not found in report {report_file}")
                continue
            
            logger.debug(f"Epoch {epoch}: {metric} = {score:.4f}")
            
            # Check if this is the best score so far
            if score > best_score:
                best_score = score
                best_epoch = epoch
        
        except Exception as e:
            logger.error(f"Error processing report file {report_file}: {e}")
            continue
    
    if best_epoch is not None:
        logger.info(f"Best {metric}: {best_score:.4f} (Epoch {best_epoch})")
    
    return best_epoch

def process_embeddings(dataset_name, embeddings_dir, batch_size=200, edge_type='constituency', label_source='original', llm_predictions=None):
    """Process embeddings to create graph structures
    
    Args:
        dataset_name: Name of the dataset
        embeddings_dir: Directory containing embeddings
        batch_size: Batch size for processing
        edge_type: Type of edges to create (constituency)
        label_source: Source of labels ('original' or 'llm')
        llm_predictions: Path to LLM predictions JSON file (required if label_source is 'llm')
        
    Returns:
        output_dir: Directory containing the processed graphs
    """
    # Create output directory
    output_dir = os.path.join(os.path.dirname(os.path.dirname(embeddings_dir)), 'graphs', dataset_name)
    os.makedirs(output_dir, exist_ok=True)
    
    # Load special embeddings
    special_embeddings = load_special_embeddings(embeddings_dir)
    
    # Process each split with enhanced progress bar
    for split in tqdm(['train', 'validation', 'test'], desc="Processing splits", unit="split", position=0, leave=True, colour='blue'):
        split_dir = os.path.join(embeddings_dir, split)
        
        if not os.path.exists(split_dir):
            logger.warning(f"Split directory {split_dir} does not exist, skipping")
            continue
        
        # Create output directory for this split
        # If using LLM predictions, create a separate directory
        if label_source == 'llm':
            split_output_dir = os.path.join(output_dir, f"{split}_llm_labels")
        else:
            split_output_dir = os.path.join(output_dir, split)
            
        os.makedirs(split_output_dir, exist_ok=True)
        
        # Load LLM predictions if needed
        llm_pred_dict = None
        if label_source == 'llm' and llm_predictions:
            logger.info(f"Loading LLM predictions for {split} split from {llm_predictions}")
            if os.path.exists(llm_predictions):
                llm_pred_dict = load_llm_predictions(llm_predictions, split)
                if not llm_pred_dict:
                    logger.warning(f"No LLM predictions loaded for {split} split. Will use dummy labels.")
            else:
                logger.error(f"LLM predictions file not found: {llm_predictions}")
        
        # Get dataset information
        try:
            dataset_info, is_chunked = get_dataset_info(split_dir)
            logger.info(f"Processing {dataset_info['total_samples']} samples for {split} split")
        except Exception as e:
            logger.error(f"Error getting dataset information for {split_dir}: {str(e)}")
            continue
        
        # Save configuration
        config = {
            'edge_type': edge_type,
            'batch_size': batch_size,
            'total_samples': dataset_info['total_samples'],
            'label_source': label_source,
            'dataset_name': dataset_name
        }
        
        config_path = os.path.join(split_output_dir, 'config.json')
        with open(config_path, 'w') as f:
            json.dump(config, f)
        
        # Process embeddings
        if is_chunked:
            # Process each chunk
            chunks_dir = os.path.join(split_dir, 'chunks')
            total_graphs = 0
            
            # Process one chunk at a time to manage memory with enhanced progress tracking
            total_chunks = len(dataset_info['chunk_dirs'])
            chunk_progress = tqdm(dataset_info['chunk_dirs'], desc=f"Processing chunks for {split}", unit="chunk", 
                                  position=1, leave=False, colour='cyan', total=total_chunks)
            for i, chunk_dir_name in enumerate(chunk_progress):
                chunk_dir = os.path.join(chunks_dir, chunk_dir_name)
                chunk_progress.set_postfix({"dir": chunk_dir_name, "progress": f"{i+1}/{total_chunks}"})
                
                try:
                    # Process each chunk file individually
                    chunk_files = [f for f in os.listdir(chunk_dir) if 
                                 (f.endswith('.pkl') and f.startswith('chunk_')) or 
                                 (f.endswith('.npz') and f.startswith('embeddings_chunk_'))]
                    
                    # Sort chunk files numerically
                    def extract_chunk_number(filename):
                        if filename.startswith('chunk_'):
                            return int(filename.replace('chunk_', '').replace('.pkl', ''))
                        elif filename.startswith('embeddings_chunk_'):
                            return int(filename.replace('embeddings_chunk_', '').replace('.npz', ''))
                    
                    chunk_files = sorted(chunk_files, key=extract_chunk_number)
                    
                    # Process each chunk file individually with enhanced progress tracking
                    total_files = len(chunk_files)
                    file_progress = tqdm(chunk_files, desc=f"Files in chunk {chunk_dir_name}", unit="file", 
                                         position=2, leave=False, colour='yellow', total=total_files)
                    for chunk_idx, chunk_file in enumerate(file_progress):
                        chunk_path = os.path.join(chunk_dir, chunk_file)
                        file_progress.set_postfix({"file": chunk_file, "progress": f"{chunk_idx+1}/{total_files}"})
                        
                        # Load single chunk file
                        try:
                            if chunk_file.endswith('.pkl'):
                                with open(chunk_path, 'rb') as f:
                                    chunk_data = pkl.load(f)
                                
                                # Extract data based on structure
                                if isinstance(chunk_data, dict):
                                    word_embeddings = chunk_data.get('word_embeddings', [])
                                    sentence_embeddings = chunk_data.get('sentence_embeddings', [])
                                    texts = chunk_data.get('texts', [])
                                    labels = chunk_data.get('labels', [])
                                elif isinstance(chunk_data, list) and len(chunk_data) == 4:
                                    word_embeddings, sentence_embeddings, texts, labels = chunk_data
                                else:
                                    logger.warning(f"Unknown chunk data format in {chunk_path}")
                                    continue
                                    
                            elif chunk_file.endswith('.npz'):
                                chunk_data = np.load(chunk_path, allow_pickle=True)
                                
                                # Extract arrays - we don't expect labels in the chunk files
                                word_embeddings = chunk_data['word_embeddings'] if 'word_embeddings' in chunk_data else []
                                sentence_embeddings = chunk_data['sentence_embeddings'] if 'sentence_embeddings' in chunk_data else []
                                texts = chunk_data['texts'] if 'texts' in chunk_data else []
                                
                                # Initialize empty labels - they will be filled from LLM predictions or original dataset
                                labels = []
                            
                            logger.info(f"Loaded {len(word_embeddings)} samples from {chunk_file}")
                            
                            # Calculate number of samples in this chunk
                            chunk_size = len(word_embeddings)
                            
                            # Process in smaller sub-batches with enhanced progress tracking
                            sub_batch_size = batch_size  # Use the full batch size as specified
                            total_batches = int(np.ceil(chunk_size / sub_batch_size))
                            
                            batch_progress = tqdm(range(0, chunk_size, sub_batch_size), 
                                                  desc=f"Batches in {os.path.basename(chunk_file)}", 
                                                  unit="batch", 
                                                  position=3, 
                                                  leave=False, 
                                                  colour='magenta',
                                                  total=total_batches)
                            for j in batch_progress:
                                end_idx = min(j + sub_batch_size, chunk_size)
                                batch_idx_display = j // sub_batch_size + 1
                                batch_progress.set_postfix({
                                    "samples": f"{j}-{end_idx}/{chunk_size}", 
                                    "batch": f"{batch_idx_display}/{total_batches}",
                                    "memory": f"{j/chunk_size:.1%}"
                                })
                                
                                # Extract sub-batch
                                sub_word_embeddings = word_embeddings[j:end_idx]
                                
                                # Handle sentence embeddings safely
                                if sentence_embeddings is not None and len(sentence_embeddings) > 0:
                                    sub_sentence_embeddings = sentence_embeddings[j:end_idx]
                                else:
                                    sub_sentence_embeddings = []
                                
                                # Handle texts safely
                                if texts is not None and len(texts) > 0:
                                    sub_texts = texts[j:end_idx]
                                else:
                                    sub_texts = []
                                
                                # Initialize batch_idx for this sub-batch
                                batch_idx = (i * 1000 + j) // sub_batch_size
                                
                                # Get the correct labels based on the source
                                if label_source == 'llm':
                                    if not llm_pred_dict:
                                        error_msg = "LLM predictions dictionary is empty but LLM label source was specified"
                                        logger.error(error_msg)
                                        raise ValueError(error_msg)
                                        
                                    # Calculate global indices for this batch
                                    sub_labels = []
                                    for idx in range(j, min(end_idx, len(word_embeddings))):
                                        # Calculate global index based on chunk and position
                                        data_idx = sum(len(c) for c in word_embeddings[:i]) + idx
                                        
                                        if data_idx in llm_pred_dict:
                                            sub_labels.append(llm_pred_dict[data_idx])
                                        else:
                                            error_msg = f"No LLM prediction found for data index {data_idx} in chunk {chunk_file}"
                                            logger.error(error_msg)
                                            raise ValueError(error_msg)
                                    
                                    # Log information about the LLM predictions
                                    logger.info(f"Applying LLM predictions to batch {j}-{end_idx} in chunk {i}")
                                    logger.info(f"Number of available LLM predictions: {len(llm_pred_dict)}")
                                    logger.info(f"Found {len(sub_labels)}/{end_idx-j} matches with LLM predictions")
                                else:
                                    # For non-LLM source, use original labels from the dataset
                                    # Try multiple approaches to find the original labels
                                    
                                    # First, check if labels were loaded with the chunk data
                                    if labels is not None and len(labels) > 0:
                                        sub_labels = labels[j:end_idx]
                                        logger.info(f"Using {len(sub_labels)} original labels from chunk data")
                                    else:
                                        # Try to load labels from a labels file if it exists
                                        labels_file = os.path.join(os.path.dirname(chunk_path), f"labels_{i}.json")
                                        labels_npz = os.path.join(os.path.dirname(chunk_path), f"labels_{i}.npz")
                                        
                                        if os.path.exists(labels_file):
                                            try:
                                                with open(labels_file, 'r') as f:
                                                    all_labels = json.load(f)
                                                sub_labels = all_labels[j:end_idx]
                                                logger.info(f"Loaded {len(sub_labels)} original labels from {labels_file}")
                                            except Exception as e:
                                                logger.error(f"Error loading original labels from {labels_file}: {str(e)}")
                                                # Continue trying other methods
                                                sub_labels = None
                                        elif os.path.exists(labels_npz):
                                            try:
                                                npz_data = np.load(labels_npz, allow_pickle=True)
                                                if 'labels' in npz_data:
                                                    all_labels = npz_data['labels']
                                                    sub_labels = all_labels[j:end_idx]
                                                    logger.info(f"Loaded {len(sub_labels)} original labels from {labels_npz}")
                                                else:
                                                    logger.error(f"No 'labels' key found in {labels_npz}")
                                                    sub_labels = None
                                            except Exception as e:
                                                logger.error(f"Error loading original labels from {labels_npz}: {str(e)}")
                                                sub_labels = None
                                        else:
                                            # If we can't find labels, check if we can extract them from metadata
                                            metadata_path = os.path.join(os.path.dirname(chunk_dir), 'metadata.json')
                                            if os.path.exists(metadata_path):
                                                try:
                                                    with open(metadata_path, 'r') as f:
                                                        metadata = json.load(f)
                                                    if 'labels' in metadata:
                                                        all_labels = metadata['labels']
                                                        # Calculate the correct indices for this chunk and batch
                                                        start_idx = sum(len(c) for c in word_embeddings[:i]) + j
                                                        end_idx = start_idx + (end_idx - j)
                                                        sub_labels = all_labels[start_idx:end_idx]
                                                        logger.info(f"Loaded {len(sub_labels)} original labels from metadata")
                                                    else:
                                                        logger.error(f"No 'labels' key found in metadata")
                                                        sub_labels = None
                                                except Exception as e:
                                                    logger.error(f"Error loading original labels from metadata: {str(e)}")
                                                    sub_labels = None
                                            else:
                                                sub_labels = None
                                        
                                        # If we still don't have labels, raise an error
                                        if sub_labels is None or len(sub_labels) == 0:
                                            error_msg = f"Could not find original labels for chunk {chunk_file}"
                                            logger.error(error_msg)
                                            raise ValueError(error_msg)
                                
                                # Process the batch with the obtained labels
                                try:
                                    num_graphs = process_embeddings_batch(
                                        sub_word_embeddings, sub_sentence_embeddings, sub_texts, sub_labels,
                                        config, split_output_dir, batch_idx, special_embeddings
                                    )
                                    
                                    # Log memory usage for debugging
                                    import psutil
                                    process = psutil.Process()
                                    logger.debug(f"Memory usage: {process.memory_info().rss / 1024 / 1024:.2f} MB")
                                    
                                except Exception as e:
                                    logger.error(f"Error processing batch: {str(e)}")
                                    # Fall back to original labels if there was an error
                                    logger.info("Falling back to original labels due to error")
                                    num_graphs = process_embeddings_batch(
                                        sub_word_embeddings, sub_sentence_embeddings, sub_texts, sub_labels,
                                        config, split_output_dir, batch_idx, special_embeddings
                                    )
                                else:
                                    # Process sub-batch with original labels
                                    batch_idx = (i * 1000 + j) // sub_batch_size
                                    num_graphs = process_embeddings_batch(
                                        sub_word_embeddings, sub_sentence_embeddings, sub_texts, sub_labels,
                                        config, split_output_dir, batch_idx, special_embeddings
                                    )
                                
                                total_graphs += num_graphs
                                
                                # Force garbage collection to free memory
                                import gc
                                gc.collect()
                                
                            # Clear variables to free memory
                            del word_embeddings, sentence_embeddings, texts, labels, chunk_data
                            gc.collect()
                            
                        except Exception as e:
                            logger.error(f"Error processing chunk file {chunk_path}: {str(e)}")
                            continue
                    
                except Exception as e:
                    logger.error(f"Error processing chunk {chunk_dir}: {str(e)}")
                    import traceback
                    logger.error(traceback.format_exc())
            
            logger.info(f"Processed {total_graphs} graphs for {split} split")
        
        else:
            # Load all embeddings
            try:
                # Load metadata
                metadata_path = os.path.join(split_dir, 'metadata.pkl')
                with open(metadata_path, 'rb') as f:
                    metadata = pkl.load(f)
                
                # Load word embeddings
                word_embeddings_path = os.path.join(split_dir, 'word_embeddings.pkl')
                with open(word_embeddings_path, 'rb') as f:
                    word_embeddings = pkl.load(f)
                
                # Load sentence embeddings
                sentence_embeddings_path = os.path.join(split_dir, 'sentence_embeddings.pkl')
                with open(sentence_embeddings_path, 'rb') as f:
                    sentence_embeddings = pkl.load(f)
                
                # Process in batches with enhanced progress tracking
                total_samples = len(word_embeddings)
                total_graphs = 0
                
                # Use smaller sub-batches for large datasets
                sub_batch_size = batch_size  # Use the full batch size as specified
                total_batches = int(np.ceil(total_samples / sub_batch_size))
                
                batch_progress = tqdm(range(0, total_samples, sub_batch_size), 
                                      desc=f"Processing {split} batches", 
                                      unit="batch",
                                      position=1,
                                      leave=False,
                                      colour='cyan',
                                      total=total_batches)
                for i in batch_progress:
                    batch_end = min(i + sub_batch_size, total_samples)
                    batch_idx_display = i // sub_batch_size + 1
                    batch_progress.set_postfix({
                        "samples": f"{i}-{batch_end}/{total_samples}",
                        "batch": f"{batch_idx_display}/{total_batches}",
                        "progress": f"{i/total_samples:.1%}"
                    })
                    
                    # Get batch data
                    batch_word_embeddings = word_embeddings[i:batch_end]
                    batch_sentence_embeddings = sentence_embeddings[i:batch_end]
                    batch_texts = metadata['texts'][i:batch_end]
                    batch_labels = metadata['labels'][i:batch_end]
                    
                    # Apply LLM predictions if needed
                    if label_source == 'llm' and llm_pred_dict:
                        # Try to match data indices to LLM predictions
                        modified_labels = []
                        for idx, label in enumerate(batch_labels):
                            # Use data index if available, otherwise use position in batch
                            data_idx = i + idx
                            if data_idx in llm_pred_dict:
                                modified_labels.append(llm_pred_dict[data_idx])
                            else:
                                modified_labels.append(label)
                                logger.warning(f"No LLM prediction found for data index {data_idx}, using original label")
                        
                        # Process batch with modified labels
                        num_graphs = process_embeddings_batch(
                            batch_word_embeddings, batch_sentence_embeddings, batch_texts, modified_labels,
                            config, split_output_dir, i // sub_batch_size, special_embeddings
                        )
                    else:
                        # Process batch with original labels
                        num_graphs = process_embeddings_batch(
                            batch_word_embeddings, batch_sentence_embeddings, batch_texts, batch_labels,
                            config, split_output_dir, i // sub_batch_size, special_embeddings
                        )
                    
                    total_graphs += num_graphs
                    
                    # Force garbage collection to free memory
                    import gc
                    gc.collect()
                
                logger.info(f"Processed {total_graphs} graphs for {split} split")
                
            except Exception as e:
                logger.error(f"Error processing embeddings for {split_dir}: {str(e)}")
                import traceback
                logger.error(traceback.format_exc())
    
    return output_dir

def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description="Process embeddings to create graph structures")
    parser.add_argument("--dataset_name", type=str, required=True, help="Name of the dataset")
    parser.add_argument("--embeddings_dir", type=str, required=True, help="Directory containing embeddings")
    parser.add_argument("--batch_size", type=int, default=200, help="Batch size for processing")
    parser.add_argument("--edge_type", type=str, default="constituency", choices=["constituency"], help="Type of edges to create")
    parser.add_argument("--label_source", type=str, default="original", choices=["original", "llm"], 
                        help="Source of labels: 'original' (use original dataset labels) or 'llm' (use LLM predictions)")
    parser.add_argument("--llm_predictions", type=str, help="Path to LLM predictions JSON file (required if label_source is 'llm')")
    
    args = parser.parse_args()
    
    # Validate arguments
    if args.label_source == "llm" and not args.llm_predictions:
        parser.error("--llm_predictions is required when --label_source is 'llm'")
    
    return args

def main():
    """Main entry point"""
    args = parse_args()
    
    # Display processing information
    print(f"\n{'='*80}")
    print(f"Starting graph generation for dataset: {args.dataset_name}")
    print(f"Embeddings directory: {args.embeddings_dir}")
    print(f"Batch size: {args.batch_size}")
    print(f"Edge type: {args.edge_type}")
    print(f"Label source: {args.label_source}")
    if args.label_source == 'llm':
        print(f"LLM predictions file: {args.llm_predictions}")
    print(f"{'='*80}\n")
    
    # Process embeddings
    output_dir = process_embeddings(
        dataset_name=args.dataset_name,
        embeddings_dir=args.embeddings_dir,
        batch_size=args.batch_size,
        edge_type=args.edge_type,
        label_source=args.label_source,
        llm_predictions=args.llm_predictions if args.label_source == 'llm' else None
    )
    
    if output_dir:
        print(f"\n{'='*80}")
        print(f"Graph processing completed successfully!")
        print(f"Graphs saved to: {output_dir}")
        print(f"{'='*80}\n")
        logger.info(f"Graph processing completed successfully. Graphs saved to: {output_dir}")
    else:
        print(f"\n{'='*80}")
        print(f"Graph processing failed!")
        print(f"{'='*80}\n")
        logger.error("Graph processing failed")

if __name__ == "__main__":
    main()
