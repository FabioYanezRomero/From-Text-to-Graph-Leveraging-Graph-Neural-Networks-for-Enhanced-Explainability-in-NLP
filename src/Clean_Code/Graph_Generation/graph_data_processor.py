"""
Graph Data Processor

This module processes word embeddings and prepares them for use in Graph Neural Networks.
It creates graph structures from text data using the embeddings generated by the embedding_generator.py.
"""

import os
import argparse
import json
import pickle as pkl
import logging
import numpy as np
from tqdm import tqdm
import torch
from glob import glob

# Set up device for PyTorch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)
logger.info(f"Using device: {device}")

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

def load_embeddings(input_dir, dataset, embedding_model, split):
    """Load word and sentence embeddings from disk, handling chunked data structure.
    
    Args:
        input_dir: Base directory containing the input embeddings
        dataset: Name of the dataset
        embedding_model: Name of the embedding model used
        split: Data split ('train', 'val', 'test')
        
    Returns:
        tuple: (texts, labels, word_embeddings, sentence_embeddings)
    """
    
    # Construct base path
    base_path = os.path.join(input_dir, dataset, split)
    
    # Find all chunk directories
    chunk_dirs = sorted([d for d in glob(os.path.join(base_path, 'chunk_*')) if os.path.isdir(d)])
    
    if not chunk_dirs:
        logger.error(f"No chunk directories found in {base_path}")
        return None
    
    all_word_embeddings = []
    all_sentence_embeddings = []
    all_texts = []
    all_labels = []
    
    logger.info(f"Found {len(chunk_dirs)} chunks in {base_path}")
    
    try:
        # Process each chunk
        for chunk_dir in chunk_dirs:
            try:
                # Load data from chunk
                word_embeddings_path = os.path.join(chunk_dir, 'word_embeddings.pkl')
                sentence_embeddings_path = os.path.join(chunk_dir, 'sentence_embeddings.pkl')
                metadata_path = os.path.join(chunk_dir, 'metadata.pkl')
                
                # Check if required files exist
                if not all(os.path.exists(p) for p in [word_embeddings_path, sentence_embeddings_path, metadata_path]):
                    logger.warning(f"Skipping incomplete chunk: {chunk_dir}")
                    continue
                
                # Load metadata which contains texts and labels
                with open(metadata_path, 'rb') as f:
                    metadata = pickle.load(f)
                
                # Load word and sentence embeddings
                with open(word_embeddings_path, 'rb') as f:
                    word_embeddings = pickle.load(f)
                with open(sentence_embeddings_path, 'rb') as f:
                    sentence_embeddings = pickle.load(f)
                
                # Extract texts and labels from metadata
                texts = [item['text'] for item in metadata]
                labels = [item['label'] for item in metadata]
                
                # Convert to lists if they're numpy arrays
                if isinstance(word_embeddings, np.ndarray):
                    word_embeddings = list(word_embeddings)
                if isinstance(sentence_embeddings, np.ndarray):
                    sentence_embeddings = list(sentence_embeddings)
                if isinstance(texts, np.ndarray):
                    texts = list(texts)
                if isinstance(labels, np.ndarray):
                    labels = list(labels)
                
                # Add to the combined lists
                all_word_embeddings.extend(word_embeddings)
                all_sentence_embeddings.extend(sentence_embeddings)
                all_texts.extend(texts)
                all_labels.extend(labels)
                
                logger.info(f"Loaded {len(word_embeddings)} samples from {os.path.basename(chunk_dir)}")
                
            except Exception as e:
                logger.error(f"Error loading chunk {chunk_dir}: {str(e)}")
                import traceback
                logger.error(traceback.format_exc())
        
        if not all_word_embeddings:
            logger.error("No data was loaded from any chunk")
            return None
            
        logger.info(f"Successfully loaded {len(all_word_embeddings)} total samples from {len(chunk_dirs)} chunks")
        return all_texts, all_labels, all_word_embeddings, all_sentence_embeddings
        
    except Exception as e:
        logger.error(f"Error in load_embeddings: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return None

def save_graphs(graphs, output_dir, batch_size=32, num_workers=4):
    """Save a list of PyTorch Geometric Data objects to disk.
    
    Args:
        graphs: List of PyTorch Geometric Data objects
        output_dir: Directory to save the graphs
        batch_size: Number of graphs to save in each file
        num_workers: Number of worker processes to use
    """
    import os
    import torch
    from torch_geometric.data import Data, Batch
    from torch_geometric.loader import DataLoader
    
    os.makedirs(output_dir, exist_ok=True)
    
    # Create a DataLoader to handle batching
    loader = DataLoader(graphs, batch_size=batch_size, shuffle=False, num_workers=num_workers)
    
    # Save each batch
    for batch_idx, batch in enumerate(loader):
        batch_path = os.path.join(output_dir, f'batch_{batch_idx:04d}.pt')
        try:
            torch.save(batch, batch_path)
            logger.info(f"Saved batch {batch_idx} with {len(batch)} graphs to {batch_path}")
        except Exception as e:
            logger.error(f"Error saving batch {batch_idx}: {str(e)}")
    
    logger.info(f"Saved {len(graphs)} graphs in {len(loader)} batches to {output_dir}")

def get_dataset_info(embeddings_dir):
    """Get information about the dataset without loading all embeddings
    
    Args:
        embeddings_dir: Directory containing embeddings
        
    Returns:
        dataset_info: Dictionary containing dataset information
        is_chunked: Whether embeddings are stored in chunks
    """
    # First, check if we need to look in a nested directory structure
    metadata_path = os.path.join(embeddings_dir, 'metadata.json')
    
    # If metadata.json is not found directly, look for subdirectories that might contain it
    if not os.path.exists(metadata_path):
        subdirs = [d for d in os.listdir(embeddings_dir) if os.path.isdir(os.path.join(embeddings_dir, d))]
        
        for subdir in subdirs:
            nested_path = os.path.join(embeddings_dir, subdir, 'metadata.json')
            if os.path.exists(nested_path):
                logger.info(f"Found metadata in nested directory: {nested_path}")
                metadata_path = nested_path
                embeddings_dir = os.path.join(embeddings_dir, subdir)
                break
    
    # Check if embeddings are stored in chunks
    embedding_chunks_dir = os.path.join(embeddings_dir, 'embedding_chunks')
    is_chunked = os.path.exists(embedding_chunks_dir)
    
    if not os.path.exists(metadata_path):
        raise FileNotFoundError(f"Metadata file not found in {embeddings_dir} or its subdirectories")
    
    with open(metadata_path, 'r') as f:
        metadata = json.load(f)
    
    if is_chunked:
        # Get list of chunk files
        chunk_files = sorted([f for f in os.listdir(embedding_chunks_dir) if f.endswith('.pkl')])
        
        dataset_info = {
            'total_samples': metadata.get('num_samples', 0),
            'chunk_dirs': [embedding_chunks_dir],
            'num_chunks': len(chunk_files)
        }
    else:
        dataset_info = {
            'total_samples': metadata.get('num_samples', 0)
        }
    
    return dataset_info, is_chunked

def load_chunk_from_disk(chunk_dir):
    """Load a chunk of embeddings from disk
    
    Args:
        chunk_dir: Directory containing chunk data
        
    Returns:
        word_embeddings, sentence_embeddings, texts, labels
    """
    # Get all chunk files
    chunk_files = [f for f in os.listdir(chunk_dir) if 
                 (f.endswith('.pkl') and f.startswith('chunk_')) or 
                 (f.endswith('.npz') and f.startswith('embeddings_chunk_'))]
    
    def extract_chunk_number(filename):
        if filename.startswith('chunk_'):
            return int(filename.replace('chunk_', '').replace('.pkl', ''))
        elif filename.startswith('embeddings_chunk_'):
            return int(filename.replace('embeddings_chunk_', '').replace('.npz', ''))
    
    # Sort chunk files numerically by their index
    chunk_files = sorted(chunk_files, key=extract_chunk_number)
    
    # Process only one chunk file at a time to save memory
    if not chunk_files:
        logger.warning(f"No chunk files found in {chunk_dir}")
        return [], [], [], []
    
    # Load the first chunk file to get data
    chunk_file = chunk_files[0]
    chunk_path = os.path.join(chunk_dir, chunk_file)
    
    word_embeddings = []
    sentence_embeddings = []
    texts = []
    labels = []
    
    if chunk_file.endswith('.pkl'):
        # Load pickle file
        with open(chunk_path, 'rb') as f:
            chunk_data = pkl.load(f)
        
        # Extract data
        if isinstance(chunk_data, dict):
            word_embeddings = chunk_data.get('word_embeddings', [])
            sentence_embeddings = chunk_data.get('sentence_embeddings', [])
            texts = chunk_data.get('texts', [])
            labels = chunk_data.get('labels', [])
        elif isinstance(chunk_data, list) and len(chunk_data) == 4:
            word_embeddings, sentence_embeddings, texts, labels = chunk_data
    
    elif chunk_file.endswith('.npz'):
        # Load numpy file
        chunk_data = np.load(chunk_path, allow_pickle=True)
        
        # Extract data
        if 'word_embeddings' in chunk_data:
            word_embeddings = chunk_data['word_embeddings']
        if 'sentence_embeddings' in chunk_data:
            sentence_embeddings = chunk_data['sentence_embeddings']
        if 'texts' in chunk_data:
            texts = chunk_data['texts']
        if 'labels' in chunk_data:
            labels = chunk_data['labels']
    
    logger.info(f"Loaded {len(word_embeddings)} samples from {chunk_file}")
    # If labels are missing, create dummy labels
    if ((isinstance(labels, list) and not labels) or 
        (isinstance(labels, np.ndarray) and len(labels) == 0)) and \
       word_embeddings is not None and len(word_embeddings) > 0:
        logger.warning(f"No labels found in chunks, creating dummy labels")
        labels = [0] * len(word_embeddings)
    
    return word_embeddings, sentence_embeddings, texts, labels

def load_special_embeddings(embeddings_dir):
    """Load special embeddings for constituency tokens
    
    Args:
        embeddings_dir: Directory containing embeddings
        
    Returns:
        special_embeddings: Dictionary of special embeddings
    """
    import os
    from glob import glob
    
    # Check if special embeddings exist in main directory
    special_embeddings_path = os.path.join(embeddings_dir, 'special_embeddings.pkl')
    
    if os.path.exists(special_embeddings_path):
        logger.info(f"Found special embeddings at: {special_embeddings_path}")
        with open(special_embeddings_path, 'rb') as f:
            special_embeddings = pkl.load(f)
        return special_embeddings
    
    # Check if special embeddings exist in special directory
    special_dir = os.path.join(os.path.dirname(embeddings_dir), 'special')
    special_embeddings_path = os.path.join(special_dir, 'special_embeddings.pkl')
    
    if os.path.exists(special_embeddings_path):
        logger.info(f"Found special embeddings at: {special_embeddings_path}")
        with open(special_embeddings_path, 'rb') as f:
            special_embeddings = pkl.load(f)
        return special_embeddings
    
    # Search recursively for special_embeddings.pkl in all subdirectories
    # Use glob to find all special_embeddings.pkl files under the embeddings_dir
    special_embeddings_files = glob(os.path.join(embeddings_dir, '**', 'special_embeddings.pkl'), recursive=True)
    
    if special_embeddings_files:
        # Use the first found file
        special_embeddings_path = special_embeddings_files[0]
        logger.info(f"Found special embeddings through recursive search: {special_embeddings_path}")
        with open(special_embeddings_path, 'rb') as f:
            special_embeddings = pkl.load(f)
        return special_embeddings
    
    # If still not found, try to look in split directories based on dataset
    # Extract dataset name from embeddings_dir
    dataset_name = os.path.basename(os.path.dirname(embeddings_dir))
    
    # Define appropriate splits based on dataset
    if 'ag_news' in dataset_name or 'ag-news' in dataset_name:
        splits = ['train', 'test']
        logger.info(f"Using train and test splits for AG News dataset")
    elif 'sst-2' in dataset_name or 'sst2' in dataset_name:
        splits = ['train', 'validation']
        logger.info(f"Using train and validation splits for SST-2 dataset")
    else:
        # Default to all possible splits
        splits = ['train', 'validation', 'test', 'val']
        logger.info(f"Using all possible splits for unknown dataset: {dataset_name}")
    
    for split in splits:
        split_dir = os.path.join(embeddings_dir, split)
        if os.path.isdir(split_dir):
            special_embeddings_files = glob(os.path.join(split_dir, '**', 'special_embeddings.pkl'), recursive=True)
            if special_embeddings_files:
                special_embeddings_path = special_embeddings_files[0]
                logger.info(f"Found special embeddings in {split} split: {special_embeddings_path}")
                with open(special_embeddings_path, 'rb') as f:
                    special_embeddings = pkl.load(f)
                return special_embeddings
    
    logger.warning(f"No special embeddings found in {embeddings_dir} or its subdirectories")
    return None

def load_constituency_tree(tree_path, sample_idx=0):
    """Load a pre-generated constituency tree from file
    
    Args:
        tree_path: Path to the pickle file containing the tree
        sample_idx: Index of the sample to extract from the tree file (for batch files)
        
    Returns:
        The loaded tree object (NetworkX DiGraph) or None if no valid tree found
    """
    import pickle
    import networkx as nx
    
    try:
        with open(tree_path, 'rb') as f:
            tree_data = pickle.load(f)
        
        # Handle different tree data structures
        # Case 1: Direct NetworkX DiGraph
        if isinstance(tree_data, nx.DiGraph):
            return tree_data
        
        # Case 2: List of trees in a tuple in a list - format from our analysis
        if isinstance(tree_data, list) and len(tree_data) > 0:
            if isinstance(tree_data[0], tuple) and len(tree_data[0]) > 0:
                tree_list = tree_data[0][0]
                
                # If tree_list is a list of NetworkX DiGraphs
                if isinstance(tree_list, list):
                    if len(tree_list) > sample_idx:
                        # Get the specific tree for this sample
                        tree = tree_list[sample_idx]
                        if isinstance(tree, nx.DiGraph):
                            return tree
                    elif len(tree_list) > 0:
                        # If index is out of range, use the first tree
                        tree = tree_list[0]
                        if isinstance(tree, nx.DiGraph):
                            return tree
        
        # Case 3: List of trees directly
        if isinstance(tree_data, list) and len(tree_data) > 0:
            if isinstance(tree_data[0], nx.DiGraph):
                if len(tree_data) > sample_idx:
                    return tree_data[sample_idx]
                elif len(tree_data) > 0:
                    return tree_data[0]
        
        # If we couldn't extract a valid tree, log a warning and return None
        logger.warning(f"Could not extract a valid tree from {tree_path}")
        return None
    except Exception as e:
        logger.error(f"Error loading constituency tree from {tree_path}: {str(e)}")
        return None

def extract_tree_structure(tree):
    """Extract node features and edge indices from a constituency tree
    
    Args:
        tree: The constituency tree object (either a custom tree object or a NetworkX DiGraph)
        
    Returns:
        tuple: (node_features, edge_indices, leaf_nodes, word_nodes)
    """
    import networkx as nx
    
    # Check if the tree is already a NetworkX DiGraph
    if isinstance(tree, nx.DiGraph):
        # Use the existing DiGraph structure
        G = tree
        
        # Get node features from the graph
        node_features = {}
        for node_id, node_data in G.nodes(data=True):
            # Store node feature (using label or other identifier from node data)
            if 'label' in node_data:
                node_features[node_id] = node_data['label']
            elif 'pos' in node_data:
                node_features[node_id] = node_data['pos']
            else:
                # Use a default feature if no label is available
                node_features[node_id] = 'NODE'
        
        # Get edge indices from the graph
        edge_indices = [(u, v) for u, v in G.edges()]
        
        # If no nodes were found, create at least one dummy node and edge
        if not node_features:
            node_features[0] = 'ROOT'
            if not edge_indices:
                edge_indices = [(0, 0)]  # Self-loop for the dummy node
        
        # Find leaf nodes (nodes with no outgoing edges)
        leaf_nodes = [n for n, d in G.out_degree() if d == 0]
        
        # Find word nodes - these are leaf nodes that have numeric labels or whose parents have POS tags
        # This is based on the tree structure where actual words are leaf nodes with numeric IDs
        word_nodes = []
        for node in leaf_nodes:
            # Check if the node label is numeric (representing word position)
            try:
                int(node)
                word_nodes.append(node)
                continue
            except (ValueError, TypeError):
                pass
            
            # Check if the node has a numeric label
            node_label = node_features.get(node, '')
            try:
                int(node_label)
                word_nodes.append(node)
            except (ValueError, TypeError):
                # Check if the node's parent has a POS tag label
                parents = list(G.predecessors(node))
                if parents:
                    parent = parents[0]
                    parent_label = node_features.get(parent, '')
                    if parent_label in ['NNP', 'NN', 'VB', 'JJ', 'RB', 'DT', 'IN', 'CC', 'PRP', 'CD']:
                        word_nodes.append(node)
        
        return node_features, edge_indices, leaf_nodes, word_nodes
    
    # Handle traditional tree objects with children attributes
    # Convert tree to networkx graph for consistent processing
    G = nx.DiGraph()
    
    # We'll use a queue for BFS traversal
    from collections import deque
    queue = deque([(tree, 0, -1)])  # (node, node_id, parent_id)
    node_id = 0
    
    # Dictionaries to store node information
    node_features = {}
    edge_indices = []
    
    while queue:
        node, current_id, parent_id = queue.popleft()
        
        # Store node feature (using label or other identifier)
        if hasattr(node, 'label'):
            node_features[current_id] = node.label[0] if isinstance(node.label, list) else node.label
        else:
            # Use a default feature if no label is available
            node_features[current_id] = 'NODE'
        
        # Add edge to parent if not root
        if parent_id != -1:
            edge_indices.append((parent_id, current_id))
        
        # Add children to queue
        if hasattr(node, 'children'):
            for child in node.children:
                next_id = node_id + 1
                node_id += 1
                queue.append((child, next_id, current_id))
    
    # If no nodes were found, create at least one dummy node and edge
    if not node_features:
        node_features[0] = 'ROOT'
        if not edge_indices:
            edge_indices = [(0, 0)]  # Self-loop for the dummy node
    
    # Create a temporary graph to find leaf nodes
    temp_G = nx.DiGraph()
    for u, v in edge_indices:
        temp_G.add_edge(u, v)
    
    # Find leaf nodes (nodes with no outgoing edges)
    leaf_nodes = [n for n, d in temp_G.out_degree() if d == 0]
    
    # For traditional trees, assume all leaf nodes are word nodes
    word_nodes = leaf_nodes

    return node_features, edge_indices, leaf_nodes, word_nodes

def create_word_graphs(word_embeddings, sentence_embeddings, texts, labels, dataset_name, split='train', edge_type='constituency', show_progress=False):
    """Create word graphs from word embeddings and constituency trees
    
    Args:
        word_embeddings: List of word embeddings for each sample
        sentence_embeddings: List of sentence embeddings for each sample
        texts: List of text samples
        labels: List of labels for each sample
        dataset_name: Name of the dataset
        split: Dataset split (train, val, test)
        edge_type: Type of edges to use (constituency, dependency)
        show_progress: Whether to show progress bar
        
    Returns:
        list: List of graph data objects
    """
    import os
    import pickle as pkl
    from tqdm import tqdm
    import torch
    
    # For graph generation, we only need word embeddings and labels
    # Texts can be derived from word embeddings if needed
    if len(word_embeddings) == 0:
        logger.error("No word embeddings provided, cannot create graphs")
        return []
    
    # If labels are missing but we have word embeddings, use default labels
    if len(labels) == 0 and len(word_embeddings) > 0:
        logger.warning(f"No labels provided, using default labels for {len(word_embeddings)} samples")
        labels = [0] * len(word_embeddings)
    
    # Ensure labels match the number of word embeddings
    if len(labels) != len(word_embeddings):
        logger.warning(f"Label count ({len(labels)}) doesn't match word embeddings count ({len(word_embeddings)})")
        # Adjust labels to match word embeddings
        if len(labels) > len(word_embeddings):
            labels = labels[:len(word_embeddings)]
            logger.info(f"Truncated labels to match word embeddings count: {len(word_embeddings)}")
        else:
            # Extend labels with defaults
            labels.extend([0] * (len(word_embeddings) - len(labels)))
            logger.info(f"Extended labels to match word embeddings count: {len(word_embeddings)}")
    
    # For texts, we don't need them for graph creation as we use the tree structure
    # But ensure the texts list is the same length as word_embeddings for compatibility
    if len(texts) != len(word_embeddings):
        # Create empty texts list of the right length
        texts = [""] * len(word_embeddings)
        logger.debug(f"Created empty texts list of length {len(texts)} to match word embeddings")
    
    # Extract dataset provider and name
    if '/' in dataset_name:
        provider, name = dataset_name.split('/', 1)
    else:
        # If no provider is specified, try to determine from the dataset name
        if dataset_name == 'stanfordnlp':
            provider, name = 'stanfordnlp', 'sst2'
        else:
            provider, name = 'stanfordnlp', dataset_name

    # Handle case sensitivity in provider names by checking actual directory names
    text_graphs_dir = '/app/src/Clean_Code/output/text_graphs'
    if os.path.exists(text_graphs_dir):
        for dir_name in os.listdir(text_graphs_dir):
            if dir_name.lower() == provider.lower():
                provider = dir_name  # Use the actual directory name with correct capitalization
                break

    logger.debug(f"Dataset name received in create_word_graphs: {dataset_name}")
    logger.debug(f"Provider: {provider}, Name: {name}")

    # Check both possible locations for constituency trees
    base_dir = os.path.join(text_graphs_dir, provider, name, split, 'constituency')
    
    # If the default path doesn't exist, try the path defined in Tree_Generation config
    if not os.path.exists(base_dir):
        base_dir = os.path.join('/app/src/Clean_Code/output/text_trees', f"{provider}/{name}", split, 'constituency')
    logger.debug(f"Looking for constituency trees in: {base_dir}")

    if not os.path.exists(base_dir):
        logger.warning(f"Constituency tree directory not found: {base_dir}. No graphs will be created.")
        return []

    logger.debug(f"Creating graphs from pre-generated constituency trees in {base_dir}")
    logger.debug(f"Input sizes: {len(word_embeddings)} word embeddings, {len(sentence_embeddings) if sentence_embeddings else 0} sentence embeddings, {len(texts)} texts, {len(labels)} labels")
    
    # For batch processing
    batch_size_tree_gen = 256  # Default batch size used in tree_generator.py (from Tree_Generation/config.py)
    
    # Process each sample with optional progress bar
    range_iter = tqdm(range(len(word_embeddings)), desc=f"Creating {split} graphs", unit="graph", position=0, leave=True, colour='green') if show_progress else range(len(word_embeddings))
    
    graphs = []
    
    for i in range_iter:
        try:
            # Calculate which batch file should contain this sample
            batch_idx = i // batch_size_tree_gen
            
            # Try different tree file naming patterns
            tree_file = os.path.join(base_dir, f"tree_{i}.pkl")  # Individual tree file (old format)
            simple_tree_file = os.path.join(base_dir, f"{i}.pkl")  # Individual tree file (old format)
            batch_tree_file = os.path.join(base_dir, f"{batch_idx}.pkl")  # Batch tree file (new format)
            
            # Check if any tree file pattern exists
            if os.path.exists(tree_file):
                # Individual tree file (old format)
                tree_path = tree_file
                tree = load_constituency_tree(tree_path, sample_idx=i)
            elif os.path.exists(simple_tree_file):
                # Individual tree file (old format)
                tree_path = simple_tree_file
                tree = load_constituency_tree(tree_path, sample_idx=i)
            elif os.path.exists(batch_tree_file):
                # Batch tree file (new format)
                # Calculate the position within the batch
                in_batch_idx = i % batch_size_tree_gen
                
                # Use our improved load_constituency_tree function with the sample index
                tree = load_constituency_tree(batch_tree_file, sample_idx=in_batch_idx)
                
                # Skip if we couldn't get a valid tree
                if tree is None:
                    logger.warning(f"Could not extract a valid tree for sample {i} from batch file {batch_tree_file}")
                    continue
            else:
                # No tree file found, skip this sample
                logger.warning(f"No tree file found for sample {i}")
                continue
            
            # Get the corresponding embeddings and label
            word_embedding = word_embeddings[i] if i < len(word_embeddings) else None
            sentence_embedding = sentence_embeddings[i] if sentence_embeddings and i < len(sentence_embeddings) else None
            label = labels[i] if i < len(labels) else None
            text = texts[i] if i < len(texts) else None
            
            # Handle label tensor
            if label is not None and isinstance(label, torch.Tensor):
                if label.numel() > 1:
                    label = label[0].item()
                else:
                    label = label.item()
            
            # Extract tree structure with leaf nodes and word nodes
            node_features, edge_indices, leaf_nodes, word_nodes = extract_tree_structure(tree)
            
            if not edge_indices:
                logger.warning(f"No edges extracted from tree for sample {i}")
                continue
            
            # Log information about the tree structure for debugging
            if i == 0 or i % 100 == 0:  # Log for first sample and every 100th sample
                logger.debug(f"Sample {i}: Tree has {len(node_features)} nodes, {len(edge_indices)} edges")
                logger.debug(f"Sample {i}: Found {len(leaf_nodes)} leaf nodes and {len(word_nodes)} word nodes")
                if word_embedding:
                    logger.debug(f"Sample {i}: Word embeddings length: {len(word_embedding)}")
            
            # Create the graph
            graph_data = create_graph_from_tree(
                node_features, 
                edge_indices, 
                word_embedding, 
                sentence_embedding, 
                text, 
                label,
                leaf_nodes=leaf_nodes,
                word_nodes=word_nodes
            )
            
            # Verify the graph has node features
            if graph_data.x.shape[0] == 0:
                logger.warning(f"Graph for sample {i} has no nodes, skipping")
                continue
            
            graphs.append(graph_data)
            
        except Exception as e:
            logger.error(f"Error processing sample {i}: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
   
    
    import torch
    import numpy as np
    from torch_geometric.data import Data
    import networkx as nx
    
    # Determine embedding dimension
    embedding_dim = None
    if word_embeddings and len(word_embeddings) > 0:
        if isinstance(word_embeddings[0], torch.Tensor):
            embedding_dim = word_embeddings[0].shape[0]
        else:
            embedding_dim = len(word_embeddings[0])
    elif sentence_embedding is not None:
        if isinstance(sentence_embedding, torch.Tensor):
            embedding_dim = sentence_embedding.shape[0]
        else:
            embedding_dim = len(sentence_embedding)
    else:
        # Default embedding dimension if no embeddings provided
        embedding_dim = 2
    
    # Create a directed graph to determine topological order
    G = nx.DiGraph()
    G.add_nodes_from(node_features.keys())
    G.add_edges_from(edge_indices)
    
    # Determine node ordering for embedding propagation
    try:
        # Try to get topological sort (from leaves to root)
        node_order = list(reversed(list(nx.topological_sort(G))))
        logger.debug("Using topological sort for node ordering")
    except nx.NetworkXUnfeasible:
        # Graph has cycles, use simple node list
        logger.warning("Graph has cycles, using simple node list instead of topological sort")
        node_order = list(node_features.keys())
    
    # Initialize node embeddings dictionary
    x = {}
    for node_id in node_features:
        x[node_id] = torch.zeros(embedding_dim)
        
    # Log information about the tree structure
    logger.debug(f"Tree has {len(node_features)} nodes, {len(edge_indices)} edges")
    logger.debug(f"Found {len(leaf_nodes) if leaf_nodes else 0} leaf nodes and {len(word_nodes) if word_nodes else 0} word nodes")
    if word_embeddings:
        logger.debug(f"Word embeddings length: {len(word_embeddings)}")
    if word_nodes and word_embeddings:
        logger.debug(f"Word nodes to embeddings ratio: {len(word_nodes)}/{len(word_embeddings)}")

    
    # Assign word embeddings to word nodes
    if word_embeddings and len(word_embeddings) > 0:
        # If word_nodes is empty but we have leaf nodes, identify word nodes from leaf nodes
        if not word_nodes and leaf_nodes:
            # First try to find numeric leaf nodes (these are typically word nodes)
            numeric_leaf_nodes = []
            for node in leaf_nodes:
                try:
                    # Check if node is numeric (either integer or string that can be converted to int)
                    if isinstance(node, int) or (isinstance(node, str) and node.isdigit()):
                        numeric_leaf_nodes.append(node)
                except Exception:
                    pass
            
            if numeric_leaf_nodes:
                word_nodes = numeric_leaf_nodes
                logger.debug(f"Using {len(word_nodes)} numeric leaf nodes as word nodes")
            else:
                # If no numeric leaf nodes, use all leaf nodes
                word_nodes = leaf_nodes
                logger.debug(f"Using all {len(word_nodes)} leaf nodes as word nodes")
        
        # If we still don't have word nodes, we need to identify them from the tree structure
        if not word_nodes:
            # Use nodes with no outgoing edges (leaf nodes in the graph)
            leaf_node_ids = [n for n, d in G.out_degree() if d == 0]
            word_nodes = leaf_node_ids
            logger.debug(f"Identified {len(word_nodes)} leaf nodes from graph structure")
            
            # If still no word nodes, we can't assign embeddings properly
            if not word_nodes:
                logger.warning("No word nodes could be identified from the tree structure")
                return Data(x=torch.zeros(1, embedding_dim), edge_index=torch.tensor([[0, 0]], dtype=torch.long).t(), y=torch.tensor([label], dtype=torch.long) if label is not None else torch.tensor([0], dtype=torch.long))        
        # Create a mapping between word nodes and their positions in the sentence
        # This is critical for correctly assigning word embeddings
        word_node_positions = {}
        
        # Try to determine the position of each word node in the sentence
        for node_id in word_nodes:
            # First try to use the node ID as a position indicator
            try:
                if isinstance(node_id, int):
                    word_node_positions[node_id] = node_id
                elif isinstance(node_id, str) and node_id.isdigit():
                    word_node_positions[node_id] = int(node_id)
                continue
            except (ValueError, TypeError):
                pass
            
            # Then try to use the node label as a position indicator
            try:
                node_label = node_features.get(node_id, '')
                if isinstance(node_label, str) and node_label.isdigit():
                    word_node_positions[node_id] = int(node_label)
                    continue
                elif isinstance(node_label, int):
                    word_node_positions[node_id] = node_label
                    continue
            except (ValueError, TypeError):
                pass
        
        # Sort word nodes by their positions
        sorted_word_nodes = []
        if word_node_positions:
            # Sort by position if we have position information
            sorted_items = sorted(word_node_positions.items(), key=lambda x: x[1])
            sorted_word_nodes = [node_id for node_id, _ in sorted_items]
            logger.debug(f"Sorted {len(sorted_word_nodes)} word nodes by position")
        else:
            # Otherwise just use the nodes as they are
            sorted_word_nodes = list(word_nodes)
            logger.debug("Using unsorted word nodes")
        
        # Log what we're doing
        logger.debug(f"Word nodes: {sorted_word_nodes[:5]}{'...' if len(sorted_word_nodes) > 5 else ''}")
        logger.debug(f"Word embeddings length: {len(word_embeddings)}")
        
        # Assign embeddings to word nodes
        assigned_count = 0
        for i, node_id in enumerate(sorted_word_nodes):
            if i < len(word_embeddings):
                x[node_id] = word_embeddings[i]
                assigned_count += 1
        
        logger.debug(f"Assigned {assigned_count} embeddings to word nodes")
        
        # If we didn't assign any embeddings, something is wrong with our mapping
        if assigned_count == 0 and word_embeddings:
            logger.warning("Failed to assign any embeddings to word nodes")
            
            # Try a direct assignment to all nodes as a last resort
            if len(node_features) <= len(word_embeddings):
                logger.debug("Attempting direct assignment to all nodes")
                for i, node_id in enumerate(node_features.keys()):
                    if i < len(word_embeddings):
                        x[node_id] = word_embeddings[i]
    
    # Compute non-leaf node embeddings as mean of children embeddings
    # Process nodes in topological order (from leaves to root)
    for node_id in node_order:
        children = [c for p, c in edge_indices if p == node_id]
        if children:  # If this is not a leaf node
            # Only include children that have valid tensor embeddings
            valid_child_embeddings = []
            for c in children:
                if c in x:
                    # Ensure the embedding is a tensor and has the right shape
                    if isinstance(x[c], torch.Tensor) and x[c].dim() == 1 and x[c].shape[0] == embedding_dim:
                        valid_child_embeddings.append(x[c])
                    elif isinstance(x[c], (list, tuple, np.ndarray)):
                        # Convert to tensor if it's not already
                        try:
                            tensor_embedding = torch.tensor(x[c], dtype=torch.float)
                            if tensor_embedding.dim() == 1 and tensor_embedding.shape[0] == embedding_dim:
                                valid_child_embeddings.append(tensor_embedding)
                        except Exception as e:
                            logger.debug(f"Could not convert embedding for node {c} to tensor: {str(e)}")
            
            # Compute mean of valid child embeddings
            if valid_child_embeddings:
                try:
                    x[node_id] = torch.stack(valid_child_embeddings).mean(dim=0)
                    logger.debug(f"Computed embedding for non-leaf node {node_id} from {len(valid_child_embeddings)} children")
                except Exception as e:
                    logger.debug(f"Failed to compute embedding for non-leaf node {node_id}: {str(e)}")
    
    # Convert node features dictionary to tensor, ensuring all entries are valid tensors
    node_ids = sorted(node_features.keys())
    valid_x_tensors = []
    
    for node_id in node_ids:
        if node_id in x:
            # Ensure the embedding is a valid tensor
            if isinstance(x[node_id], torch.Tensor) and x[node_id].dim() == 1 and x[node_id].shape[0] == embedding_dim:
                valid_x_tensors.append(x[node_id])
        elif isinstance(x[node_id], (list, tuple, np.ndarray)):
            try:
                tensor_embedding = torch.tensor(x[node_id], dtype=torch.float)
                if tensor_embedding.dim() == 1 and tensor_embedding.shape[0] == embedding_dim:
                    valid_x_tensors.append(tensor_embedding)
            except Exception:
                pass
    
    # Stack valid tensors or create an empty tensor
    if valid_x_tensors:
        x_tensor = torch.stack(valid_x_tensors)
    else:
        x_tensor = torch.zeros(0, embedding_dim)
    
    # Check if we have valid node features
    if x_tensor.shape[0] == 0 or (x_tensor.shape[0] > 0 and torch.all(x_tensor == 0)):
        logger.warning("No valid node features detected, ensuring we have at least one valid node")
        
    # Use sentence embedding if available
    if sentence_embedding is not None and isinstance(sentence_embedding, torch.Tensor):
        try:
            # Ensure sentence embedding is the right shape
            if sentence_embedding.dim() == 1 and sentence_embedding.shape[0] == embedding_dim:
                x_tensor = sentence_embedding.unsqueeze(0)
                logger.debug("Using sentence embedding as node feature")
            else:
                logger.warning(f"Sentence embedding has wrong shape: {sentence_embedding.shape}, expected ({embedding_dim},)")
                x_tensor = torch.randn(1, embedding_dim)
        except Exception as e:
            logger.warning(f"Error using sentence embedding: {str(e)}")
            x_tensor = torch.randn(1, embedding_dim)
    # Use first word embedding if available
    elif word_embeddings is not None and len(word_embeddings) > 0:
        try:
            first_embedding = word_embeddings[0]
            if isinstance(first_embedding, torch.Tensor):
                x_tensor = first_embedding.unsqueeze(0)
            else:
                x_tensor = torch.tensor(first_embedding, dtype=torch.float).unsqueeze(0)
            logger.debug("Using first word embedding as node feature")
        except Exception as e:
            logger.warning(f"Error using first word embedding: {str(e)}")
            x_tensor = torch.randn(1, embedding_dim)
    # Fallback to a single node with random features
        else:
            # As a last resort, create a single node with non-zero features
            logger.warning("No sentence embedding available, creating non-zero features")
            # Create a single node with non-zero features
            x_tensor = torch.ones((1, embedding_dim))
            # Create a self-loop edge
            edge_indices = [[0, 0]]
    
    # Log final node feature tensor shape
    logger.info(f"Final node feature tensor shape: {x_tensor.shape}")
    
    # Verify that we have valid node features
    if x_tensor.shape[0] == 0:
        logger.error("Still have empty node features after all attempts")
        # Create a minimal valid graph with one node
        # We use ones instead of random values as requested by the USER
        x_tensor = torch.ones((1, embedding_dim))
        edge_indices = [[0, 0]]
        logger.warning("Created minimal valid graph with one node and non-zero features")
    
    # Convert edge indices to tensor
    # Map node IDs to consecutive integers
    node_to_idx = {node_id: i for i, node_id in enumerate(node_ids)}
    
    # Ensure we have valid edges (both parent and child exist in node_to_idx)
    valid_edges = [(p, c) for p, c in edge_indices if p in node_to_idx and c in node_to_idx]
    
    # If no valid edges but we have nodes, create self-loops for each node
    if not valid_edges and node_ids:
        valid_edges = [(node_id, node_id) for node_id in node_ids]
        logger.warning(f"No valid edges found, creating self-loops for {len(node_ids)} nodes")
    
    # Convert edges to tensor
    if valid_edges:
        # Map node IDs to consecutive indices
        edge_index = torch.tensor([
            [node_to_idx[p], node_to_idx[c]] for p, c in valid_edges
        ], dtype=torch.long).t()
    else:
        # If we somehow still have no valid edges, create a self-loop for node 0
        # This should only happen if we have no nodes at all
        edge_index = torch.tensor([[0, 0]], dtype=torch.long).t()
        logger.warning("No valid edges possible, created fallback self-loop for node 0")
    
    # Final sanity check - ensure we have at least one node and one edge
    if x_tensor.shape[0] == 0:
        logger.warning("Still no nodes after all fallbacks, creating emergency dummy node")
        x_tensor = torch.randn(1, embedding_dim)
        # Reset edge index to match the single node
        edge_index = torch.tensor([[0, 0]], dtype=torch.long).t()
    
    # Ensure edge indices don't exceed node count
    if edge_index.shape[1] > 0 and edge_index.max() >= x_tensor.shape[0]:
        logger.warning(f"Edge indices ({edge_index.max()}) exceed node count ({x_tensor.shape[0]}), creating self-loops")
        # Create self-loops for each node as a safe fallback
        edge_index = torch.tensor([[i, i] for i in range(x_tensor.shape[0])], dtype=torch.long).t()
    
    # Create the graph data object
    y = torch.tensor([label], dtype=torch.long) if label is not None else torch.tensor([0], dtype=torch.long)
    data = Data(x=x_tensor, edge_index=edge_index, y=y)
    
    # Add text as an attribute
    if text is not None:
        data.text = text
    
    # Add label as a string attribute
    if label is not None:
        # Convert label to string representation
        if isinstance(label, int):
            # Map numeric labels to class names if available
            label_map = {0: "world", 1: "sports", 2: "business", 3: "sci/tech"}
            label_str = label_map.get(label, str(label))
        else:
            label_str = str(label)
        
        data.label_str = label_str
    
    return data


def process_embeddings_batch(batch_word_embeddings, batch_sentence_embeddings, batch_texts, batch_labels, config, split_output_dir, batch_idx, special_embeddings=None):
    """Process a single batch of embeddings"""
    # Log label source
    label_source = config.get('label_source', 'original')
    if label_source == 'llm':
        logger.debug(f"Using LLM predictions as labels for batch {batch_idx}")
    
    # Extract dataset name from the path
    # We need to use the original dataset_name passed from process_embeddings
    # Instead of trying to extract it from the split_output_dir which might be incorrect
    
    # Get the dataset name from the config
    dataset_name = config.get('dataset_name', 'stanfordnlp/sst2')
    
    # Log the dataset name we're using
    logger.debug(f"Using dataset name: {dataset_name} for graph creation")

    
    # Extract split from the path
    split = os.path.basename(split_output_dir).replace('_llm_labels', '')
    
    # Create graphs
    batch_graphs = create_word_graphs(
        batch_word_embeddings,
        batch_sentence_embeddings,
        batch_texts,
        batch_labels,
        dataset_name,
        split,
        config['edge_type'],
        show_progress=False  # Disable progress bar for batch processing
    )
    
    # Save batch graphs with consistent numerical naming to preserve original order
    # Format with leading zeros to ensure correct sorting
    batch_graphs_path = os.path.join(split_output_dir, f"graphs_{config['edge_type']}_batch_{batch_idx:04d}.pkl")
    
    with open(batch_graphs_path, 'wb') as f:
        pkl.dump(batch_graphs, f)
    
    logger.debug(f"Saved {len(batch_graphs)} graphs to {batch_graphs_path}")
    
    return len(batch_graphs)

def load_llm_predictions(predictions_file, split='test'):
    """Load LLM predictions from a JSON file using the best epoch
    
    Args:
        predictions_file: Path to the JSON file containing LLM predictions
        split: Data split to load predictions for ('train', 'validation', 'test')
        
    Returns:
        Dictionary mapping data index to predicted label
    """
    # Log the function call for debugging
    logger.debug(f"Loading LLM predictions from {predictions_file} for split {split}")
    import json
    import os
    import sys
    
    # Map split names to match those in the predictions file
    split_map = {
        'train': 'train',
        'validation': 'validation',
        'test': 'test'
    }
    
    split_name = split_map.get(split, split)
    
    try:
        # Get the model directory from the predictions file path
        model_dir = os.path.dirname(predictions_file)
        
        # Find the best epoch by analyzing the classification reports
        best_epoch = find_best_epoch(model_dir)
        
        if best_epoch is None:
            logger.warning(f"Could not determine best epoch, using latest epoch from predictions")
            # Fallback to the latest epoch if best epoch cannot be determined
            with open(predictions_file, 'r') as f:
                predictions_data = json.load(f)
            
            max_epoch = 0
            for item in predictions_data:
                if item.get('dataset') == split_name and item.get('epoch', 0) > max_epoch:
                    max_epoch = item.get('epoch', 0)
            best_epoch = max_epoch
        
        # Check if there's an epoch numbering mismatch between classification reports and predictions
        # Load all predictions to check epoch numbering
        with open(predictions_file, 'r') as f:
            predictions_data = json.load(f)
        
        # Get the unique epochs in the predictions file
        pred_epochs = set(item.get('epoch') for item in predictions_data if item.get('epoch') is not None)
        logger.info(f"Epochs in predictions file: {sorted(pred_epochs)}")
        
        # Check if the best_epoch exists in the predictions file
        if best_epoch not in pred_epochs:
            logger.warning(f"Best epoch {best_epoch} not found in predictions file epochs {sorted(pred_epochs)}")
            
            # Check if there's a 0/1-based indexing mismatch
            if best_epoch - 1 in pred_epochs:
                logger.info(f"Found epoch {best_epoch-1} in predictions file, likely a 0/1-based indexing mismatch")
                best_epoch = best_epoch - 1
            elif best_epoch + 1 in pred_epochs:
                logger.info(f"Found epoch {best_epoch+1} in predictions file, likely a 0/1-based indexing mismatch")
                best_epoch = best_epoch + 1
            else:
                # If best epoch is not found at all, use the latest available epoch
                if pred_epochs:
                    latest_epoch = max(pred_epochs)
                    logger.info(f"Using latest available epoch {latest_epoch} from predictions file")
                    best_epoch = latest_epoch
        
        # Load predictions
        with open(predictions_file, 'r') as f:
            predictions_data = json.load(f)
        
        # Create a dictionary mapping data index to predicted label for the best epoch
        predictions = {}
        matched_epoch_count = 0
        
        for item in predictions_data:
            if item.get('dataset') == split_name and item.get('epoch') == best_epoch:
                predictions[item.get('data_index')] = item.get('predicted_label')
                matched_epoch_count += 1
        
        # If we didn't find any predictions for the best epoch, try adjacent epochs
        if matched_epoch_count == 0:
            logger.warning(f"No predictions found for epoch {best_epoch}, trying adjacent epochs")
            
            # Try epochs +/- 1 from the best epoch
            for offset in [-1, 1]:
                test_epoch = best_epoch + offset
                test_count = 0
                
                for item in predictions_data:
                    if item.get('dataset') == split_name and item.get('epoch') == test_epoch:
                        test_count += 1
                
                if test_count > 0:
                    logger.info(f"Found {test_count} predictions for epoch {test_epoch}, using this instead")
                    
                    # Use this epoch instead
                    for item in predictions_data:
                        if item.get('dataset') == split_name and item.get('epoch') == test_epoch:
                            predictions[item.get('data_index')] = item.get('predicted_label')
                    
                    best_epoch = test_epoch
                    break
        
        # Log some sample predictions for debugging
        sample_items = list(predictions.items())[:5]
        logger.debug(f"Sample predictions (data_index: predicted_label): {sample_items}")
        
        # Log statistics about the predictions
        logger.debug(f"Loaded {len(predictions)} LLM predictions for {split_name} split from best epoch {best_epoch}")
        
        # Log the range of data indices
        if predictions:
            min_idx = min(predictions.keys())
            max_idx = max(predictions.keys())
            logger.debug(f"Data index range: {min_idx} to {max_idx}")
        
        return predictions
    
    except Exception as e:
        logger.error(f"Error loading LLM predictions: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return {}

def find_best_epoch(model_dir, metric='f1-score', split='validation'):
    """Find the best epoch based on validation F1 score
    
    Args:
        model_dir: Directory containing model checkpoints and classification reports
        metric: Metric to use for selecting the best epoch (default: 'f1-score')
        split: Data split to use for evaluation (default: 'validation')
        
    Returns:
        Best epoch number or None if it cannot be determined
    """
    import glob
    import json
    import os
    
    # Find all classification reports for the specified split
    report_files = glob.glob(os.path.join(model_dir, f"classification_report_{split}_epoch*.json"))
    
    # If no validation reports are found, try test reports
    if not report_files and split == 'validation':
        logger.warning(f"No classification reports found for validation split, trying test split instead")
        report_files = glob.glob(os.path.join(model_dir, f"classification_report_test_epoch*.json"))
        split = 'test'
    
    if not report_files:
        logger.warning(f"No classification reports found in {model_dir} for split {split}")
        return None
    
    best_score = -1
    best_epoch = None
    
    # Iterate through all epochs
    for report_file in report_files:
        try:
            with open(report_file, 'r') as f:
                report = json.load(f)
            
            # Extract epoch number from filename
            epoch = int(os.path.basename(report_file).split('_')[-1].split('.')[0].replace('epoch', ''))
            
            # For multi-class classification, use macro avg F1-score
            # For binary classification, use weighted avg F1-score
            if 'macro avg' in report and metric in report['macro avg']:
                score = report['macro avg'][metric]
            elif 'weighted avg' in report and metric in report['weighted avg']:
                score = report['weighted avg'][metric]
            else:
                logger.warning(f"Metric {metric} not found in report {report_file}")
                continue
            
            logger.debug(f"Epoch {epoch}: {metric} = {score:.4f}")
            
            # Check if this is the best score so far
            if score > best_score:
                best_score = score
                best_epoch = epoch
        
        except Exception as e:
            logger.error(f"Error processing report file {report_file}: {e}")
            continue
    
    if best_epoch is not None:
        logger.info(f"Best {metric}: {best_score:.4f} (Epoch {best_epoch})")
    
    return best_epoch

def process_embeddings(dataset_name, embeddings_dir, batch_size=200, edge_type='constituency', label_source='original', llm_predictions=None):
    """Process embeddings to create graph structures
    
    Args:
        dataset_name: Name of the dataset
        embeddings_dir: Directory containing embeddings
        batch_size: Batch size for processing
        edge_type: Type of edges to create (constituency)
        label_source: Source of labels ('original' or 'llm')
        llm_predictions: Path to LLM predictions JSON file (required if label_source is 'llm')
        
    Returns:
        output_dir: Directory containing the processed graphs
    """
    # Create output directory
    output_dir = os.path.join(os.path.dirname(os.path.dirname(embeddings_dir)), 'graphs', dataset_name)
    os.makedirs(output_dir, exist_ok=True)
    
    # Load special embeddings
    special_embeddings = load_special_embeddings(embeddings_dir)
    
    # Process each split with enhanced progress bar
    for split in tqdm(['train', 'validation', 'test'], desc="Processing splits", unit="split", position=0, leave=True, colour='blue'):
        split_dir = os.path.join(embeddings_dir, split)
        
        if not os.path.exists(split_dir):
            logger.warning(f"Split directory {split_dir} does not exist, skipping")
            continue
        
        # Create output directory for this split
        # If using LLM predictions, create a separate directory
        if label_source == 'llm':
            split_output_dir = os.path.join(output_dir, f"{split}_llm_labels")
        else:
            split_output_dir = os.path.join(output_dir, split)
            
        os.makedirs(split_output_dir, exist_ok=True)
        
        # Load LLM predictions if needed
        llm_pred_dict = None
        if label_source == 'llm' and llm_predictions:
            logger.info(f"Loading LLM predictions for {split} split from {llm_predictions}")
            if os.path.exists(llm_predictions):
                llm_pred_dict = load_llm_predictions(llm_predictions, split)
                if not llm_pred_dict:
                    logger.warning(f"No LLM predictions loaded for {split} split. Will use dummy labels.")
            else:
                logger.error(f"LLM predictions file not found: {llm_predictions}")
        
        # Get dataset information
        try:
            dataset_info, is_chunked = get_dataset_info(split_dir)
            logger.info(f"Processing {dataset_info['total_samples']} samples for {split} split")
        except Exception as e:
            logger.error(f"Error getting dataset information for {split_dir}: {str(e)}")
            continue
        
        # Save configuration
        config = {
            'edge_type': edge_type,
            'batch_size': batch_size,
            'total_samples': dataset_info['total_samples'],
            'label_source': label_source,
            'dataset_name': dataset_name
        }
        
        config_path = os.path.join(split_output_dir, 'config.json')
        with open(config_path, 'w') as f:
            json.dump(config, f)
        
        # Process embeddings
        if is_chunked:
            # Process each chunk
            chunks_dir = os.path.join(split_dir, 'chunks')
            total_graphs = 0
            
            # Process one chunk at a time to manage memory with enhanced progress tracking
            total_chunks = len(dataset_info['chunk_dirs'])
            chunk_progress = tqdm(dataset_info['chunk_dirs'], desc=f"Processing chunks for {split}", unit="chunk", 
                                  position=1, leave=False, colour='cyan', total=total_chunks)
            for i, chunk_dir_name in enumerate(chunk_progress):
                chunk_dir = os.path.join(chunks_dir, chunk_dir_name)
                chunk_progress.set_postfix({"dir": chunk_dir_name, "progress": f"{i+1}/{total_chunks}"})
                
                try:
                    # Process each chunk file individually
                    chunk_files = [f for f in os.listdir(chunk_dir) if 
                                 (f.endswith('.pkl') and f.startswith('chunk_')) or 
                                 (f.endswith('.npz') and f.startswith('embeddings_chunk_'))]
                    
                    # Sort chunk files numerically
                    def extract_chunk_number(filename):
                        if filename.startswith('chunk_'):
                            return int(filename.replace('chunk_', '').replace('.pkl', ''))
                        elif filename.startswith('embeddings_chunk_'):
                            return int(filename.replace('embeddings_chunk_', '').replace('.npz', ''))
                    
                    chunk_files = sorted(chunk_files, key=extract_chunk_number)
                    
                    # Process each chunk file individually with enhanced progress tracking
                    total_files = len(chunk_files)
                    file_progress = tqdm(chunk_files, desc=f"Files in chunk {chunk_dir_name}", unit="file", 
                                         position=2, leave=False, colour='yellow', total=total_files)
                    for chunk_idx, chunk_file in enumerate(file_progress):
                        chunk_path = os.path.join(chunk_dir, chunk_file)
                        file_progress.set_postfix({"file": chunk_file, "progress": f"{chunk_idx+1}/{total_files}"})
                        
                        # Load single chunk file
                        try:
                            if chunk_file.endswith('.pkl'):
                                with open(chunk_path, 'rb') as f:
                                    chunk_data = pkl.load(f)
                                
                                # Extract data based on structure
                                if isinstance(chunk_data, dict):
                                    word_embeddings = chunk_data.get('word_embeddings', [])
                                    sentence_embeddings = chunk_data.get('sentence_embeddings', [])
                                    texts = chunk_data.get('texts', [])
                                    labels = chunk_data.get('labels', [])
                                elif isinstance(chunk_data, list) and len(chunk_data) == 4:
                                    word_embeddings, sentence_embeddings, texts, labels = chunk_data
                                else:
                                    logger.warning(f"Unknown chunk data format in {chunk_path}")
                                    continue
                                    
                            elif chunk_file.endswith('.npz'):
                                chunk_data = np.load(chunk_path, allow_pickle=True)
                                logger.debug(f"NPZ file keys: {list(chunk_data.keys())}")
                                
                                # Extract arrays - handle different naming conventions and missing keys
                                word_embeddings = None
                                if 'word_embeddings' in chunk_data:
                                    word_embeddings = chunk_data['word_embeddings']
                                    logger.debug(f"Loaded word_embeddings with shape {word_embeddings.shape if hasattr(word_embeddings, 'shape') else 'unknown'}")
                                    
                                # Handle different naming conventions for sentence embeddings
                                sentence_embeddings = None
                                if 'sentence_embeddings' in chunk_data:
                                    sentence_embeddings = chunk_data['sentence_embeddings']
                                elif 'sent_embeddings' in chunk_data:
                                    sentence_embeddings = chunk_data['sent_embeddings']
                                
                                # We don't need texts for graph creation as we use the tree structure
                                # Create an empty list with the right length for compatibility
                                texts = []
                                if word_embeddings is not None:
                                    texts = [""] * len(word_embeddings)
                                
                                # Initialize empty labels - they will be filled from LLM predictions or original dataset
                                labels = []
                            
                            logger.info(f"Loaded {len(word_embeddings)} samples from {chunk_file}")
                            
                            # Calculate number of samples in this chunk
                            chunk_size = len(word_embeddings)
                            
                            # Process in smaller sub-batches with enhanced progress tracking
                            sub_batch_size = batch_size  # Use the full batch size as specified
                            total_batches = int(np.ceil(chunk_size / sub_batch_size))
                            
                            batch_progress = tqdm(range(0, chunk_size, sub_batch_size), 
                                                  desc=f"Batches in {os.path.basename(chunk_file)}", 
                                                  unit="batch", 
                                                  position=3, 
                                                  leave=False, 
                                                  colour='magenta',
                                                  total=total_batches)
                            for j in batch_progress:
                                end_idx = min(j + sub_batch_size, chunk_size)
                                batch_idx_display = j // sub_batch_size + 1
                                batch_progress.set_postfix({
                                    "samples": f"{j}-{end_idx}/{chunk_size}", 
                                    "batch": f"{batch_idx_display}/{total_batches}",
                                    "memory": f"{j/chunk_size:.1%}"
                                })
                                
                                # Extract sub-batch
                                sub_word_embeddings = word_embeddings[j:end_idx]
                                
                                # Handle sentence embeddings safely
                                if sentence_embeddings is not None and len(sentence_embeddings) > 0:
                                    sub_sentence_embeddings = sentence_embeddings[j:end_idx]
                                else:
                                    sub_sentence_embeddings = []
                                
                                # Handle texts safely
                                if texts is not None and len(texts) > 0:
                                    sub_texts = texts[j:end_idx]
                                else:
                                    sub_texts = []
                                
                                # Initialize batch_idx for this sub-batch
                                batch_idx = (i * 1000 + j) // sub_batch_size
                                
                                # Get the correct labels based on the source
                                if label_source == 'llm':
                                    if not llm_pred_dict:
                                        error_msg = "LLM predictions dictionary is empty but LLM label source was specified"
                                        logger.error(error_msg)
                                        raise ValueError(error_msg)
                                    
                                    sub_labels = []
                                    valid_indices = []
                                    missing_indices = []
                                    
                                    # Extract the chunk number from the filename
                                    chunk_num = extract_chunk_number(chunk_file)
                                    
                                    # Calculate the base index for this chunk
                                    # For LLM predictions, we need to track the global index across all chunks
                                    # Each chunk has a fixed size (sub_batch_size) in the LLM predictions indexing
                                    base_index = chunk_num * sub_batch_size
                                    
                                    # Log the base index for debugging
                                    logger.info(f"Base index for chunk {chunk_num}: {base_index}")
                                    
                                    # Special handling for the last chunk
                                    is_last_chunk = chunk_idx == total_files - 1
                                    
                                    for idx in range(j, min(end_idx, len(word_embeddings))):
                                        # For the last chunk, try multiple indexing strategies
                                        if is_last_chunk:
                                            # Try different indexing strategies for the last chunk
                                            strategies = [
                                                base_index + (idx - j),  # Base index + relative position (most accurate)
                                                chunk_num * sub_batch_size + (idx - j),  # Standard indexing
                                                idx,  # Try direct index
                                                len(llm_pred_dict) - (len(word_embeddings) - idx)  # Try reverse mapping from end
                                            ]
                                            
                                            # Try each strategy
                                            found = False
                                            for data_idx in strategies:
                                                if data_idx in llm_pred_dict:
                                                    sub_labels.append(llm_pred_dict[data_idx])
                                                    valid_indices.append(idx - j)
                                                    found = True
                                                    break
                                            
                                            if not found:
                                                # If all strategies fail, try to find the closest available index
                                                closest_idx = min(llm_pred_dict.keys(), key=lambda x: abs(x - strategies[0]))
                                                if abs(closest_idx - strategies[0]) < 100:  # Only use if reasonably close
                                                    sub_labels.append(llm_pred_dict[closest_idx])
                                                    valid_indices.append(idx - j)
                                                    logger.warning(f"Using closest available prediction {closest_idx} for index {strategies[0]} in last chunk")
                                                else:
                                                    missing_indices.append(strategies[0])
                                                    logger.warning(f"No LLM prediction found for last chunk index {strategies[0]}, skipping")
                                        else:
                                            # Standard indexing for non-last chunks
                                            # Use the base_index + relative position in the current chunk
                                            data_idx = base_index + (idx - j)
                                            
                                            if data_idx in llm_pred_dict:
                                                sub_labels.append(llm_pred_dict[data_idx])
                                                valid_indices.append(idx - j)
                                            else:
                                                missing_indices.append(data_idx)
                                                logger.warning(f"No LLM prediction found for data index {data_idx} in chunk {chunk_file}, skipping")
                                    
                                    # Log information about the LLM predictions
                                    logger.info(f"Applying LLM predictions to batch {j}-{end_idx} in chunk {chunk_num}")
                                    logger.info(f"Number of available LLM predictions: {len(llm_pred_dict)}")
                                    logger.info(f"Found {len(sub_labels)}/{end_idx-j} matches with LLM predictions")
                                else:
                                    # For non-LLM source, use original labels from the dataset
                                    # Try multiple approaches to find the original labels
                                    
                                    # First, check if labels were loaded with the chunk data
                                    if labels is not None and len(labels) > 0:
                                        sub_labels = labels[j:end_idx]
                                        logger.info(f"Using {len(sub_labels)} original labels from chunk data")
                                    else:
                                        # Try to load labels from a labels file if it exists
                                        labels_file = os.path.join(os.path.dirname(chunk_path), f"labels_{i}.json")
                                        labels_npz = os.path.join(os.path.dirname(chunk_path), f"labels_{i}.npz")
                                        
                                        if os.path.exists(labels_file):
                                            try:
                                                with open(labels_file, 'r') as f:
                                                    all_labels = json.load(f)
                                                sub_labels = all_labels[j:end_idx]
                                                logger.info(f"Loaded {len(sub_labels)} original labels from {labels_file}")
                                            except Exception as e:
                                                logger.error(f"Error loading original labels from {labels_file}: {str(e)}")
                                                # Continue trying other methods
                                                sub_labels = None
                                        elif os.path.exists(labels_npz):
                                            try:
                                                npz_data = np.load(labels_npz, allow_pickle=True)
                                                if 'labels' in npz_data:
                                                    all_labels = npz_data['labels']
                                                    sub_labels = all_labels[j:end_idx]
                                                    logger.info(f"Loaded {len(sub_labels)} original labels from {labels_npz}")
                                                else:
                                                    logger.error(f"No 'labels' key found in {labels_npz}")
                                                    sub_labels = None
                                            except Exception as e:
                                                logger.error(f"Error loading original labels from {labels_npz}: {str(e)}")
                                                sub_labels = None
                                        else:
                                            # If we can't find labels, check if we can extract them from metadata
                                            metadata_path = os.path.join(os.path.dirname(chunk_dir), 'metadata.json')
                                            if os.path.exists(metadata_path):
                                                try:
                                                    with open(metadata_path, 'r') as f:
                                                        metadata = json.load(f)
                                                    if 'labels' in metadata:
                                                        all_labels = metadata['labels']
                                                        # Calculate the correct indices for this chunk and batch
                                                        start_idx = sum(len(c) for c in word_embeddings[:i]) + j
                                                        end_idx = start_idx + (end_idx - j)
                                                        sub_labels = all_labels[start_idx:end_idx]
                                                        logger.info(f"Loaded {len(sub_labels)} original labels from metadata")
                                                    else:
                                                        logger.error(f"No 'labels' key found in metadata")
                                                        sub_labels = None
                                                except Exception as e:
                                                    logger.error(f"Error loading original labels from metadata: {str(e)}")
                                                    sub_labels = None
                                            else:
                                                sub_labels = None
                                        
                                        # If we still don't have labels, raise an error
                                        if sub_labels is None or len(sub_labels) == 0:
                                            error_msg = f"Could not find original labels for chunk {chunk_file}"
                                            logger.error(error_msg)
                                            raise ValueError(error_msg)
                                
                                # Process the batch with the obtained labels
                                try:
                                    num_graphs = process_embeddings_batch(
                                        sub_word_embeddings, sub_sentence_embeddings, sub_texts, sub_labels,
                                        config, split_output_dir, batch_idx, special_embeddings
                                    )
                                    
                                    # Log memory usage for debugging
                                    import psutil
                                    process = psutil.Process()
                                    logger.debug(f"Memory usage: {process.memory_info().rss / 1024 / 1024:.2f} MB")
                                    
                                except Exception as e:
                                    logger.error(f"Error processing batch: {str(e)}")
                                    # Fall back to original labels if there was an error
                                    logger.info("Falling back to original labels due to error")
                                    num_graphs = process_embeddings_batch(
                                        sub_word_embeddings, sub_sentence_embeddings, sub_texts, sub_labels,
                                        config, split_output_dir, batch_idx, special_embeddings
                                    )
                                else:
                                    # This else block is redundant and causing duplicate processing
                                    # The batch has already been processed in the try block above
                                    pass
                                
                                total_graphs += num_graphs
                                
                                # Force garbage collection to free memory
                                import gc
                                gc.collect()
                                
                            # Clear variables to free memory
                            del word_embeddings, sentence_embeddings, texts, labels, chunk_data
                            gc.collect()
                            
                        except Exception as e:
                            logger.error(f"Error processing chunk file {chunk_path}: {str(e)}")
                            continue
                    
                except Exception as e:
                    logger.error(f"Error processing chunk {chunk_dir}: {str(e)}")
                    import traceback
                    logger.error(traceback.format_exc())
            
            logger.info(f"Processed {total_graphs} graphs for {split} split")
        
        else:
            # Load all embeddings
            try:
                # Load metadata
                metadata_path = os.path.join(split_dir, 'metadata.pkl')
                with open(metadata_path, 'rb') as f:
                    metadata = pkl.load(f)
                
                # Load word embeddings
                word_embeddings_path = os.path.join(split_dir, 'word_embeddings.pkl')
                with open(word_embeddings_path, 'rb') as f:
                    word_embeddings = pkl.load(f)
                
                # Load sentence embeddings
                sentence_embeddings_path = os.path.join(split_dir, 'sentence_embeddings.pkl')
                with open(sentence_embeddings_path, 'rb') as f:
                    sentence_embeddings = pkl.load(f)
                
                # Process in batches with enhanced progress tracking
                total_samples = len(word_embeddings)
                total_graphs = 0
                
                # Use smaller sub-batches for large datasets
                sub_batch_size = batch_size  # Use the full batch size as specified
                total_batches = int(np.ceil(total_samples / sub_batch_size))
                
                batch_progress = tqdm(range(0, total_samples, sub_batch_size), 
                                      desc=f"Processing {split} batches", 
                                      unit="batch",
                                      position=1,
                                      leave=False,
                                      colour='cyan',
                                      total=total_batches)
                for i in batch_progress:
                    batch_end = min(i + sub_batch_size, total_samples)
                    batch_idx_display = i // sub_batch_size + 1
                    batch_progress.set_postfix({
                        "samples": f"{i}-{batch_end}/{total_samples}",
                        "batch": f"{batch_idx_display}/{total_batches}",
                        "progress": f"{i/total_samples:.1%}"
                    })
                    
                    # Get batch data
                    batch_word_embeddings = word_embeddings[i:batch_end]
                    batch_sentence_embeddings = sentence_embeddings[i:batch_end]
                    batch_texts = metadata['texts'][i:batch_end]
                    batch_labels = metadata['labels'][i:batch_end]
                    
                    # Apply LLM predictions if needed
                    if label_source == 'llm' and llm_pred_dict:
                        # Try to match data indices to LLM predictions
                        modified_labels = []
                        for idx, label in enumerate(batch_labels):
                            # Use data index if available, otherwise use position in batch
                            data_idx = i + idx
                            if data_idx in llm_pred_dict:
                                modified_labels.append(llm_pred_dict[data_idx])
                            else:
                                modified_labels.append(label)
                                logger.warning(f"No LLM prediction found for data index {data_idx}, using original label")
                        
                        # Process batch with modified labels
                        num_graphs = process_embeddings_batch(
                            batch_word_embeddings, batch_sentence_embeddings, batch_texts, modified_labels,
                            config, split_output_dir, i // sub_batch_size, special_embeddings
                        )
                    else:
                        # Process batch with original labels
                        num_graphs = process_embeddings_batch(
                            batch_word_embeddings, batch_sentence_embeddings, batch_texts, batch_labels,
                            config, split_output_dir, i // sub_batch_size, special_embeddings
                        )
                    
                    total_graphs += num_graphs
                    
                    # Force garbage collection to free memory
                    import gc
                    gc.collect()
                
                logger.info(f"Processed {total_graphs} graphs for {split} split")
                
            except Exception as e:
                logger.error(f"Error processing embeddings for {split_dir}: {str(e)}")
                import traceback
                logger.error(traceback.format_exc())
    
    return output_dir

def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description="Process embeddings to create graph structures")
    parser.add_argument("--dataset_name", type=str, required=True, help="Name of the dataset")
    parser.add_argument("--embeddings_dir", type=str, required=True, help="Directory containing embeddings")
    parser.add_argument("--batch_size", type=int, default=200, help="Batch size for processing")
    parser.add_argument("--edge_type", type=str, default="constituency", choices=["constituency"], help="Type of edges to create")
    parser.add_argument("--label_source", type=str, default="original", choices=["original", "llm"], 
                        help="Source of labels: 'original' (use original dataset labels) or 'llm' (use LLM predictions)")
    parser.add_argument("--llm_predictions", type=str, help="Path to LLM predictions JSON file (required if label_source is 'llm')")
    
    args = parser.parse_args()
    
    # Validate arguments
    if args.label_source == "llm" and not args.llm_predictions:
        parser.error("--llm_predictions is required when --label_source is 'llm'")
    
    return args


def determine_embedding_dimension(word_embeddings, sentence_embedding):
    """Determine the embedding dimension from available embeddings"""
    import torch
    
    embedding_dim = None
    if word_embeddings and len(word_embeddings) > 0:
        if isinstance(word_embeddings[0], torch.Tensor):
            embedding_dim = word_embeddings[0].shape[0]
        else:
            embedding_dim = len(word_embeddings[0])
    elif sentence_embedding is not None:
        if isinstance(sentence_embedding, torch.Tensor):
            embedding_dim = sentence_embedding.shape[0]
        else:
            embedding_dim = len(sentence_embedding)
    else:
        # Default embedding dimension if no embeddings provided
        embedding_dim = 2
    
    return embedding_dim


def identify_word_nodes(G, leaf_nodes):
    """Identify word nodes from leaf nodes or graph structure"""
    import logging
    logger = logging.getLogger(__name__)
    
    word_nodes = None
    
    if leaf_nodes:
        # First try to find numeric leaf nodes (these are typically word nodes)
        numeric_leaf_nodes = []
        for node in leaf_nodes:
            try:
                # Check if node is numeric (either integer or string that can be converted to int)
                if isinstance(node, int) or (isinstance(node, str) and node.isdigit()):
                    numeric_leaf_nodes.append(node)
            except Exception:
                pass
        
        if numeric_leaf_nodes:
            word_nodes = numeric_leaf_nodes
            logger.debug(f"Using {len(word_nodes)} numeric leaf nodes as word nodes")
        else:
            # If no numeric leaf nodes, use all leaf nodes
            word_nodes = leaf_nodes
            logger.debug(f"Using all {len(word_nodes)} leaf nodes as word nodes")
    
    # If we still don't have word nodes, identify them from the graph structure
    if not word_nodes:
        # Use nodes with no outgoing edges (leaf nodes in the graph)
        leaf_node_ids = [n for n, d in G.out_degree() if d == 0]
        word_nodes = leaf_node_ids
        logger.debug(f"Identified {len(word_nodes)} leaf nodes from graph structure")
    
    return word_nodes


def assign_word_embeddings(node_features, word_nodes, word_embeddings, embedding_dim):
    """Assign word embeddings to word nodes"""
    import torch
    import logging
    logger = logging.getLogger(__name__)
    
    x = {node_id: torch.zeros(embedding_dim) for node_id in node_features}
    
    # Create a mapping between word nodes and their positions in the sentence
    word_node_positions = {}
    
    # Try to determine the position of each word node in the sentence
    for node_id in word_nodes:
        # First try to use the node ID as a position indicator
        try:
            if isinstance(node_id, int):
                word_node_positions[node_id] = node_id
            elif isinstance(node_id, str) and node_id.isdigit():
                word_node_positions[node_id] = int(node_id)
            continue
        except (ValueError, TypeError):
            pass
        
        # Then try to use the node label as a position indicator
        try:
            node_label = node_features.get(node_id, '')
            if isinstance(node_label, str) and node_label.isdigit():
                word_node_positions[node_id] = int(node_label)
                continue
            elif isinstance(node_label, int):
                word_node_positions[node_id] = node_label
                continue
        except (ValueError, TypeError):
            pass
    
    # Sort word nodes by their positions
    sorted_word_nodes = []
    if word_node_positions:
        # Sort by position if we have position information
        sorted_items = sorted(word_node_positions.items(), key=lambda x: x[1])
        sorted_word_nodes = [node_id for node_id, _ in sorted_items]
        logger.debug(f"Sorted {len(sorted_word_nodes)} word nodes by position")
    else:
        # Otherwise just use the nodes as they are
        sorted_word_nodes = list(word_nodes)
        logger.debug("Using unsorted word nodes")
    
    # Assign embeddings to word nodes
    assigned_count = 0
    for i, node_id in enumerate(sorted_word_nodes):
        if i < len(word_embeddings):
            x[node_id] = word_embeddings[i]
            assigned_count += 1
    
    logger.debug(f"Assigned {assigned_count} embeddings to word nodes")
    
    # If we didn't assign any embeddings, try a direct assignment to all nodes
    if assigned_count == 0 and word_embeddings:
        logger.warning("Failed to assign any embeddings to word nodes")
        
        # Try a direct assignment to all nodes as a last resort
        if len(node_features) <= len(word_embeddings):
            logger.debug("Attempting direct assignment to all nodes")
            for i, node_id in enumerate(node_features.keys()):
                if i < len(word_embeddings):
                    x[node_id] = word_embeddings[i]
    
    return x


def compute_internal_node_embeddings(x, node_order, edge_indices, embedding_dim):
    """Compute embeddings for internal nodes by averaging child embeddings"""
    import torch
    import numpy as np
    import logging
    logger = logging.getLogger(__name__)
    
    # Process nodes in topological order (from leaves to root)
    for node_id in node_order:
        children = [c for p, c in edge_indices if p == node_id]
        if children:  # If this is not a leaf node
            # Only include children that have valid tensor embeddings
            valid_child_embeddings = []
            for c in children:
                if c in x:
                    # Ensure the embedding is a tensor and has the right shape
                    if isinstance(x[c], torch.Tensor) and x[c].dim() == 1 and x[c].shape[0] == embedding_dim:
                        valid_child_embeddings.append(x[c])
                    elif isinstance(x[c], (list, tuple, np.ndarray)):
                        # Convert to tensor if it's not already
                        try:
                            tensor_embedding = torch.tensor(x[c], dtype=torch.float)
                            if tensor_embedding.dim() == 1 and tensor_embedding.shape[0] == embedding_dim:
                                valid_child_embeddings.append(tensor_embedding)
                        except Exception as e:
                            logger.debug(f"Could not convert embedding for node {c} to tensor: {str(e)}")
            
            # Compute mean of valid child embeddings
            if valid_child_embeddings:
                try:
                    x[node_id] = torch.stack(valid_child_embeddings).mean(dim=0)
                    logger.debug(f"Computed embedding for non-leaf node {node_id} from {len(valid_child_embeddings)} children")
                except Exception as e:
                    logger.warning(f"Error computing mean of child embeddings for node {node_id}: {str(e)}")
                    # If we can't compute the mean, just use the first valid child embedding
                    if valid_child_embeddings:
                        x[node_id] = valid_child_embeddings[0]
                        logger.debug(f"Using first valid child embedding for node {node_id}")
    
    return x


def create_node_feature_tensor(x, node_features, embedding_dim):
    """Convert node features dictionary to a tensor"""
    import torch
    import numpy as np
    import logging
    logger = logging.getLogger(__name__)
    
    node_ids = sorted(node_features.keys())
    valid_x_tensors = []
    
    for node_id in node_ids:
        if node_id in x:
            # Ensure the embedding is a valid tensor
            if isinstance(x[node_id], torch.Tensor) and x[node_id].dim() == 1 and x[node_id].shape[0] == embedding_dim:
                valid_x_tensors.append(x[node_id])
            elif isinstance(x[node_id], (list, tuple, np.ndarray)):
                try:
                    tensor_embedding = torch.tensor(x[node_id], dtype=torch.float)
                    if tensor_embedding.dim() == 1 and tensor_embedding.shape[0] == embedding_dim:
                        valid_x_tensors.append(tensor_embedding)
                except Exception:
                    pass
    
    # Stack valid tensors or create an empty tensor
    if valid_x_tensors:
        x_tensor = torch.stack(valid_x_tensors)
    else:
        x_tensor = torch.zeros(0, embedding_dim)
    
    return x_tensor, node_ids


def ensure_valid_node_features(x_tensor, embedding_dim, sentence_embedding, word_embeddings):
    """Ensure we have valid node features"""
    import torch
    import logging
    logger = logging.getLogger(__name__)
    
    # Check if we have valid node features
    if x_tensor.shape[0] == 0 or (x_tensor.shape[0] > 0 and torch.all(x_tensor == 0)):
        logger.warning("No valid node features detected, ensuring we have at least one valid node")
        
        # Use sentence embedding if available
        if sentence_embedding is not None and isinstance(sentence_embedding, torch.Tensor):
            try:
                # Ensure sentence embedding is the right shape
                if sentence_embedding.dim() == 1 and sentence_embedding.shape[0] == embedding_dim:
                    x_tensor = sentence_embedding.unsqueeze(0)
                    logger.debug("Using sentence embedding as node feature")
                else:
                    logger.warning(f"Sentence embedding has wrong shape: {sentence_embedding.shape}, expected ({embedding_dim},)")
                    x_tensor = torch.ones(1, embedding_dim)  # Use ones instead of random
            except Exception as e:
                logger.warning(f"Error using sentence embedding: {str(e)}")
                x_tensor = torch.ones(1, embedding_dim)  # Use ones instead of random
        # Use first word embedding if available
        elif word_embeddings is not None and len(word_embeddings) > 0:
            try:
                first_embedding = word_embeddings[0]
                if isinstance(first_embedding, torch.Tensor):
                    x_tensor = first_embedding.unsqueeze(0)
                else:
                    x_tensor = torch.tensor(first_embedding, dtype=torch.float).unsqueeze(0)
                logger.debug("Using first word embedding as node feature")
            except Exception as e:
                logger.warning(f"Error using first word embedding: {str(e)}")
                x_tensor = torch.ones(1, embedding_dim)  # Use ones instead of random
        else:
            # As a last resort, create a single node with non-zero features
            logger.warning("No embeddings available, creating non-zero features")
            x_tensor = torch.ones((1, embedding_dim))
    
    return x_tensor


def create_edge_index_tensor(edge_indices, node_ids):
    """Create edge index tensor from edge indices"""
    import torch
    import logging
    logger = logging.getLogger(__name__)
    
    # Map node IDs to consecutive integers
    node_to_idx = {node_id: i for i, node_id in enumerate(node_ids)}
    
    # Ensure we have valid edges (both parent and child exist in node_to_idx)
    valid_edges = [(p, c) for p, c in edge_indices if p in node_to_idx and c in node_to_idx]
    
    # If no valid edges but we have nodes, create self-loops for each node
    if not valid_edges and node_ids:
        valid_edges = [(node_id, node_id) for node_id in node_ids]
        logger.warning(f"No valid edges found, creating self-loops for {len(node_ids)} nodes")
    
    # Convert edges to tensor
    if valid_edges:
        # Map node IDs to consecutive indices
        edge_index = torch.tensor([
            [node_to_idx[p], node_to_idx[c]] for p, c in valid_edges
        ], dtype=torch.long).t()
    else:
        # If we somehow still have no valid edges, create a self-loop for node 0
        edge_index = torch.tensor([[0, 0]], dtype=torch.long).t()
        logger.warning("No valid edges possible, created fallback self-loop for node 0")
    
    return edge_index


def create_graph_from_tree(node_features, edge_indices, word_embeddings, sentence_embedding, text, label, leaf_nodes=None, word_nodes=None):
    """Create a PyTorch Geometric graph from a tree structure
    
    Args:
        node_features: Dictionary mapping node IDs to node features
        edge_indices: List of (parent, child) tuples representing edges
        word_embeddings: List of word embeddings for the leaf nodes
        sentence_embedding: Embedding for the entire sentence
        text: Original text
        label: Label for the graph
        leaf_nodes: Optional list of leaf node IDs
        word_nodes: Optional list of word node IDs (subset of leaf nodes that correspond to words)
        
    Returns:
        PyTorch Geometric Data object
    """
    import torch
    import networkx as nx
    from torch_geometric.data import Data
    import logging
    logger = logging.getLogger(__name__)
    
    # Determine embedding dimension
    embedding_dim = determine_embedding_dimension(word_embeddings, sentence_embedding)
    
    # Create a directed graph to determine topological order
    G = nx.DiGraph()
    G.add_nodes_from(node_features.keys())
    G.add_edges_from(edge_indices)
    
    # Determine node ordering for embedding propagation
    try:
        # Try to get topological sort (from leaves to root)
        node_order = list(reversed(list(nx.topological_sort(G))))
        logger.debug("Using topological sort for node ordering")
    except nx.NetworkXUnfeasible:
        # Graph has cycles, use simple node list
        logger.warning("Graph has cycles, using simple node list instead of topological sort")
        node_order = list(node_features.keys())
    
    # Log information about the tree structure
    logger.debug(f"Tree has {len(node_features)} nodes, {len(edge_indices)} edges")
    logger.debug(f"Found {len(leaf_nodes) if leaf_nodes else 0} leaf nodes and {len(word_nodes) if word_nodes else 0} word nodes")
    
    # Identify word nodes if not provided
    if not word_nodes and word_embeddings and len(word_embeddings) > 0:
        word_nodes = identify_word_nodes(G, leaf_nodes)
        
        # If still no word nodes, we can't assign embeddings properly
        if not word_nodes:
            logger.warning("No word nodes could be identified from the tree structure")
            return Data(x=torch.ones(1, embedding_dim), 
                       edge_index=torch.tensor([[0, 0]], dtype=torch.long).t(), 
                       y=torch.tensor([label], dtype=torch.long) if label is not None else torch.tensor([0], dtype=torch.long))
    
    # Assign word embeddings to word nodes
    if word_embeddings and len(word_embeddings) > 0 and word_nodes:
        x = assign_word_embeddings(node_features, word_nodes, word_embeddings, embedding_dim)
    else:
        # Initialize node embeddings dictionary with zeros
        x = {node_id: torch.zeros(embedding_dim) for node_id in node_features}
    
    # Compute internal node embeddings
    x = compute_internal_node_embeddings(x, node_order, edge_indices, embedding_dim)
    
    # Convert node features dictionary to tensor
    x_tensor, node_ids = create_node_feature_tensor(x, node_features, embedding_dim)
    
    # Ensure we have valid node features
    x_tensor = ensure_valid_node_features(x_tensor, embedding_dim, sentence_embedding, wor

def create_graph_from_tree(node_features, edge_indices, word_embeddings, sentence_embedding, text, label, leaf_nodes=None, word_nodes=None):
    """Create a PyTorch Geometric graph from a tree structure
    
    Args:
        node_features: Dictionary mapping node IDs to node features
        edge_indices: List of (parent, child) tuples representing edges
        word_embeddings: List of word embeddings for the leaf nodes
        sentence_embedding: Embedding for the entire sentence
        text: Original text
        label: Label for the graph
        leaf_nodes: Optional list of leaf node IDs
        word_nodes: Optional list of word node IDs (subset of leaf nodes that correspond to words)
        
    Returns:
        PyTorch Geometric Data object
    """
    import torch
    import networkx as nx
    from torch_geometric.data import Data
    import logging
    logger = logging.getLogger(__name__)
    
    # Determine embedding dimension
    embedding_dim = determine_embedding_dimension(word_embeddings, sentence_embedding)
    


    # Create a directed graph to determine topological order
    G = nx.DiGraph()
    G.add_nodes_from(node_features.keys())
    G.add_edges_from(edge_indices)
    
    # Determine node ordering for embedding propagation
    try:
        # Try to get topological sort (from leaves to root)
        node_order = list(reversed(list(nx.topological_sort(G))))
        logger.debug("Using topological sort for node ordering")
    except nx.NetworkXUnfeasible:
        # Graph has cycles, use simple node list
        logger.warning("Graph has cycles, using simple node list instead of topological sort")
        node_order = list(node_features.keys())
    
    # Log information about the tree structure
    logger.debug(f"Tree has {len(node_features)} nodes, {len(edge_indices)} edges")
    logger.debug(f"Found {len(leaf_nodes) if leaf_nodes else 0} leaf nodes and {len(word_nodes) if word_nodes else 0} word nodes")
    
    # Identify word nodes if not provided
    if not word_nodes and word_embeddings and len(word_embeddings) > 0:
        word_nodes = identify_word_nodes(G, leaf_nodes)
        
        # If still no word nodes, we can't assign embeddings properly
        if not word_nodes:
            logger.warning("No word nodes could be identified from the tree structure")
            return Data(x=torch.ones(1, embedding_dim), 
                       edge_index=torch.tensor([[0, 0]], dtype=torch.long).t(), 
                       y=torch.tensor([label], dtype=torch.long) if label is not None else torch.tensor([0], dtype=torch.long))
    
    # Assign word embeddings to word nodes
    if word_embeddings and len(word_embeddings) > 0 and word_nodes:
        x = assign_word_embeddings(node_features, word_nodes, word_embeddings, embedding_dim)
    else:
        # Initialize node embeddings dictionary with zeros
        x = {node_id: torch.zeros(embedding_dim) for node_id in node_features}
    
    # Compute internal node embeddings
    x = compute_internal_node_embeddings(x, node_order, edge_indices, embedding_dim)
    
    # Convert node features dictionary to tensor
    x_tensor, node_ids = create_node_feature_tensor(x, node_features, embedding_dim)
    
    # Ensure we have valid node features
    x_tensor = ensure_valid_node_features(x_tensor, embedding_dim, sentence_embedding, word_embeddings)
    
    # Create edge index tensor
    edge_index = create_edge_index_tensor(edge_indices, node_ids)
    
    # Ensure edge indices don't exceed node count
    if edge_index.shape[1] > 0 and edge_index.max() >= x_tensor.shape[0]:
        logger.warning(f"Edge indices ({edge_index.max()}) exceed node count ({x_tensor.shape[0]}), creating self-loops")
        # Create self-loops for each node as a safe fallback
        edge_index = torch.tensor([[i, i] for i in range(x_tensor.shape[0])], dtype=torch.long).t()
    
    # Create the graph data object
    y = torch.tensor([label], dtype=torch.long) if label is not None else torch.tensor([0], dtype=torch.long)
    data = Data(x=x_tensor, edge_index=edge_index, y=y)
    
    # Add text as an attribute
    if text is not None:
        data.text = text
    
    # Add label as a string attribute
    if label is not None:
        # Convert label to string representation
        if isinstance(label, int):
            # Map numeric labels to class names if available
            label_map = {0: "world", 1: "sports", 2: "business", 3: "sci/tech"}
            label_str = label_map.get(label, str(label))
        else:
            label_str = str(label)
        
        data.label_str = label_str
    
    return data


def main():
    """Main entry point"""
    args = parse_args()
    
    # Display processing information
    print(f"\n{'='*80}")
    print(f"Starting graph generation for dataset: {args.dataset_name}")
    print(f"Embeddings directory: {args.embeddings_dir}")
    print(f"Batch size: {args.batch_size}")
    print(f"Edge type: {args.edge_type}")
    print(f"Label source: {args.label_source}")
    if args.label_source == 'llm':
        print(f"LLM predictions file: {args.llm_predictions}")
    print(f"{'='*80}\n")
    
    # Process embeddings
    output_dir = process_embeddings(
        dataset_name=args.dataset_name,
        embeddings_dir=args.embeddings_dir,
        batch_size=args.batch_size,
        edge_type=args.edge_type,
        label_source=args.label_source,
        llm_predictions=args.llm_predictions if args.label_source == 'llm' else None
    )
    
    if output_dir:
        print(f"\n{'='*80}")
        print(f"Graph processing completed successfully!")
        print(f"Graphs saved to: {output_dir}")
        print(f"{'='*80}\n")
        logger.info(f"Graph processing completed successfully. Graphs saved to: {output_dir}")
    else:
        print(f"\n{'='*80}")
        print(f"Graph processing failed!")
        print(f"{'='*80}\n")
        logger.error("Graph processing failed")


if __name__ == "__main__":
    main()