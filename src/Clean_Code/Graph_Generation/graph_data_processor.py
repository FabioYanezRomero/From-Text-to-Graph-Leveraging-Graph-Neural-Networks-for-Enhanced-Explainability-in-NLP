"""
Graph Data Processor

This module processes word embeddings and prepares them for use in Graph Neural Networks.
It creates graph structures from text data using the embeddings generated by the embedding_generator.py.
"""

import os
import argparse
import logging
import json
import pickle as pkl
import numpy as np
from tqdm import tqdm
import torch
import torch.nn.functional as F
import torch_geometric
from torch_geometric.data import Data
import networkx as nx

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

def load_embeddings(input_dir, dataset, embedding_model, split):
    """Load word and sentence embeddings from disk, handling chunked data structure.
    
    Args:
        input_dir: Base directory containing the input embeddings
        dataset: Name of the dataset
        embedding_model: Name of the embedding model used
        split: Data split ('train', 'val', 'test')
        
    Returns:
        tuple: (texts, labels, word_embeddings, sentence_embeddings)
    """
    import os
    import pickle
    import numpy as np
    from glob import glob
    
    # Construct base path
    base_path = os.path.join(input_dir, dataset, split)
    
    # Find all chunk directories
    chunk_dirs = sorted([d for d in glob(os.path.join(base_path, 'chunk_*')) if os.path.isdir(d)])
    
    if not chunk_dirs:
        logger.error(f"No chunk directories found in {base_path}")
        return None
    
    all_word_embeddings = []
    all_sentence_embeddings = []
    all_texts = []
    all_labels = []
    
    logger.info(f"Found {len(chunk_dirs)} chunks in {base_path}")
    
    try:
        # Process each chunk
        for chunk_dir in chunk_dirs:
            try:
                # Load data from chunk
                word_embeddings_path = os.path.join(chunk_dir, 'word_embeddings.pkl')
                sentence_embeddings_path = os.path.join(chunk_dir, 'sentence_embeddings.pkl')
                metadata_path = os.path.join(chunk_dir, 'metadata.pkl')
                
                # Check if required files exist
                if not all(os.path.exists(p) for p in [word_embeddings_path, sentence_embeddings_path, metadata_path]):
                    logger.warning(f"Skipping incomplete chunk: {chunk_dir}")
                    continue
                
                # Load metadata which contains texts and labels
                with open(metadata_path, 'rb') as f:
                    metadata = pickle.load(f)
                
                # Load word and sentence embeddings
                with open(word_embeddings_path, 'rb') as f:
                    word_embeddings = pickle.load(f)
                with open(sentence_embeddings_path, 'rb') as f:
                    sentence_embeddings = pickle.load(f)
                
                # Extract texts and labels from metadata
                texts = [item['text'] for item in metadata]
                labels = [item['label'] for item in metadata]
                
                # Convert to lists if they're numpy arrays
                if isinstance(word_embeddings, np.ndarray):
                    word_embeddings = list(word_embeddings)
                if isinstance(sentence_embeddings, np.ndarray):
                    sentence_embeddings = list(sentence_embeddings)
                if isinstance(texts, np.ndarray):
                    texts = list(texts)
                if isinstance(labels, np.ndarray):
                    labels = list(labels)
                
                # Add to the combined lists
                all_word_embeddings.extend(word_embeddings)
                all_sentence_embeddings.extend(sentence_embeddings)
                all_texts.extend(texts)
                all_labels.extend(labels)
                
                logger.info(f"Loaded {len(word_embeddings)} samples from {os.path.basename(chunk_dir)}")
                
            except Exception as e:
                logger.error(f"Error loading chunk {chunk_dir}: {str(e)}")
                import traceback
                logger.error(traceback.format_exc())
        
        if not all_word_embeddings:
            logger.error("No data was loaded from any chunk")
            return None
            
        logger.info(f"Successfully loaded {len(all_word_embeddings)} total samples from {len(chunk_dirs)} chunks")
        return all_texts, all_labels, all_word_embeddings, all_sentence_embeddings
        
    except Exception as e:
        logger.error(f"Error in load_embeddings: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return None

def save_graphs(graphs, output_dir, batch_size=32, num_workers=4):
    """Save a list of PyTorch Geometric Data objects to disk.
    
    Args:
        graphs: List of PyTorch Geometric Data objects
        output_dir: Directory to save the graphs
        batch_size: Number of graphs to save in each file
        num_workers: Number of worker processes to use
    """
    import os
    import torch
    from torch_geometric.data import Data, Batch
    from torch_geometric.loader import DataLoader
    
    os.makedirs(output_dir, exist_ok=True)
    
    # Create a DataLoader to handle batching
    loader = DataLoader(graphs, batch_size=batch_size, shuffle=False, num_workers=num_workers)
    
    # Save each batch
    for batch_idx, batch in enumerate(loader):
        batch_path = os.path.join(output_dir, f'batch_{batch_idx:04d}.pt')
        try:
            torch.save(batch, batch_path)
            logger.info(f"Saved batch {batch_idx} with {len(batch)} graphs to {batch_path}")
        except Exception as e:
            logger.error(f"Error saving batch {batch_idx}: {str(e)}")
    
    logger.info(f"Saved {len(graphs)} graphs in {len(loader)} batches to {output_dir}")

def get_dataset_info(embeddings_dir):
    """Get information about the dataset without loading all embeddings
    
    Args:
        embeddings_dir: Directory containing embeddings
        
    Returns:
        dataset_info: Dictionary containing dataset information
        is_chunked: Whether embeddings are stored in chunks
    """
    # First, check if we need to look in a nested directory structure
    metadata_path = os.path.join(embeddings_dir, 'metadata.json')
    
    # If metadata.json is not found directly, look for subdirectories that might contain it
    if not os.path.exists(metadata_path):
        subdirs = [d for d in os.listdir(embeddings_dir) if os.path.isdir(os.path.join(embeddings_dir, d))]
        
        for subdir in subdirs:
            nested_path = os.path.join(embeddings_dir, subdir, 'metadata.json')
            if os.path.exists(nested_path):
                logger.info(f"Found metadata in nested directory: {nested_path}")
                metadata_path = nested_path
                embeddings_dir = os.path.join(embeddings_dir, subdir)
                break
    
    # Check if embeddings are stored in chunks
    embedding_chunks_dir = os.path.join(embeddings_dir, 'embedding_chunks')
    is_chunked = os.path.exists(embedding_chunks_dir)
    
    if not os.path.exists(metadata_path):
        raise FileNotFoundError(f"Metadata file not found in {embeddings_dir} or its subdirectories")
    
    with open(metadata_path, 'r') as f:
        metadata = json.load(f)
    
    if is_chunked:
        # Get list of chunk files
        chunk_files = sorted([f for f in os.listdir(embedding_chunks_dir) if f.endswith('.pkl')])
        
        dataset_info = {
            'total_samples': metadata.get('num_samples', 0),
            'chunk_dirs': [embedding_chunks_dir],
            'num_chunks': len(chunk_files)
        }
    else:
        dataset_info = {
            'total_samples': metadata.get('num_samples', 0)
        }
    
    return dataset_info, is_chunked

def load_chunk_from_disk(chunk_dir):
    """Load a chunk of embeddings from disk
    
    Args:
        chunk_dir: Directory containing chunk data
        
    Returns:
        word_embeddings: List of word embeddings
        sentence_embeddings: List of sentence embeddings
        texts: List of texts
        labels: List of labels
    """
    # Get all chunk files in the directory
    # Look for both old format (.pkl files with chunk_ prefix) and new format (.npz files with embeddings_chunk_ prefix)
    chunk_files = [f for f in os.listdir(chunk_dir) if 
                 (f.endswith('.pkl') and f.startswith('chunk_')) or 
                 (f.endswith('.npz') and f.startswith('embeddings_chunk_'))]
    
    # Sort files numerically by their index to maintain original order
    def extract_chunk_number(filename):
        if filename.startswith('chunk_'):
            # Extract number from old format: chunk_X.pkl
            return int(filename.replace('chunk_', '').replace('.pkl', ''))
        elif filename.startswith('embeddings_chunk_'):
            # Extract number from new format: embeddings_chunk_X.npz
            return int(filename.replace('embeddings_chunk_', '').replace('.npz', ''))
        return 0
    
    chunk_files = sorted(chunk_files, key=extract_chunk_number)
    
    if not chunk_files:
        raise FileNotFoundError(f"No chunk files found in {chunk_dir}")
    
    # Initialize lists to store all data
    word_embeddings = []
    sentence_embeddings = []
    texts = []
    labels = []
    
    # Process all chunks
    for chunk_file in chunk_files:
        chunk_path = os.path.join(chunk_dir, chunk_file)
        
        # Handle different file formats
        if chunk_file.endswith('.pkl'):
            # Old format with pickle files
            with open(chunk_path, 'rb') as f:
                chunk_data = pkl.load(f)
                
            # Extract data based on the structure of the chunk
            if isinstance(chunk_data, dict):
                # New format with dictionary structure
                word_embeddings.extend(chunk_data.get('word_embeddings', []))
                sentence_embeddings.extend(chunk_data.get('sentence_embeddings', []))
                texts.extend(chunk_data.get('texts', []))
                labels.extend(chunk_data.get('labels', []))
            elif isinstance(chunk_data, tuple) and len(chunk_data) >= 2:
                # Old format with tuple structure
                word_embeddings.extend(chunk_data[0])
                sentence_embeddings.extend(chunk_data[1])
                if len(chunk_data) > 2:
                    texts.extend(chunk_data[2])
                if len(chunk_data) > 3:
                    labels.extend(chunk_data[3])
        elif chunk_file.endswith('.npz'):
            # New format with numpy compressed files
            try:
                chunk_data = np.load(chunk_path, allow_pickle=True)
                
                # Extract arrays from the npz file
                if 'word_embeddings' in chunk_data:
                    word_embeddings.extend(chunk_data['word_embeddings'])
                if 'sentence_embeddings' in chunk_data:
                    sentence_embeddings.extend(chunk_data['sentence_embeddings'])
                if 'texts' in chunk_data:
                    texts.extend(chunk_data['texts'])
                if 'labels' in chunk_data:
                    labels.extend(chunk_data['labels'])
                    
                logger.info(f"Loaded {len(chunk_data['word_embeddings']) if 'word_embeddings' in chunk_data else 0} samples from {chunk_file}")
            except Exception as e:
                logger.error(f"Error loading .npz file {chunk_path}: {e}")
                continue
    
    # If labels are missing, create dummy labels
    if not labels and word_embeddings:
        logger.warning(f"No labels found in chunks, creating dummy labels")
        labels = [0] * len(word_embeddings)
    
    return word_embeddings, sentence_embeddings, texts, labels

def load_special_embeddings(embeddings_dir):
    """Load special embeddings for constituency tokens
    
    Args:
        embeddings_dir: Directory containing embeddings
        
    Returns:
        special_embeddings: Dictionary of special embeddings
    """
    # Check if special embeddings exist in main directory
    special_embeddings_path = os.path.join(embeddings_dir, 'special_embeddings.pkl')
    
    if os.path.exists(special_embeddings_path):
        with open(special_embeddings_path, 'rb') as f:
            special_embeddings = pkl.load(f)
        return special_embeddings
    
    # Check if special embeddings exist in special directory
    special_dir = os.path.join(os.path.dirname(embeddings_dir), 'special')
    special_embeddings_path = os.path.join(special_dir, 'special_embeddings.pkl')
    
    if os.path.exists(special_embeddings_path):
        with open(special_embeddings_path, 'rb') as f:
            special_embeddings = pkl.load(f)
        return special_embeddings
    
    # Check if we need to look in subdirectories
    subdirs = [d for d in os.listdir(embeddings_dir) if os.path.isdir(os.path.join(embeddings_dir, d))]
    for subdir in subdirs:
        nested_path = os.path.join(embeddings_dir, subdir, 'special_embeddings.pkl')
        if os.path.exists(nested_path):
            logger.info(f"Found special embeddings in nested directory: {nested_path}")
            with open(nested_path, 'rb') as f:
                special_embeddings = pkl.load(f)
            return special_embeddings
    
    logger.warning(f"No special embeddings found in {embeddings_dir} or its subdirectories")
    return None

def load_constituency_tree(tree_path):
    """Load a pre-generated constituency tree from file
    
    Args:
        tree_path: Path to the pickle file containing the tree
        
    Returns:
        The loaded tree object
    """
    import pickle
    with open(tree_path, 'rb') as f:
        return pickle.load(f)

def extract_tree_structure(tree):
    """Extract node features and edge indices from a constituency tree
    
    Args:
        tree: The constituency tree object
        
    Returns:
        tuple: (node_features, edge_indices)
    """
    import networkx as nx
    
    # Convert tree to networkx graph
    G = nx.Graph()
    
    # We'll use a queue for BFS traversal
    from collections import deque
    queue = deque([(tree, 0)])  # (node, parent_id)
    node_id = 0
    
    # Dictionaries to store node information
    node_features = {}
    edge_indices = []
    
    while queue:
        node, parent_id = queue.popleft()
        current_id = node_id
        node_id += 1
        
        # Store node feature (using label or other identifier)
        if hasattr(node, 'label'):
            node_features[current_id] = node.label[0] if isinstance(node.label, list) else node.label
        
        # Add edge to parent if not root
        if parent_id != -1:
            edge_indices.append((parent_id, current_id))
        
        # Add children to queue
        if hasattr(node, 'children'):
            for child in node.children:
                queue.append((child, current_id))
    
    return node_features, edge_indices

def create_word_graphs(word_embeddings, sentence_embeddings, texts, labels, dataset_name, split='train', edge_type='constituency'):
    """Create graph structures from pre-generated constituency trees and embeddings
    
    Args:
        word_embeddings: List of word embeddings for each text
        sentence_embeddings: List of sentence embeddings for each text
        texts: List of texts
        labels: List of labels
        dataset_name: Name of the dataset (e.g., 'sst2')
        split: Data split ('train', 'val', 'test')
        edge_type: Type of edges to create (only 'constituency' is supported)
        
    Returns:
        List of torch_geometric.data.Data objects
    """
    import os
    import pickle
    import networkx as nx
    from tqdm import tqdm
    
    # Base directory for constituency trees
    logger.info(f"Dataset name received in create_word_graphs: {dataset_name}")
    
    # Extract dataset provider and name
    if '/' in dataset_name:
        provider, name = dataset_name.split('/', 1)
    else:
        # If no provider is specified, try to determine from the dataset name
        if dataset_name == 'stanfordnlp':
            provider, name = 'stanfordnlp', 'sst2'  # Default to sst2 if only stanfordnlp is provided
        else:
            provider, name = 'stanfordnlp', dataset_name
    
    logger.info(f"Provider: {provider}, Name: {name}")
    base_dir = os.path.join('/app/src/Clean_Code/output/text_graphs', provider, name, split, 'constituency')
    logger.info(f"Looking for constituency trees in: {base_dir}")
    if not os.path.exists(base_dir):
        logger.warning(f"Constituency tree directory not found: {base_dir}. No graphs will be created.")
        return []
    
    logger.info(f"Creating graphs from pre-generated constituency trees in {base_dir}")
    logger.info(f"Input sizes: {len(word_embeddings)} word embeddings, {len(sentence_embeddings)} sentence embeddings, {len(texts)} texts, {len(labels)} labels")
    
    graphs = []
    
    # Get list of all tree files
    tree_files = sorted([f for f in os.listdir(base_dir) if f.endswith('.pkl')], 
                        key=lambda x: int(x.split('.')[0]) if x.split('.')[0].isdigit() else float('inf'))
    
    # Process each sample
    graphs = []
    for i in tqdm(range(len(word_embeddings)), desc="Processing graphs"):
        try:
            # Try different tree file naming patterns
            tree_file = os.path.join(base_dir, f"tree_{i}.pkl")
            simple_tree_file = os.path.join(base_dir, f"{i}.pkl")
            
            # Check if any tree file pattern exists
            if os.path.exists(tree_file):
                tree_path = tree_file
            elif os.path.exists(simple_tree_file):
                tree_path = simple_tree_file
            else:
                logger.warning(f"No tree file found for sample {i}")
                continue
            
            # Load the tree
            tree = load_constituency_tree(tree_path)
            
            # Get the corresponding embeddings and label
            word_embedding = word_embeddings[i] if i < len(word_embeddings) else None
            sentence_embedding = sentence_embeddings[i] if i < len(sentence_embeddings) else None
            label = labels[i] if i < len(labels) else None
            text = texts[i] if i < len(texts) else None
            
            # Handle label tensor
            if label is not None and isinstance(label, torch.Tensor):
                if label.numel() > 1:
                    label = label[0].item()
                else:
                    label = label.item()
            
            # Extract tree structure
            node_features, edge_indices = extract_tree_structure(tree)
            
            if not edge_indices:
                logger.warning(f"No edges extracted from tree for sample {i}")
                continue
            
            # Create the graph
            graph_data = create_graph_from_tree(
                node_features, 
                edge_indices, 
                word_embedding, 
                sentence_embedding, 
                text, 
                label
            )
            
            graphs.append(graph_data)
            
        except Exception as e:
            logger.error(f"Error processing sample {i}: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
    
    logger.info(f"Created {len(graphs)} graphs successfully")
    return graphs

def create_graph_from_tree(node_features, edge_indices, word_embeddings, sentence_embedding, text, label):
    import torch
    from torch_geometric.data import Data
    
    # Convert edge indices to tensor
    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()
    
    # Process node features (embeddings)
    num_nodes = len(node_features)
    
    # Initialize node features tensor
    if word_embeddings and len(word_embeddings) > 0:
        # Use the first embedding to determine feature dimension
        feature_dim = word_embeddings[0].shape[0] if hasattr(word_embeddings[0], 'shape') else len(word_embeddings[0])
    else:
        # Fallback to sentence embedding dimension
        feature_dim = sentence_embedding.shape[0] if sentence_embedding is not None else 768
    
    x = torch.zeros((num_nodes, feature_dim), dtype=torch.float)
    
    # Assign word embeddings to leaf nodes
    leaf_count = 0
    for node_id, node_feature in node_features.items():
        # Check if this is a leaf node (no outgoing edges where this is the parent)
        is_leaf = not any(p == node_id for p, _ in edge_indices)
        if is_leaf and word_embeddings and leaf_count < len(word_embeddings):
            x[node_id] = word_embeddings[leaf_count]
            leaf_count += 1
    
    # For non-leaf nodes, use the mean of their children's embeddings
    # Process nodes in reverse order (leaves to root)
    for node_id in reversed(range(num_nodes)):
        children = [c for p, c in edge_indices if p == node_id]
        if children:
            child_embeddings = x[children]
            x[node_id] = child_embeddings.mean(dim=0)
    
    # Create graph data object
    graph_data = Data(
        x=x,
        edge_index=edge_index,
        y=torch.tensor([label], dtype=torch.long) if label is not None else None,
        text=text,
        sentence_embedding=torch.tensor(sentence_embedding, dtype=torch.float) if sentence_embedding is not None else None
    )
    
    return graph_data

def process_embeddings_batch(batch_word_embeddings, batch_sentence_embeddings, batch_texts, batch_labels, config, split_output_dir, batch_idx, special_embeddings=None):
    """Process a single batch of embeddings"""
    # Log label source
    label_source = config.get('label_source', 'original')
    if label_source == 'llm':
        logger.info(f"Using LLM predictions as labels for batch {batch_idx}")
    
    # Extract dataset name from the path
    # We need to use the original dataset_name passed from process_embeddings
    # Instead of trying to extract it from the split_output_dir which might be incorrect
    
    # Get the dataset name from the config
    dataset_name = config.get('dataset_name', 'stanfordnlp/sst2')
    
    # Log the dataset name we're using
    logger.info(f"Using dataset name: {dataset_name} for graph creation")

    
    # Extract split from the path
    split = os.path.basename(split_output_dir).replace('_llm_labels', '')
    
    # Create graphs
    batch_graphs = create_word_graphs(
        batch_word_embeddings,
        batch_sentence_embeddings,
        batch_texts,
        batch_labels,
        dataset_name,
        split,
        config['edge_type']
    )
    
    # Save batch graphs with consistent numerical naming to preserve original order
    # Format with leading zeros to ensure correct sorting
    batch_graphs_path = os.path.join(split_output_dir, f"graphs_{config['edge_type']}_batch_{batch_idx:04d}.pkl")
    
    with open(batch_graphs_path, 'wb') as f:
        pkl.dump(batch_graphs, f)
    
    logger.info(f"Saved {len(batch_graphs)} graphs to {batch_graphs_path}")
    
    return len(batch_graphs)

def load_llm_predictions(predictions_file, split='test'):
    """Load LLM predictions from a JSON file using the best epoch
    
    Args:
        predictions_file: Path to the JSON file containing LLM predictions
        split: Data split to load predictions for ('train', 'validation', 'test')
        
    Returns:
        Dictionary mapping data index to predicted label
    """
    import json
    import os
    import sys
    
    # Map split names to match those in the predictions file
    split_map = {
        'train': 'train',
        'validation': 'validation',
        'test': 'test'
    }
    
    split_name = split_map.get(split, split)
    
    try:
        # Get the model directory from the predictions file path
        model_dir = os.path.dirname(predictions_file)
        
        # Find the best epoch by analyzing the classification reports
        best_epoch = find_best_epoch(model_dir)
        
        if best_epoch is None:
            logger.warning(f"Could not determine best epoch, using latest epoch from predictions")
            # Fallback to the latest epoch if best epoch cannot be determined
            with open(predictions_file, 'r') as f:
                predictions_data = json.load(f)
            
            max_epoch = 0
            for item in predictions_data:
                if item.get('dataset') == split_name and item.get('epoch', 0) > max_epoch:
                    max_epoch = item.get('epoch', 0)
            best_epoch = max_epoch
        
        # Load predictions
        with open(predictions_file, 'r') as f:
            predictions_data = json.load(f)
        
        # Create a dictionary mapping data index to predicted label for the best epoch
        predictions = {}
        for item in predictions_data:
            if item.get('dataset') == split_name and item.get('epoch') == best_epoch:
                predictions[item.get('data_index')] = item.get('predicted_label')
        
        logger.info(f"Loaded {len(predictions)} LLM predictions for {split_name} split from best epoch {best_epoch}")
        return predictions
    
    except Exception as e:
        logger.error(f"Error loading LLM predictions: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return {}

def find_best_epoch(model_dir, metric='f1-score', split='validation'):
    """Find the best epoch based on validation F1 score
    
    Args:
        model_dir: Directory containing model checkpoints and classification reports
        metric: Metric to use for selecting the best epoch (default: 'f1-score')
        split: Data split to use for evaluation (default: 'validation')
        
    Returns:
        Best epoch number or None if it cannot be determined
    """
    import glob
    import json
    import os
    
    # Find all classification reports for the specified split
    report_files = glob.glob(os.path.join(model_dir, f"classification_report_{split}_epoch*.json"))
    
    if not report_files:
        logger.warning(f"No classification reports found in {model_dir} for split {split}")
        return None
    
    best_score = -1
    best_epoch = None
    
    # Iterate through all epochs
    for report_file in report_files:
        try:
            with open(report_file, 'r') as f:
                report = json.load(f)
            
            # Extract epoch number from filename
            epoch = int(os.path.basename(report_file).split('_')[-1].split('.')[0].replace('epoch', ''))
            
            # For multi-class classification, use macro avg F1-score
            # For binary classification, use weighted avg F1-score
            if 'macro avg' in report and metric in report['macro avg']:
                score = report['macro avg'][metric]
            elif 'weighted avg' in report and metric in report['weighted avg']:
                score = report['weighted avg'][metric]
            else:
                logger.warning(f"Metric {metric} not found in report {report_file}")
                continue
            
            logger.info(f"Epoch {epoch}: {metric} = {score:.4f}")
            
            # Check if this is the best score so far
            if score > best_score:
                best_score = score
                best_epoch = epoch
        
        except Exception as e:
            logger.error(f"Error processing report file {report_file}: {e}")
            continue
    
    if best_epoch is not None:
        logger.info(f"Best {metric}: {best_score:.4f} (Epoch {best_epoch})")
    
    return best_epoch

def process_embeddings(dataset_name, embeddings_dir, batch_size=10, edge_type='constituency', label_source='original', llm_predictions=None):
    """Process embeddings to create graph structures
    
    Args:
        dataset_name: Name of the dataset
        embeddings_dir: Directory containing embeddings
        batch_size: Batch size for processing
        edge_type: Type of edges to create (constituency)
        label_source: Source of labels ('original' or 'llm')
        llm_predictions: Path to LLM predictions JSON file (required if label_source is 'llm')
        
    Returns:
        output_dir: Directory containing the processed graphs
    """
    # Create output directory
    output_dir = os.path.join(os.path.dirname(os.path.dirname(embeddings_dir)), 'graphs', dataset_name)
    os.makedirs(output_dir, exist_ok=True)
    
    # Load special embeddings
    special_embeddings = load_special_embeddings(embeddings_dir)
    
    # Process each split
    for split in ['train', 'validation', 'test']:
        split_dir = os.path.join(embeddings_dir, split)
        
        if not os.path.exists(split_dir):
            logger.warning(f"Split directory {split_dir} does not exist, skipping")
            continue
        
        # Create output directory for this split
        # If using LLM predictions, create a separate directory
        if label_source == 'llm':
            split_output_dir = os.path.join(output_dir, f"{split}_llm_labels")
        else:
            split_output_dir = os.path.join(output_dir, split)
            
        os.makedirs(split_output_dir, exist_ok=True)
        
        # Load LLM predictions if needed
        llm_pred_dict = None
        if label_source == 'llm' and llm_predictions:
            logger.info(f"Loading LLM predictions for {split} split from {llm_predictions}")
            llm_pred_dict = load_llm_predictions(llm_predictions, split)
        
        # Get dataset information
        try:
            dataset_info, is_chunked = get_dataset_info(split_dir)
            logger.info(f"Processing {dataset_info['total_samples']} samples for {split} split")
        except Exception as e:
            logger.error(f"Error getting dataset information for {split_dir}: {str(e)}")
            continue
        
        # Save configuration
        config = {
            'edge_type': edge_type,
            'batch_size': batch_size,
            'total_samples': dataset_info['total_samples'],
            'label_source': label_source,
            'dataset_name': dataset_name
        }
        
        config_path = os.path.join(split_output_dir, 'config.json')
        with open(config_path, 'w') as f:
            json.dump(config, f)
        
        # Process embeddings
        if is_chunked:
            # Process each chunk
            chunks_dir = os.path.join(split_dir, 'chunks')
            total_graphs = 0
            
            for i, chunk_dir_name in enumerate(dataset_info['chunk_dirs']):
                chunk_dir = os.path.join(chunks_dir, chunk_dir_name)
                
                try:
                    # Load chunk data
                    word_embeddings, sentence_embeddings, texts, labels = load_chunk_from_disk(chunk_dir)
                    
                    # Apply LLM predictions if needed
                    if label_source == 'llm' and llm_pred_dict:
                        # Try to match data indices to LLM predictions
                        modified_labels = []
                        for idx, label in enumerate(labels):
                            # Use data index if available, otherwise use position in chunk
                            data_idx = i * batch_size + idx
                            if data_idx in llm_pred_dict:
                                modified_labels.append(llm_pred_dict[data_idx])
                            else:
                                modified_labels.append(label)
                                logger.warning(f"No LLM prediction found for data index {data_idx}, using original label")
                        
                        # Process chunk with modified labels
                        num_graphs = process_embeddings_batch(
                            word_embeddings, sentence_embeddings, texts, modified_labels,
                            config, split_output_dir, i, special_embeddings
                        )
                    else:
                        # Process chunk with original labels
                        num_graphs = process_embeddings_batch(
                            word_embeddings, sentence_embeddings, texts, labels,
                            config, split_output_dir, i, special_embeddings
                        )
                    
                    total_graphs += num_graphs
                    
                except Exception as e:
                    logger.error(f"Error processing chunk {chunk_dir}: {str(e)}")
                    import traceback
                    logger.error(traceback.format_exc())
            
            logger.info(f"Processed {total_graphs} graphs for {split} split")
        
        else:
            # Load all embeddings
            try:
                # Load metadata
                metadata_path = os.path.join(split_dir, 'metadata.pkl')
                with open(metadata_path, 'rb') as f:
                    metadata = pkl.load(f)
                
                # Load word embeddings
                word_embeddings_path = os.path.join(split_dir, 'word_embeddings.pkl')
                with open(word_embeddings_path, 'rb') as f:
                    word_embeddings = pkl.load(f)
                
                # Load sentence embeddings
                sentence_embeddings_path = os.path.join(split_dir, 'sentence_embeddings.pkl')
                with open(sentence_embeddings_path, 'rb') as f:
                    sentence_embeddings = pkl.load(f)
                
                # Process in batches
                total_samples = len(word_embeddings)
                total_graphs = 0
                
                for i in range(0, total_samples, batch_size):
                    batch_end = min(i + batch_size, total_samples)
                    
                    # Get batch data
                    batch_word_embeddings = word_embeddings[i:batch_end]
                    batch_sentence_embeddings = sentence_embeddings[i:batch_end]
                    batch_texts = metadata['texts'][i:batch_end]
                    batch_labels = metadata['labels'][i:batch_end]
                    
                    # Apply LLM predictions if needed
                    if label_source == 'llm' and llm_pred_dict:
                        # Try to match data indices to LLM predictions
                        modified_labels = []
                        for idx, label in enumerate(batch_labels):
                            # Use data index if available, otherwise use position in batch
                            data_idx = i + idx
                            if data_idx in llm_pred_dict:
                                modified_labels.append(llm_pred_dict[data_idx])
                            else:
                                modified_labels.append(label)
                                logger.warning(f"No LLM prediction found for data index {data_idx}, using original label")
                        
                        # Process batch with modified labels
                        num_graphs = process_embeddings_batch(
                            batch_word_embeddings, batch_sentence_embeddings, batch_texts, modified_labels,
                            config, split_output_dir, i // batch_size, special_embeddings
                        )
                    else:
                        # Process batch with original labels
                        num_graphs = process_embeddings_batch(
                            batch_word_embeddings, batch_sentence_embeddings, batch_texts, batch_labels,
                            config, split_output_dir, i // batch_size, special_embeddings
                        )
                    
                    total_graphs += num_graphs
                
                logger.info(f"Processed {total_graphs} graphs for {split} split")
                
            except Exception as e:
                logger.error(f"Error processing embeddings for {split_dir}: {str(e)}")
                import traceback
                logger.error(traceback.format_exc())
    
    return output_dir

def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description="Process embeddings to create graph structures")
    parser.add_argument("--dataset_name", type=str, required=True, help="Name of the dataset")
    parser.add_argument("--embeddings_dir", type=str, required=True, help="Directory containing embeddings")
    parser.add_argument("--batch_size", type=int, default=10, help="Batch size for processing")
    parser.add_argument("--edge_type", type=str, default="constituency", choices=["constituency"], help="Type of edges to create")
    parser.add_argument("--label_source", type=str, default="original", choices=["original", "llm"], 
                        help="Source of labels: 'original' (use original dataset labels) or 'llm' (use LLM predictions)")
    parser.add_argument("--llm_predictions", type=str, help="Path to LLM predictions JSON file (required if label_source is 'llm')")
    
    args = parser.parse_args()
    
    # Validate arguments
    if args.label_source == "llm" and not args.llm_predictions:
        parser.error("--llm_predictions is required when --label_source is 'llm'")
    
    return args

def main():
    """Main entry point"""
    args = parse_args()
    
    # Process embeddings
    output_dir = process_embeddings(
        dataset_name=args.dataset_name,
        embeddings_dir=args.embeddings_dir,
        batch_size=args.batch_size,
        edge_type=args.edge_type,
        label_source=args.label_source,
        llm_predictions=args.llm_predictions if args.label_source == 'llm' else None
    )
    
    if output_dir:
        logger.info(f"Graph processing completed successfully. Graphs saved to: {output_dir}")
    else:
        logger.error("Graph processing failed")

if __name__ == "__main__":
    main()
