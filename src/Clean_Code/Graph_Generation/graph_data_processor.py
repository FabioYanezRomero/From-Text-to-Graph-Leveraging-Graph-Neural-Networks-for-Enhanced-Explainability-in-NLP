"""
Graph Data Processor

This module processes word embeddings and prepares them for use in Graph Neural Networks.
It creates graph structures from text data using the embeddings generated by the embedding_generator.py.
"""

import os
import argparse
import logging
import json
import pickle as pkl
import numpy as np
from tqdm import tqdm
import torch
import torch.nn.functional as F
import torch_geometric
from torch_geometric.data import Data
import networkx as nx

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

def load_embeddings(input_dir, dataset, embedding_model, split):
    """Load word and sentence embeddings from disk, handling chunked data structure.
    
    Args:
        input_dir: Base directory containing the input embeddings
        dataset: Name of the dataset
        embedding_model: Name of the embedding model used
        split: Data split ('train', 'val', 'test')
        
    Returns:
        tuple: (texts, labels, word_embeddings, sentence_embeddings)
    """
    import os
    import pickle
    import numpy as np
    from glob import glob
    
    # Construct base path
    base_path = os.path.join(input_dir, dataset, split)
    
    # Find all chunk directories
    chunk_dirs = sorted([d for d in glob(os.path.join(base_path, 'chunk_*')) if os.path.isdir(d)])
    
    if not chunk_dirs:
        logger.error(f"No chunk directories found in {base_path}")
        return None
    
    all_word_embeddings = []
    all_sentence_embeddings = []
    all_texts = []
    all_labels = []
    
    logger.info(f"Found {len(chunk_dirs)} chunks in {base_path}")
    
    try:
        # Process each chunk
        for chunk_dir in chunk_dirs:
            try:
                # Load data from chunk
                word_embeddings_path = os.path.join(chunk_dir, 'word_embeddings.pkl')
                sentence_embeddings_path = os.path.join(chunk_dir, 'sentence_embeddings.pkl')
                metadata_path = os.path.join(chunk_dir, 'metadata.pkl')
                
                # Check if required files exist
                if not all(os.path.exists(p) for p in [word_embeddings_path, sentence_embeddings_path, metadata_path]):
                    logger.warning(f"Skipping incomplete chunk: {chunk_dir}")
                    continue
                
                # Load metadata which contains texts and labels
                with open(metadata_path, 'rb') as f:
                    metadata = pickle.load(f)
                
                # Load word and sentence embeddings
                with open(word_embeddings_path, 'rb') as f:
                    word_embeddings = pickle.load(f)
                with open(sentence_embeddings_path, 'rb') as f:
                    sentence_embeddings = pickle.load(f)
                
                # Extract texts and labels from metadata
                texts = [item['text'] for item in metadata]
                labels = [item['label'] for item in metadata]
                
                # Convert to lists if they're numpy arrays
                if isinstance(word_embeddings, np.ndarray):
                    word_embeddings = list(word_embeddings)
                if isinstance(sentence_embeddings, np.ndarray):
                    sentence_embeddings = list(sentence_embeddings)
                if isinstance(texts, np.ndarray):
                    texts = list(texts)
                if isinstance(labels, np.ndarray):
                    labels = list(labels)
                
                # Add to the combined lists
                all_word_embeddings.extend(word_embeddings)
                all_sentence_embeddings.extend(sentence_embeddings)
                all_texts.extend(texts)
                all_labels.extend(labels)
                
                logger.info(f"Loaded {len(word_embeddings)} samples from {os.path.basename(chunk_dir)}")
                
            except Exception as e:
                logger.error(f"Error loading chunk {chunk_dir}: {str(e)}")
                import traceback
                logger.error(traceback.format_exc())
        
        if not all_word_embeddings:
            logger.error("No data was loaded from any chunk")
            return None
            
        logger.info(f"Successfully loaded {len(all_word_embeddings)} total samples from {len(chunk_dirs)} chunks")
        return all_texts, all_labels, all_word_embeddings, all_sentence_embeddings
        
    except Exception as e:
        logger.error(f"Error in load_embeddings: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return None

def save_graphs(graphs, output_dir, batch_size=32, num_workers=4):
    """Save a list of PyTorch Geometric Data objects to disk.
    
    Args:
        graphs: List of PyTorch Geometric Data objects
        output_dir: Directory to save the graphs
        batch_size: Number of graphs to save in each file
        num_workers: Number of worker processes to use
    """
    import os
    import torch
    from torch_geometric.data import Data, Batch
    from torch_geometric.loader import DataLoader
    
    os.makedirs(output_dir, exist_ok=True)
    
    # Create a DataLoader to handle batching
    loader = DataLoader(graphs, batch_size=batch_size, shuffle=False, num_workers=num_workers)
    
    # Save each batch
    for batch_idx, batch in enumerate(loader):
        batch_path = os.path.join(output_dir, f'batch_{batch_idx:04d}.pt')
        try:
            torch.save(batch, batch_path)
            logger.info(f"Saved batch {batch_idx} with {len(batch)} graphs to {batch_path}")
        except Exception as e:
            logger.error(f"Error saving batch {batch_idx}: {str(e)}")
    
    logger.info(f"Saved {len(graphs)} graphs in {len(loader)} batches to {output_dir}")

def get_dataset_info(embeddings_dir):
    """Get information about the dataset without loading all embeddings
    
    Args:
        embeddings_dir: Directory containing embeddings
        
    Returns:
        dataset_info: Dictionary containing dataset information
        is_chunked: Whether embeddings are stored in chunks
    """
    # Check if embeddings are stored in chunks
    chunks_dir = os.path.join(embeddings_dir, 'chunks')
    is_chunked = os.path.exists(chunks_dir)
    
    if is_chunked:
        # Get metadata from first chunk
        chunk_dirs = sorted([d for d in os.listdir(chunks_dir) if os.path.isdir(os.path.join(chunks_dir, d))])
        if not chunk_dirs:
            raise ValueError(f"No chunk directories found in {chunks_dir}")
        
        first_chunk_dir = os.path.join(chunks_dir, chunk_dirs[0])
        metadata_path = os.path.join(first_chunk_dir, 'metadata.pkl')
        
        if not os.path.exists(metadata_path):
            raise ValueError(f"Metadata file not found in {first_chunk_dir}")
        
        with open(metadata_path, 'rb') as f:
            metadata = pkl.load(f)
        
        # Count total samples across all chunks
        total_samples = 0
        for chunk_dir in chunk_dirs:
            chunk_metadata_path = os.path.join(chunks_dir, chunk_dir, 'metadata.pkl')
            with open(chunk_metadata_path, 'rb') as f:
                chunk_metadata = pkl.load(f)
            total_samples += len(chunk_metadata['texts'])
        
        dataset_info = {
            'total_samples': total_samples,
            'num_chunks': len(chunk_dirs),
            'chunk_dirs': chunk_dirs
        }
    else:
        # Get metadata from main directory
        metadata_path = os.path.join(embeddings_dir, 'metadata.pkl')
        
        if not os.path.exists(metadata_path):
            raise ValueError(f"Metadata file not found in {embeddings_dir}")
        
        with open(metadata_path, 'rb') as f:
            metadata = pkl.load(f)
        
        dataset_info = {
            'total_samples': len(metadata['texts'])
        }
    
    return dataset_info, is_chunked

def load_chunk_from_disk(chunk_dir):
    """Load a chunk of embeddings from disk
    
    Args:
        chunk_dir: Directory containing chunk data
        
    Returns:
        word_embeddings: List of word embeddings
        sentence_embeddings: List of sentence embeddings
        texts: List of texts
        labels: List of labels
    """
    # Load metadata
    metadata_path = os.path.join(chunk_dir, 'metadata.pkl')
    with open(metadata_path, 'rb') as f:
        metadata = pkl.load(f)
    
    # Load word embeddings
    word_embeddings_path = os.path.join(chunk_dir, 'word_embeddings.pkl')
    with open(word_embeddings_path, 'rb') as f:
        word_embeddings = pkl.load(f)
    
    # Load sentence embeddings
    sentence_embeddings_path = os.path.join(chunk_dir, 'sentence_embeddings.pkl')
    with open(sentence_embeddings_path, 'rb') as f:
        sentence_embeddings = pkl.load(f)
    
    return word_embeddings, sentence_embeddings, metadata['texts'], metadata['labels']

def load_special_embeddings(embeddings_dir):
    """Load special embeddings for constituency tokens
    
    Args:
        embeddings_dir: Directory containing embeddings
        
    Returns:
        special_embeddings: Dictionary of special embeddings
    """
    # Check if special embeddings exist in main directory
    special_embeddings_path = os.path.join(embeddings_dir, 'special_embeddings.pkl')
    
    if os.path.exists(special_embeddings_path):
        with open(special_embeddings_path, 'rb') as f:
            special_embeddings = pkl.load(f)
        return special_embeddings
    
    # Check if special embeddings exist in special directory
    special_dir = os.path.join(os.path.dirname(embeddings_dir), 'special')
    special_embeddings_path = os.path.join(special_dir, 'special_embeddings.pkl')
    
    if os.path.exists(special_embeddings_path):
        with open(special_embeddings_path, 'rb') as f:
            special_embeddings = pkl.load(f)
        return special_embeddings
    
    logger.warning(f"No special embeddings found in {embeddings_dir} or {special_dir}")
    return None

def load_constituency_tree(tree_path):
    """Load a pre-generated constituency tree from file
    
    Args:
        tree_path: Path to the pickle file containing the tree
        
    Returns:
        The loaded tree object
    """
    import pickle
    with open(tree_path, 'rb') as f:
        return pickle.load(f)

def extract_tree_structure(tree):
    """Extract node features and edge indices from a constituency tree
    
    Args:
        tree: The constituency tree object
        
    Returns:
        tuple: (node_features, edge_indices)
    """
    import networkx as nx
    
    # Convert tree to networkx graph
    G = nx.Graph()
    
    # We'll use a queue for BFS traversal
    from collections import deque
    queue = deque([(tree, 0)])  # (node, parent_id)
    node_id = 0
    
    # Dictionaries to store node information
    node_features = {}
    edge_indices = []
    
    while queue:
        node, parent_id = queue.popleft()
        current_id = node_id
        node_id += 1
        
        # Store node feature (using label or other identifier)
        if hasattr(node, 'label'):
            node_features[current_id] = node.label[0] if isinstance(node.label, list) else node.label
        
        # Add edge to parent if not root
        if parent_id != -1:
            edge_indices.append((parent_id, current_id))
        
        # Add children to queue
        if hasattr(node, 'children'):
            for child in node.children:
                queue.append((child, current_id))
    
    return node_features, edge_indices

def create_word_graphs(word_embeddings, sentence_embeddings, texts, labels, dataset_name, split='train', edge_type='constituency'):
    """Create graph structures from pre-generated constituency trees and embeddings
    
    Args:
        word_embeddings: List of word embeddings for each text
        sentence_embeddings: List of sentence embeddings for each text
        texts: List of texts
        labels: List of labels
        dataset_name: Name of the dataset (e.g., 'sst2')
        split: Data split ('train', 'val', 'test')
        edge_type: Type of edges to create (only 'constituency' is supported)
        
    Returns:
        List of torch_geometric.data.Data objects
    """
    import os
    import pickle
    import networkx as nx
    from tqdm import tqdm
    
    # Base directory for constituency trees
    base_dir = os.path.join('/app/src/Clean_Code/output/text_graphs', 'stanfordnlp', dataset_name, split, 'constituency')
    
    graphs = []
    
    logger.info(f"Creating graphs from pre-generated constituency trees in {base_dir}")
    logger.info(f"Input sizes: {len(word_embeddings)} word embeddings, {len(sentence_embeddings)} sentence embeddings, {len(texts)} texts, {len(labels)} labels")
    
    # Ensure the directory exists
    if not os.path.exists(base_dir):
        raise FileNotFoundError(f"Constituency tree directory not found: {base_dir}")
    
    # Get list of all tree files
    tree_files = sorted([f for f in os.listdir(base_dir) if f.endswith('.pkl')], 
                        key=lambda x: int(x.split('.')[0]) if x.split('.')[0].isdigit() else float('inf'))
    
    # Process each sample
    for i in tqdm(range(len(word_embeddings)), desc="Processing graphs"):
        try:
            if i >= len(tree_files):
                logger.warning(f"No tree file found for sample {i}")
                continue
                
            # Load the pre-generated tree
            tree_path = os.path.join(base_dir, tree_files[i])
            tree = load_constituency_tree(tree_path)
            
            # Get the corresponding embeddings and label
            word_embedding = word_embeddings[i] if i < len(word_embeddings) else None
            sentence_embedding = sentence_embeddings[i] if i < len(sentence_embeddings) else None
            label = labels[i] if i < len(labels) else None
            text = texts[i] if i < len(texts) else None
            
            # Handle label tensor
            if label is not None and isinstance(label, torch.Tensor):
                if label.numel() > 1:
                    label = label[0].item()
                else:
                    label = label.item()
            
            # Extract tree structure
            node_features, edge_indices = extract_tree_structure(tree)
            
            if not edge_indices:
                logger.warning(f"No edges extracted from tree for sample {i}")
                continue
            
            # Convert edge indices to tensor
            edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()
            
            # Process node features (embeddings)
            num_nodes = len(node_features)
            
            # Initialize node features tensor
            if word_embedding is not None and len(word_embedding) > 0:
                # Use the first embedding to determine feature dimension
                feature_dim = word_embedding[0].shape[0] if hasattr(word_embedding[0], 'shape') else len(word_embedding[0])
            else:
                # Fallback to sentence embedding dimension
                feature_dim = sentence_embedding.shape[0] if sentence_embedding is not None else 768
            
            x = torch.zeros((num_nodes, feature_dim), dtype=torch.float)
            
            # Assign word embeddings to leaf nodes
            leaf_count = 0
            for node_id, node_feature in node_features.items():
                # Check if this is a leaf node (no outgoing edges where this is the parent)
                is_leaf = not any(p == node_id for p, _ in edge_indices)
                if is_leaf and word_embedding is not None and leaf_count < len(word_embedding):
                    x[node_id] = word_embedding[leaf_count]
                    leaf_count += 1
            
            # For non-leaf nodes, use the mean of their children's embeddings
            # Process nodes in reverse order (leaves to root)
            for node_id in reversed(range(num_nodes)):
                children = [c for p, c in edge_indices if p == node_id]
                if children:
                    child_embeddings = x[children]
                    x[node_id] = child_embeddings.mean(dim=0)
            
            # Create graph data object
            graph_data = Data(
                x=x,
                edge_index=edge_index,
                y=torch.tensor([label], dtype=torch.long) if label is not None else None,
                text=text,
                sentence_embedding=torch.tensor(sentence_embedding, dtype=torch.float) if sentence_embedding is not None else None
            )
            
            graphs.append(graph_data)
            
        except Exception as e:
            logger.error(f"Error processing sample {i}: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            
        except Exception as e:
            logger.error(f"Error creating graph for sample {i}: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
    
    logger.info(f"Created {len(graphs)} graphs successfully")
    return graphs

def process_embeddings_batch(batch_word_embeddings, batch_sentence_embeddings, batch_texts, batch_labels, config, split_output_dir, batch_idx, special_embeddings=None):
    """Process a single batch of embeddings"""
    # Create graph structures for this batch
    batch_graphs = create_word_graphs(
        batch_word_embeddings,
        batch_sentence_embeddings,
        batch_texts,
        batch_labels,
        special_embeddings=special_embeddings,
        window_size=config['window_size'],
        edge_type=config['edge_type']
    )
    
    # Save batch graphs with consistent numerical naming to preserve original order
    # Format with leading zeros to ensure correct sorting
    batch_graphs_path = os.path.join(split_output_dir, f"graphs_{config['edge_type']}_batch_{batch_idx:04d}.pkl")
    
    with open(batch_graphs_path, 'wb') as f:
        pkl.dump(batch_graphs, f)
    
    logger.info(f"Saved {len(batch_graphs)} graphs to {batch_graphs_path}")
    
    return len(batch_graphs)

def process_embeddings(dataset_name, embeddings_dir, batch_size=10, window_size=3, edge_type='window'):
    """Process embeddings to create graph structures
    
    Args:
        dataset_name: Name of the dataset
        embeddings_dir: Directory containing embeddings
        batch_size: Batch size for processing
        window_size: Window size for creating edges
        edge_type: Type of edges to create (window, fully_connected)
        
    Returns:
        output_dir: Directory containing the processed graphs
    """
    # Create output directory
    output_dir = os.path.join(os.path.dirname(os.path.dirname(embeddings_dir)), 'graphs', dataset_name)
    os.makedirs(output_dir, exist_ok=True)
    
    # Load special embeddings
    special_embeddings = load_special_embeddings(embeddings_dir)
    
    # Process each split
    for split in ['train', 'validation', 'test']:
        split_dir = os.path.join(embeddings_dir, split)
        
        if not os.path.exists(split_dir):
            logger.warning(f"Split directory {split_dir} does not exist, skipping")
            continue
        
        # Create output directory for this split
        split_output_dir = os.path.join(output_dir, split)
        os.makedirs(split_output_dir, exist_ok=True)
        
        # Get dataset information
        try:
            dataset_info, is_chunked = get_dataset_info(split_dir)
            logger.info(f"Processing {dataset_info['total_samples']} samples for {split} split")
        except Exception as e:
            logger.error(f"Error getting dataset information for {split_dir}: {str(e)}")
            continue
        
        # Save configuration
        config = {
            'window_size': window_size,
            'edge_type': edge_type,
            'batch_size': batch_size,
            'total_samples': dataset_info['total_samples']
        }
        
        config_path = os.path.join(split_output_dir, 'config.json')
        with open(config_path, 'w') as f:
            json.dump(config, f)
        
        # Process embeddings
        if is_chunked:
            # Process each chunk
            chunks_dir = os.path.join(split_dir, 'chunks')
            total_graphs = 0
            
            for i, chunk_dir_name in enumerate(dataset_info['chunk_dirs']):
                chunk_dir = os.path.join(chunks_dir, chunk_dir_name)
                
                try:
                    # Load chunk data
                    word_embeddings, sentence_embeddings, texts, labels = load_chunk_from_disk(chunk_dir)
                    
                    # Process chunk
                    num_graphs = process_embeddings_batch(
                        word_embeddings, sentence_embeddings, texts, labels,
                        config, split_output_dir, i, special_embeddings
                    )
                    
                    total_graphs += num_graphs
                    
                except Exception as e:
                    logger.error(f"Error processing chunk {chunk_dir}: {str(e)}")
                    import traceback
                    logger.error(traceback.format_exc())
            
            logger.info(f"Processed {total_graphs} graphs for {split} split")
        
        else:
            # Load all embeddings
            try:
                # Load metadata
                metadata_path = os.path.join(split_dir, 'metadata.pkl')
                with open(metadata_path, 'rb') as f:
                    metadata = pkl.load(f)
                
                # Load word embeddings
                word_embeddings_path = os.path.join(split_dir, 'word_embeddings.pkl')
                with open(word_embeddings_path, 'rb') as f:
                    word_embeddings = pkl.load(f)
                
                # Load sentence embeddings
                sentence_embeddings_path = os.path.join(split_dir, 'sentence_embeddings.pkl')
                with open(sentence_embeddings_path, 'rb') as f:
                    sentence_embeddings = pkl.load(f)
                
                # Process in batches
                total_samples = len(word_embeddings)
                total_graphs = 0
                
                for i in range(0, total_samples, batch_size):
                    batch_end = min(i + batch_size, total_samples)
                    
                    # Get batch data
                    batch_word_embeddings = word_embeddings[i:batch_end]
                    batch_sentence_embeddings = sentence_embeddings[i:batch_end]
                    batch_texts = metadata['texts'][i:batch_end]
                    batch_labels = metadata['labels'][i:batch_end]
                    
                    # Process batch
                    num_graphs = process_embeddings_batch(
                        batch_word_embeddings, batch_sentence_embeddings, batch_texts, batch_labels,
                        config, split_output_dir, i // batch_size, special_embeddings
                    )
                    
                    total_graphs += num_graphs
                
                logger.info(f"Processed {total_graphs} graphs for {split} split")
                
            except Exception as e:
                logger.error(f"Error processing embeddings for {split_dir}: {str(e)}")
                import traceback
                logger.error(traceback.format_exc())
    
    return output_dir

def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description="Process embeddings to create graph structures")
    parser.add_argument("--dataset_name", type=str, required=True, help="Name of the dataset")
    parser.add_argument("--embeddings_dir", type=str, required=True, help="Directory containing embeddings")
    parser.add_argument("--batch_size", type=int, default=10, help="Batch size for processing")
    parser.add_argument("--window_size", type=int, default=3, help="Window size for creating edges")
    parser.add_argument("--edge_type", type=str, default="window", choices=["window", "fully_connected"], help="Type of edges to create")
    
    return parser.parse_args()

def main():
    """Main entry point"""
    args = parse_args()
    
    # Process embeddings
    output_dir = process_embeddings(
        dataset_name=args.dataset_name,
        embeddings_dir=args.embeddings_dir,
        batch_size=args.batch_size,
        window_size=args.window_size,
        edge_type=args.edge_type
    )
    
    if output_dir:
        logger.info(f"Graph processing completed successfully. Graphs saved to: {output_dir}")
    else:
        logger.error("Graph processing failed")

if __name__ == "__main__":
    main()
