from __future__ import annotations

import argparse
import json
from pathlib import Path
import random
from typing import Dict, Iterable, List, Sequence, Tuple

import networkx as nx
import torch
from torch_geometric.data import Data
from transformers import AutoModel, AutoTokenizer


class LegalBertEmbedder:
    """Caches embeddings generated by nlpaueb/legal-bert-base-uncased."""

    def __init__(
        self,
        model_name: str = "nlpaueb/legal-bert-base-uncased",
        batch_size: int = 16,
        max_length: int = 96,
        device: str | None = None,
    ) -> None:
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        try:
            self.model = AutoModel.from_pretrained(model_name, use_safetensors=True)
        except ValueError:
            # Fallback for models without safetensors weights.
            self.model = AutoModel.from_pretrained(model_name)
        self.model.eval()
        self.device = torch.device(device or ("cuda" if torch.cuda.is_available() else "cpu"))
        self.model.to(self.device)
        self.batch_size = batch_size
        self.max_length = max_length
        self._cache: Dict[str, torch.Tensor] = {}
        self._empty_token = "[NO_TEXT]"

    @property
    def hidden_size(self) -> int:
        return self.model.config.hidden_size

    def encode(self, texts: Sequence[str | None]) -> torch.Tensor:
        """Return CLS embeddings for the provided texts."""
        if not texts:
            return torch.empty((0, self.hidden_size), dtype=torch.float)

        normalized: List[str] = []
        for text in texts:
            if text is None:
                normalized.append(self._empty_token)
            else:
                stripped = text.strip()
                normalized.append(stripped if stripped else self._empty_token)

        needs_encoding: List[tuple[int, str]] = []
        for idx, text in enumerate(normalized):
            if text not in self._cache:
                needs_encoding.append((idx, text))

        for i in range(0, len(needs_encoding), self.batch_size):
            chunk = needs_encoding[i : i + self.batch_size]
            chunk_texts = [text for _, text in chunk]
            tokenized = self.tokenizer(
                chunk_texts,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=self.max_length,
            )
            tokenized = {k: v.to(self.device) for k, v in tokenized.items()}
            with torch.no_grad():
                outputs = self.model(**tokenized)
                cls_embeddings = outputs.last_hidden_state[:, 0, :].detach().cpu()
            for (orig_idx, text_key), embedding in zip(chunk, cls_embeddings):
                # Store a clone so the cached tensor is not tied to downstream operations.
                self._cache[text_key] = embedding.clone()

        stacked = torch.stack([self._cache[text] for text in normalized]).float()
        return stacked


def _clean_text(value: str | None) -> str | None:
    if value is None:
        return None
    text = str(value).strip()
    return text or None


def load_labels(jsonl_path: Path) -> Dict[str, int]:
    """Load id -> label mapping from the legal background dataset."""
    labels: Dict[str, int] = {}
    with jsonl_path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            record = json.loads(line)
            record_id = record.get("id")
            if not record_id:
                continue
            labels[record_id] = int(record["label"])
    if not labels:
        raise ValueError(f"No labels loaded from {jsonl_path}")
    return labels


def graph_to_data(
    graph_path: Path,
    embedder: LegalBertEmbedder,
    label: int,
    data_index: int,
) -> Data:
    graph = nx.read_graphml(graph_path)
    node_ids: List[str] = []
    node_texts: List[str] = []
    node_labels: List[str] = []
    node_positions: Dict[object, int] = {}
    for idx, (node_id, attrs) in enumerate(graph.nodes(data=True)):
        node_positions[node_id] = idx
        node_ids.append(str(node_id))
        label_text = (
            _clean_text(attrs.get("label"))
            or _clean_text(attrs.get("text"))
            or str(node_id)
        )
        node_texts.append(label_text)
        node_labels.append(label_text)
    node_features = embedder.encode(node_texts).contiguous()

    edge_sources: List[int] = []
    edge_targets: List[int] = []
    relation_texts: List[str | None] = []
    inference_texts: List[str | None] = []
    edge_labels: List[str | None] = []

    for src, dst, attrs in graph.edges(data=True):
        edge_sources.append(node_positions[src])
        edge_targets.append(node_positions[dst])
        relation = _clean_text(attrs.get("relation"))
        inference = _clean_text(attrs.get("inference"))
        relation_texts.append(relation)
        inference_texts.append(inference)
        edge_labels.append(relation or inference)

    if edge_sources:
        edge_index = torch.tensor([edge_sources, edge_targets], dtype=torch.long)
        edge_attr = embedder.encode(edge_labels).contiguous()
    else:
        edge_index = torch.empty((2, 0), dtype=torch.long)
        edge_attr = torch.empty((0, embedder.hidden_size), dtype=torch.float)

    data = Data(x=node_features, edge_index=edge_index)
    data.edge_attr = edge_attr
    label_tensor = torch.tensor([label], dtype=torch.long)
    data.y = label_tensor.clone()
    data.true_label = label_tensor.clone()
    data.data_index = data_index
    data.graph_id = graph_path.stem
    data.node_labels = node_labels
    data.nx_node_names = node_ids
    data.edge_relations = relation_texts
    data.edge_inference = inference_texts
    data.edge_labels = edge_labels
    return data


def chunked(iterable: Sequence[Data], chunk_size: int) -> Iterable[List[Data]]:
    for i in range(0, len(iterable), chunk_size):
        yield list(iterable[i : i + chunk_size])


def _normalized_split_counts(total: int, ratios: Tuple[float, float, float]) -> Tuple[int, int, int]:
    if total <= 0:
        return (0, 0, 0)
    total_ratio = sum(ratios)
    if total_ratio <= 0:
        raise ValueError("At least one split ratio must be positive.")
    normalized = [r / total_ratio for r in ratios]
    raw_counts = [r * total for r in normalized]
    counts = [int(count) for count in raw_counts]
    remainder = total - sum(counts)
    fractional = [rc - int(rc) for rc in raw_counts]
    for idx in sorted(range(len(fractional)), key=lambda i: fractional[i], reverse=True):
        if remainder <= 0:
            break
        counts[idx] += 1
        remainder -= 1
    return tuple(counts)  # type: ignore[return-value]


def _save_split_batches(
    graphs: Sequence[Data],
    split_dir: Path,
    batch_size: int,
    file_prefix: str,
) -> List[Path]:
    split_dir.mkdir(parents=True, exist_ok=True)
    saved_paths: List[Path] = []
    for batch_idx, batch in enumerate(chunked(list(graphs), batch_size)):
        out_path = split_dir / f"{file_prefix}_{batch_idx:04d}.pt"
        torch.save(batch, out_path)
        saved_paths.append(out_path)
    return saved_paths


def convert_directory(
    graph_dir: Path,
    labels_path: Path,
    output_dir: Path,
    graphs_per_file: int = 32,
    encoder_batch_size: int = 12,
    encoder_max_length: int = 112,
    train_ratio: float = 0.8,
    val_ratio: float = 0.1,
    test_ratio: float = 0.1,
    shuffle: bool = True,
    split_seed: int | None = 42,
) -> Dict[str, Dict[str, Sequence[Path] | int]]:
    labels = load_labels(labels_path)
    graph_paths = sorted(graph_dir.glob("*.graphml"))
    if not graph_paths:
        raise FileNotFoundError(f"No .graphml files found in {graph_dir}")

    output_dir.mkdir(parents=True, exist_ok=True)
    embedder = LegalBertEmbedder(batch_size=encoder_batch_size, max_length=encoder_max_length)

    pyg_graphs: List[Data] = []
    for idx, graph_path in enumerate(graph_paths):
        graph_id = graph_path.stem
        if graph_id not in labels:
            raise KeyError(f"No label found for graph id {graph_id}")
        data = graph_to_data(graph_path, embedder, labels[graph_id], data_index=idx)
        pyg_graphs.append(data)

    if shuffle and len(pyg_graphs) > 1:
        rng = random.Random(split_seed)
        rng.shuffle(pyg_graphs)

    split_names = ("train", "validation", "test")
    split_counts = _normalized_split_counts(len(pyg_graphs), (train_ratio, val_ratio, test_ratio))
    dataset_prefix = output_dir.name
    saved_summary: Dict[str, Dict[str, Sequence[Path] | int]] = {}

    cursor = 0
    for split_name, count in zip(split_names, split_counts):
        split_graphs = pyg_graphs[cursor : cursor + count]
        cursor += count
        split_dir = output_dir / split_name
        file_prefix = f"{dataset_prefix}_{split_name}_batch"
        saved_paths = _save_split_batches(split_graphs, split_dir, graphs_per_file, file_prefix) if split_graphs else []
        saved_summary[split_name] = {"paths": saved_paths, "num_graphs": len(split_graphs)}

    return saved_summary


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Convert legal GraphML knowledge graphs into PyTorch Geometric format.",
    )
    parser.add_argument(
        "--graph-dir",
        type=Path,
        default=Path("outputs/graphs/legal_cases"),
        help="Directory containing GraphML files generated from the legal cases.",
    )
    parser.add_argument(
        "--labels-jsonl",
        type=Path,
        default=Path("outputs/datasets/legal_cases/legal_background.jsonl"),
        help="JSONL file storing the mapping between case id and label.",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("outputs/pyg_graphs/legal_cases"),
        help="Destination directory for the generated PyG tensors.",
    )
    parser.add_argument(
        "--graphs-per-file",
        type=int,
        default=32,
        help="Number of graphs to pack inside each serialized .pt file.",
    )
    parser.add_argument(
        "--encoder-batch-size",
        type=int,
        default=12,
        help="Batch size to use when embedding node or relation text with Legal-BERT.",
    )
    parser.add_argument(
        "--encoder-max-length",
        type=int,
        default=112,
        help="Maximum tokenized length fed into Legal-BERT.",
    )
    parser.add_argument(
        "--train-ratio",
        type=float,
        default=0.8,
        help="Fraction of graphs allocated to the training split.",
    )
    parser.add_argument(
        "--val-ratio",
        type=float,
        default=0.1,
        help="Fraction of graphs allocated to the validation split.",
    )
    parser.add_argument(
        "--test-ratio",
        type=float,
        default=0.1,
        help="Fraction of graphs allocated to the test split.",
    )
    parser.add_argument(
        "--no-shuffle",
        action="store_true",
        help="Disable shuffling before split assignment.",
    )
    parser.add_argument(
        "--split-seed",
        type=int,
        default=42,
        help="PRNG seed controlling the shuffle order before splitting.",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    saved = convert_directory(
        graph_dir=args.graph_dir,
        labels_path=args.labels_jsonl,
        output_dir=args.output_dir,
        graphs_per_file=args.graphs_per_file,
        encoder_batch_size=args.encoder_batch_size,
        encoder_max_length=args.encoder_max_length,
        train_ratio=args.train_ratio,
        val_ratio=args.val_ratio,
        test_ratio=args.test_ratio,
        shuffle=not args.no_shuffle,
        split_seed=args.split_seed,
    )
    total_graphs = sum(split_info["num_graphs"] for split_info in saved.values())
    print(f"Saved {total_graphs} graphs split across {args.output_dir}")
    for split_name, split_info in saved.items():
        num_graphs = split_info["num_graphs"]
        num_files = len(split_info["paths"])
        print(f"  - {split_name}: {num_graphs} graphs into {num_files} file(s)")


if __name__ == "__main__":
    main()
